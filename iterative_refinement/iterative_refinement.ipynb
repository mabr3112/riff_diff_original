{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d9f9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda027a",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "748c63f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/home/mabr3112/anaconda3/bin/python3.9\n",
    "####!!!!!!!!!!!!!!!!!!!!!!###########!!!!!!!!!!!!!!!!!!!!!!!!!####\n",
    "# --- Reactivate sbatch_array_jobstarter\n",
    "# --- reactivate wait_for_job\n",
    "# --- reqctivate sbatch_jobstarter\n",
    "####!!!!!!!!!!!!!!!!!!!!!!###########!!!!!!!!!!!!!!!!!!!!!!!!!####\n",
    "\n",
    "## ------------------------ Imports ------------------------------------------\n",
    "import sys\n",
    "sys.path += [\"/home/mabr3112/anaconda3/lib/python3.9/site-packages/\"]\n",
    "\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "from copy import deepcopy\n",
    "import re\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio import PDB\n",
    "import pandas as pd\n",
    "from subprocess import run\n",
    "import os\n",
    "import time\n",
    "from glob import glob\n",
    "import json\n",
    "from parse_multiple_chains import parse_poses\n",
    "from mpnn_scorecollector import *\n",
    "import numpy as np\n",
    "import calc_composite_score\n",
    "import bb_rmsd\n",
    "import motif_rmsd\n",
    "import mpnn_tools\n",
    "from Bio.PDB.PDBIO import PDBIO\n",
    "import remove_index_layers\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "import superimposition_tools\n",
    "import chainbreak_tools\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "# import custom modules\n",
    "import utils.biopython_mutate\n",
    "import utils.plotting as plots\n",
    "import utils.biopython_tools\n",
    "\n",
    "## ------------------------ Variables ----------------------------------------------\n",
    "__version__ = \"0.1\"\n",
    "_mpnn_options = {\"sampling_temp\": \"0.1\", \"batch_size\": \"1\", \"num_seq_per_target\": \"10\"}\n",
    "_relax_options = {} #\"nstruct\": \"10\"}\n",
    "_relax_flags = {\"-beta\", \"-overwrite\"}\n",
    "_rosetta_options = {}\n",
    "_rosetta_flags = {\"-beta\"}\n",
    "_proteinmpnn_path = \"/home/mabr3112/ProteinMPNN/vanilla_proteinmpnn/\"\n",
    "_of_path = \"/home/mabr3112/OmegaFold/main.py\"\n",
    "_af2_path = \"/home/mabr3112/anaconda3/bin/\"\n",
    "#_af2_path = \"/home/mabr3112/af2_localcolabfold/colabfold_batch/bin/\"\n",
    "_rosetta_paths = [\"/home/markus/Rosetta/\", \"/home/florian_wieser/Rosetta_new/\"]\n",
    "_rfdesign_python = \"/home/mabr3112/anaconda3/envs/SE3-nvidia/bin/python3\"\n",
    "_inpaint_path = \"/home/mabr3112/RFDesign/inpainting/\"\n",
    "_scripts_path = \"/home/mabr3112/projects/iterative_refinement/rosetta_scripts/\"\n",
    "_esm_opts = {\"chunk-size\": 8, \"max-tokens-per-batch\": 2}\n",
    "_esmfold_inference_script = \"/home/mabr3112/scripts/esmfold_inference.py\"\n",
    "_trf_relax_script_path = \"/home/mabr3112/RFDesign_old_scripts/scripts/trfold_relax.sh\"\n",
    "_python_path = \"/home/mabr3112/anaconda3/bin/python3.9\"\n",
    "_hallucination_path = \"/home/mabr3112/RFDesign/hallucination/\"\n",
    "_rfdiffusion_inference_script = \"/home/mabr3112/RFdiffusion/run_inference.py\"\n",
    "\n",
    "## ------------------------ Slurm Functions ----------------------------------------\n",
    "\n",
    "def wait_for_job(jobname: str, interval=5) -> str:\n",
    "    '''\n",
    "    Waits for a slurm job to be completed, then prints a \"completed job\" statement and returns jobname.\n",
    "    <jobname>: name of the slurm job. Job Name can be set in slurm with the flag -J\n",
    "    <interval>: interval in seconds, how log to wait until checking again if the job still runs.\n",
    "    '''\n",
    "    # Check if job is running by capturing the length of the output of squeue command that only returns jobs with <jobname>:\n",
    "    while len(run(f'squeue -n {jobname} -o \"%A\"', shell=True, capture_output=True, text=True).stdout.strip().split(\"\\n\")) > 1:\n",
    "        time.sleep(interval)\n",
    "    print(f\"Job {jobname} completed.\\n\")\n",
    "    time.sleep(10)\n",
    "    return jobname\n",
    "\n",
    "def add_timestamp(x: str) -> str:\n",
    "    '''\n",
    "    Adds a unique (in most cases) timestamp to a string using the \"time\" library.\n",
    "    Returns string with timestamp added to it.\n",
    "    '''\n",
    "    return \"_\".join([x, f\"{str(time.time()).replace('.', '')}\"])\n",
    "\n",
    "def split_list(input_list, element_length):\n",
    "    '''AAA'''\n",
    "    result = []\n",
    "    iterator = iter(input_list)\n",
    "    while True:\n",
    "        sublist = list(itertools.islice(iterator, element_length))\n",
    "        if not sublist:\n",
    "            break\n",
    "        result.append(sublist)\n",
    "    return result\n",
    "\n",
    "def sbatch_array_jobstarter(cmds: list, sbatch_options: list, jobname=\"sbatch_array_job\", max_array_size=10, wait=True, remove_cmdfile=True, cmdfile_dir=\"./\"):\n",
    "    '''\n",
    "    Writes [cmds] into a cmd_file that contains each cmd in a separate line.\n",
    "    Then starts an sbatch job running down the cmd-file.\n",
    "    '''\n",
    "    # check if cmds is smaller than 1000! ## TODO: if yes, split cmds and start split array!\n",
    "    if len(cmds) > 1000:\n",
    "        print(f\"The commands-list you supplied is longer than 1000 commands. This cluster does not support arrays that are longer than 1000 commands, so your job will be subdivided into multiple arrays.\")\n",
    "        for sublist in split_list(cmds, 1000):\n",
    "            sbatch_array_jobstarter(cmds=sublist, sbatch_options=sbatch_options, jobname=jobname, max_array_size=max_array_size, wait=wait, remove_cmdfile=remove_cmdfile, cmdfile_dir=cmdfile_dir)\n",
    "        return None\n",
    "    \n",
    "    # write cmd-file\n",
    "    jobname = add_timestamp(jobname)\n",
    "    with open((cmdfile := f\"{cmdfile_dir}/{jobname}_cmds\"), 'w') as f:\n",
    "        f.write(\"\\n\".join(cmds))\n",
    "    \n",
    "    # write sbatch command and run\n",
    "    sbatch_cmd = f'sbatch -a 1-{str(len(cmds))}%{str(max_array_size)} -J {jobname} -vvv {\" \".join(sbatch_options)} --wrap \"eval {chr(92)}`sed -n {chr(92)}${{SLURM_ARRAY_TASK_ID}}p {cmdfile}{chr(92)}`\"'\n",
    "    print(f\"\\nRunning:\\n{sbatch_cmd}\")\n",
    "    run(sbatch_cmd, shell=True, stdout=True, stderr=True, check=True)\n",
    "    if wait: wait_for_job(jobname)\n",
    "    if remove_cmdfile: run(f\"rm {cmdfile}\", shell=True, stdout=True, stderr=True, check=True)\n",
    "    return None\n",
    "    \n",
    "def sbatch_jobstarter(cmd, sbatch_options: list, jobname=\"sbatch_job\", wait=True):\n",
    "    '''\n",
    "    Starts an sbatch job that runs <cmd>.\n",
    "    Add any <sbatch_options> as a list: [\"-e err.log\", \"-o out.log\", \"...\"]\n",
    "    '''\n",
    "    jobname = add_timestamp(jobname)\n",
    "    sbatch_cmd = f\"sbatch -J {jobname} {' '.join(sbatch_options)} -vvv {cmd}\"\n",
    "    run(sbatch_cmd, shell=True, stdout=True, stderr=True, check=True)\n",
    "    if wait: wait_for_job(jobname)    \n",
    "    return None\n",
    "\n",
    "# --------------------------------------- Classes -------------------------------------\n",
    "\n",
    "class Poses():\n",
    "    '''\n",
    "    Python class for poses (proteins) either as fastas or as pdbs\n",
    "    Contains all information about the refinement cycle as attributes.\n",
    "    \n",
    "    Has to be initialized with Poses(work_dir, input_data):\n",
    "        <work_dir>:   Working directory of the cycle. Within this directory, Cycle will create its cycle_dir:\n",
    "                      work_dir/refinement_cycle_0001 (for n = 1)\n",
    "        <input_data>: Input data, has to be list with paths to input files. Can be any file that operational functions can work on (currently .fa or .pdb files)\n",
    "                      \n",
    "    Attributes:\n",
    "        <self.poses_df>         pd.DataFrame: This is a pandas DataFrame that contains all information about the current poses.\n",
    "                                The input_poses are stored in self.poses_df[input_poses]\n",
    "                                Any DataFrame that is added to self.poses_df should carry in its columns a prefix\n",
    "                                of where the columns being added are coming from. Example: After running ProteinMPNN\n",
    "                                for the first time, the ProteinMPNN scores will be prefixed with \"mpnn_0001_\".\n",
    "        \n",
    "        <self.scorefiles>       The Cycle Class stores the location of all scorefiles generated by operational functions\n",
    "                                in a dictionary. This dictionary is stored as the attribute self.scorefiles\n",
    "        \n",
    "        <self.poses>            list: Contains paths to the current poses in a list. Example: [\"path_to/pose_1.fa\", ..., \"path_to/pose_n.fa\"]\n",
    "        \n",
    "        <self.index_layers>     int: How many index layers (_0001) have been added to the poses starting from <input_data>\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, work_dir: str, input_data: list):\n",
    "        '''\n",
    "        Args:\n",
    "            <n>:          Iteration of the cycle.\n",
    "            <work_dir>:   Working directory of refinement.\n",
    "            <input_data>: Input data, has to be list with paths to input files. Can be any file that operational functions can work on.\n",
    "        '''\n",
    "        # sanity checks\n",
    "        if all([os.path.isfile(f) for f in input_data]):\n",
    "            print(f\"{len(input_data)} input poses found in input data.\\n\")\n",
    "        else: raise FileNotFoundError(f\"ERROR: One or more files of input data were not found at the specified location. Check input data!\")\n",
    "        \n",
    "        # initialize\n",
    "        self.dir = os.path.abspath(work_dir)\n",
    "        self.scores_dir = f\"{self.dir}/scores/\"\n",
    "        self.plot_dir = f\"{self.dir}/plots/\"\n",
    "        os.makedirs(self.dir, exist_ok=True)\n",
    "        os.makedirs(self.scores_dir, exist_ok=True)\n",
    "        os.makedirs(self.plot_dir, exist_ok=True)\n",
    "        self.poses = [input_data] if type(input_data) == str else [os.path.abspath(p) for p in input_data]\n",
    "        if not self.poses: raise ValueError(f\"ERROR: No poses found with {input_data}\")\n",
    "        self.poses_df = pd.DataFrame({\"input_poses\": self.poses, \n",
    "                                      \"poses_description\": [x.split(\"/\")[-1].split(\".\")[0] for x in self.poses],\n",
    "                                      \"poses\": self.poses\n",
    "                                     })\n",
    "        self.index_layers = 0\n",
    "        self.scorefile = f\"{self.dir}/{self.dir.split('/')[-1]}_scores.json\"\n",
    "        self.scorefiles = dict()\n",
    "        self.auto_dump_df = True\n",
    "        \n",
    "        # Indeces for operational functions\n",
    "        self.mpnn_runs = 1\n",
    "        self.prediction_runs = 1\n",
    "        self.relax_runs = 1\n",
    "        self.filter_runs = 1\n",
    "        self.rosetta_runs = 1\n",
    "        self.set_poses_number = 1\n",
    "        self.reindex_number = 1\n",
    "        \n",
    "        # Slurm Resource Settings\n",
    "        self.max_predict_gpus = 10\n",
    "        self.max_mpnn_gpus = 5\n",
    "        self.max_relax_cpus = 320\n",
    "        self.max_rosetta_cpus = 320\n",
    "        self.max_inpaint_gpus = 10 \n",
    "        self.max_diffusion_gpus = 10\n",
    "        \n",
    "        # MPNN attributes\n",
    "    \n",
    "    # ------------------ Poses Initializing functions ----------------------------------------\n",
    "    def increment_attribute(self, attribute):\n",
    "        if hasattr(self, attribute):\n",
    "            setattr(self, attribute, getattr(self, attribute) + 1)\n",
    "            return getattr(self, attribute) + 1\n",
    "        else:\n",
    "            setattr(self, attribute, 1)\n",
    "            return 1\n",
    "    \n",
    "    # ------------------ Poses Operational functions -----------------------------------------\n",
    "    \n",
    "    def filter_poses_by_score(self, n: float, score_col: str, remove_layers=None, layer_col=\"poses_description\", sep=\"_\", ascending=True, prefix=None, plot=False) -> pd.DataFrame:\n",
    "        '''\n",
    "        Filters your current poses by a specified scoreterm down to either a fraction (of all poses) or a total number of poses,\n",
    "        depending on which value was given with <n>.\n",
    "        \n",
    "        Args:\n",
    "            <n>:                   Any positive number. Number between 0 and 1 are interpreted as a fraction to filter,\n",
    "                                   Numbers >1 are interpreted as absolute number of poses after filtering.\n",
    "            <score_col>:           Column name of the column that contains the scores by which the poses should be filtered.\n",
    "            <plot>:                (bool, str, list) Do you want to plot filtered stats? If True, it will plot filtered scoreterm. \n",
    "                                   If str, it will look for the argument in self.poses_df and use that scoreterm for plotting.\n",
    "                                   If list, it will try to plot all scoreterms in the list.\n",
    "        \n",
    "        To filter your DataFrame poses based on parent poses, add arguments: \n",
    "            <remove_layers>        how many index layers must be removed to reach parent pose?\n",
    "            <layer_col>            column name that contains names of the poses, Default=\"poses_description\"\n",
    "            <sep>                  index layer separator for pose names in layer_col (pose_0001_0003.pdb -> '_')\n",
    "            \n",
    "            \n",
    "        Returns filtered DataFrame. Updates Poses and pose_df.\n",
    "        '''\n",
    "        if score_col not in self.poses_df.columns:\n",
    "            # check if any column in self.poses_df contains <score_col> and if so, return the column that has the hightest number in it (for example mpnn_run_0002)\n",
    "            if any([x.endswith(score_col) for x in self.poses_df.columns]):\n",
    "                score_col = sorted([x for x in list(self.poses_df.columns) if x.endswith(score_col)])[-1]\n",
    "            else:\n",
    "                raise KeyError(f\"ERROR: Scoreterm {score_col} not found in DataFrame. Available Scoreterms: {', '.join(list(self.poses_df.columns))}\")\n",
    "        \n",
    "        # store current self.poses_df as prefilter_1_scores.json\n",
    "        output_name = prefix or f\"filter_run_{str(self.filter_runs).zfill(4)}\"\n",
    "        self.poses_df.to_json(f\"{self.scores_dir}/{output_name}.json\")\n",
    "        self.increment_attribute(\"filter_runs\")\n",
    "        \n",
    "        # filter df down by n (either fraction if n < 0, or number of filtered poses if n > 1)\n",
    "        n = determine_filter_n(self.poses_df, n)\n",
    "        \n",
    "        # Filter poses_df down to the number of poses specified with <n>\n",
    "        orig_len = str(len(self.poses_df))\n",
    "        filter_df = filter_dataframe(df=self.poses_df, col=score_col, n=n, remove_layers=remove_layers, layer_col=layer_col, sep=sep, ascending=ascending)\n",
    "        print(f\"Filtered poses from {orig_len} to {str(len(filter_df))} structures.\")\n",
    "        \n",
    "        # create filter-plots if specified.\n",
    "        if plot:\n",
    "            columns = plots.parse_cols_for_plotting(plot, subst=score_col)\n",
    "            plots.violinplot_multiple_cols_dfs(dfs=[self.poses_df, filter_df], df_names=[\"Before Filtering\", \"After Filtering\"], \n",
    "                                               cols=columns, titles=columns, y_labels=columns, out_path=f\"{self.plot_dir}/{output_name}.png\")\n",
    "        \n",
    "        # Print filtering stats\n",
    "        newline = \"\\n\"\n",
    "        print(f\"\\nFiltered scores:\\n{newline.join([description + ': ' + str(score) for description, score in zip(list(filter_df['poses_description']), list(filter_df[score_col]))])}\")\n",
    "        \n",
    "        # update object attributs [poses_df]\n",
    "        self.poses_df = filter_df\n",
    "        self.update_poses()\n",
    "        \n",
    "        # if argument self.auto_dump_df is set to True, dump the new poses_df in self.dir\n",
    "        if self.auto_dump_df:\n",
    "            self.poses_df.to_json(self.scorefile)\n",
    "        \n",
    "        return filter_df\n",
    "    \n",
    "    def mpnn_design(self, mpnn_options=\"\", pose_options=None, mpnn_scorefile=\"mpnn_scores.json\", prefix=None, \n",
    "                    fixed_positions_col=None, tied_positions_col=None, design_chains=None) -> str:\n",
    "        '''\n",
    "        Runs ProteinMPNN on all structures in <poses> with specified <mpnn_options>.\n",
    "        \n",
    "        Args:\n",
    "            <mpnn_options>              Commandline options in string format that shall be passed to ProteinMPNN for every Pose.\n",
    "            <pose_options>              List of commandline options (string) that shall be passed to ProteinMPNN. List needs to be same length as poses.\n",
    "            <mpnn_scorefile>            Set a custom scorefile name (default: mpnn_scores.json)\n",
    "            <prefix>                    Set a prefix for this poses operation. This will also be the name of the working directory for this operation.\n",
    "            <fixed_positions_col>       Column name in self.poses_df that contains dictionaries with fixed_positions dictionaries for ProteinMPNN.\n",
    "            <tied_positions_col>        Column name in self.poses_df that contains dictionaries with tied_positions dictionaries for ProteinMPNN.\n",
    "        \n",
    "        Increments the self.mpnn_runs and self.index_layers attribute by one.\n",
    "        Adds mpnn_output_0001 as attribute to self.\n",
    "        Updates poses attribute to location of *.fa files generated by ProteinMPNN.\n",
    "        \n",
    "        Returns: dir\n",
    "            Returns the directory that stores individual .fa files.\n",
    "        '''\n",
    "        # Prepare mpnn_options (make sure that all necessary options are present) and check if poses are .fa files\n",
    "        mpnn_options = parse_options_string(mpnn_options)\n",
    "        mpnn_options = prep_options(mpnn_options, _mpnn_options)\n",
    "        if not all([x.endswith(\".pdb\") for x in self.poses]): raise TypeError(f\"ERROR: your current poses are not .pdb files! They must be .pdb files to run ProteinMPNN!\")\n",
    "        \n",
    "        # Setup mpnn_run directory\n",
    "        mpnn_run = prefix or f\"mpnn_run_{str(self.mpnn_runs).zfill(4)}\"\n",
    "        abs_mpnn_dir = f\"{self.dir}/{mpnn_run}\"\n",
    "        mpnn_options[\"out_folder\"] = abs_mpnn_dir\n",
    "        if not os.path.isdir(abs_mpnn_dir): os.makedirs(abs_mpnn_dir, exist_ok=True)\n",
    "        \n",
    "        # Parse pose_options into dictionaries if pose_options is set. Otherwise create empty dictionaries for each pose (no pose_options)\n",
    "        if pose_options: pose_options = [parse_options_string(options_string) for options_string in pose_options]\n",
    "        else: pose_options = [{} for pose in self.poses_df[\"poses_description\"]]\n",
    "        \n",
    "        # if fixed_positions option is set, write fixed_positions_jsonl file and add it to mpnn_options:\n",
    "        if fixed_positions_col:\n",
    "            fixed_positions_filename = f\"{mpnn_options['out_folder']}/fixed_positions.jsonl\"\n",
    "            if not os.path.isfile(fixed_positions_filename):\n",
    "                # check if fixed_positions in fixed_positions_col are valid\n",
    "                fixed_positions_list = check_fixed_positions(list(self.poses_df[fixed_positions_col]))\n",
    "                ffn = write_keysvalues_to_file(keys=list(self.poses_df[\"poses_description\"]),\n",
    "                                               values=fixed_positions_list,\n",
    "                                               outfile_name=fixed_positions_filename)\n",
    "            mpnn_options[\"fixed_positions_jsonl\"] = fixed_positions_filename\n",
    "        \n",
    "        # if tied_positions option is set, write tied_positions_jsonl file and add it to mpnn_options:\n",
    "        if tied_positions_col:\n",
    "            mpnn_options[\"tied_positions_jsonl\"] = tied_positions_filename = f\"{mpnn_options['out_folder']}/tied_positions.jsonl\"\n",
    "            if not os.path.isfile(tied_positions_filename): write_keysvalues_to_file(keys=list(self.poses_df[\"poses_description\"]), values=list(self.poses_df[tied_positions_col]), outfile_name=tied_positions_filename)\n",
    "            \n",
    "        # if design_chains is set, write design_chains_jsonl file for ProteinMPNN:\n",
    "        if design_chains:\n",
    "            it = mpnn_tools.check_design_chains(design_chains, self.poses_df[\"poses_description\"])\n",
    "            mpnn_options[\"chain_id_jsonl\"] = self.poses_mpnn_jsonl_writer(design_chains, f\"{mpnn_options['out_folder']}/design_chains.jsonl\", iterate=it)\n",
    "        \n",
    "        # Run ProteinMPNN function that handles ProteinMPNN and check if ProteinMPNN was successful:\n",
    "        scores = proteinmpnn(self.poses, mpnn_options, pose_options=pose_options, max_gpus=self.max_mpnn_gpus, scorefile=mpnn_scorefile)\n",
    "        if not glob((mpnn_globstr := f\"{mpnn_options['out_folder']}/seqs/*.fa\")):\n",
    "            raise FileNotFoundError(f\"No *.fa files found in {mpnn_globstr}. ProteinMPNN did not run properly.\")\n",
    "        \n",
    "        # Update attributes: [scorefiles, poses, index_layers, mpnn_runs]\n",
    "        self.scorefiles[mpnn_run] = f\"{abs_mpnn_dir}/{mpnn_scorefile}\"\n",
    "        self.poses = list(scores[\"location\"])\n",
    "        self.increment_attribute(\"index_layers\")\n",
    "        self.increment_attribute(\"mpnn_runs\")\n",
    "        scores = pd.DataFrame(scores).add_prefix((mpnn_prefix := f\"{mpnn_run}_\"))\n",
    "        scores = update_df(scores, f\"{mpnn_prefix}description\", self.poses_df, \"poses_description\", new_df_col_remove_layer=1)\n",
    "        \n",
    "        # update poses_description column in scores and update poses_df\n",
    "        scores.loc[:,\"poses_description\"] = scores.loc[:, f\"{mpnn_prefix}description\"]\n",
    "        self.poses_df = scores\n",
    "        self.poses_df.loc[:, \"poses\"] = self.poses_df[f\"{mpnn_prefix}location\"]\n",
    "        self.poses_df.to_json(self.scorefile)\n",
    "        \n",
    "        return f\"{abs_mpnn_dir}/seqs\"\n",
    "    \n",
    "    def poses_mpnn_jsonl_writer(self, values, json_filename, iterate=False):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        # Check in self.poses_df for column with name <values> from which to retreive values list.\n",
    "        if type(values) == str: \n",
    "            values = list(self.poses_df[values])\n",
    "            iterate = True\n",
    "        \n",
    "        # write jsonl file:\n",
    "        return mpnn_tools.write_mpnn_jsonl(list(self.poses_df[\"poses_description\"]), values=values, json_filename=json_filename, iterate=iterate)\n",
    "    \n",
    "    def relax_poses(self, relax_options=\"\", pose_options=None, n=10, prefix=None, filter_scoreterm=\"total_score\", remove_bad_decoys=True):\n",
    "        '''\n",
    "        Runs (default) 10 relax trajectories of each pose and sets the output with the lowest energy as the new pose.\n",
    "        This version of relax does not use nstruct, as nstruct without mpirun is limited to one cpu which is inefficient on the cluster!\n",
    "        \n",
    "        Args:\n",
    "            <relax_options>         Commandline options that you want to use to run Rosetta Relax.\n",
    "                                    Default options are stored in the global variable _relax_options.\n",
    "            <pose_options>          List of commandline options (sorted by self.poses_df[\"poses_description\"]) \n",
    "                                    that can be supplied to the poses individually.\n",
    "            <n>                     How many relax-trajectories to run. If this option is set, it overwrites 'nstruct' given in <relax_options>!\n",
    "            <prefix>                Set a custom prefix which will be added to all relax_scores when they are written into poses_df.\n",
    "                                    By default, it will add relax_poses_0001.\n",
    "                                    ! Prefix also determines the name of the directory in which relax results will be stored !\n",
    "            <filter_scoreterm>      The scoreterm that will be used to select the lowest energy pose from all relax trajectories of each pose.\n",
    "                                    (Default: total_score)\n",
    "        '''\n",
    "        # Setup relax_run directory\n",
    "        if not prefix: prefix = f\"relax_run_{str(self.relax_runs).zfill(4)}\"\n",
    "        self.increment_attribute(\"relax_runs\")\n",
    "        relax_dir = f\"{self.dir}/{prefix}\"\n",
    "        \n",
    "        # Parse pose_options into dictionaries if pose_options is set. Otherwise create empty dictionaries for each pose (no pose_options)\n",
    "        if pose_options: pose_options = [parse_rosetta_options_string(options_string) for options_string in pose_options]\n",
    "        else: pose_options = [({}, []) for pose in self.poses_df[\"poses_description\"]]\n",
    "        \n",
    "        # Prepare relax options\n",
    "        relax_options, relax_flags = parse_rosetta_options_string(relax_options)\n",
    "        relax_flags = list(set(relax_flags) | _relax_flags) # combine with set operation and reconvert them into a list.\n",
    "        relax_options = prep_options(relax_options, _relax_options)\n",
    "        \n",
    "        # Run Rosetta Relax with relax_options and check if run was successful\n",
    "        relax_output = run_relax(self.poses, relax_options, relax_flags, pose_options=[x[0] for x in pose_options], pose_flags=[x[1] for x in pose_options],\n",
    "                                 n=n, work_dir=relax_dir, scorefile=\"relax_scores.sc\", max_cpus=self.max_relax_cpus)\n",
    "        \n",
    "        # Filter decoys down to the lowest score decoy of each pose\n",
    "        relax_scores = relax_output[\"scores\"]\n",
    "        filtered_poses = filter_dataframe(relax_scores, filter_scoreterm, n=1, remove_layers=1, layer_col=\"description\", sep=\"_\", ascending=True)\n",
    "\n",
    "        if remove_bad_decoys:\n",
    "            # check if files were already removed:\n",
    "            if len(glob(f\"{relax_dir}/*.pdb\")) > len(filtered_poses):\n",
    "                # Collect DataFrame that contains all but the filtered poses.\n",
    "                remove_pdbs = relax_scores[~relax_scores[\"description\"].isin(filtered_poses[\"description\"])]\n",
    "            \n",
    "                # remove all .pdb files that are not top decoys from pdb_dir\n",
    "                print(f\"Deleting higher energy relax decoys\")\n",
    "                for pdb in list(remove_pdbs[\"location\"]):\n",
    "                    try:\n",
    "                        os.remove(pdb)\n",
    "                    except FileNotFoundError:\n",
    "                        print(f\"WARNING: Trying to remove file that does not exist anymore: {pdb}\")\n",
    "\n",
    "        # Update poses_df and poses\n",
    "        self.poses = list(filtered_poses[\"location\"])\n",
    "        filtered_poses = filtered_poses.add_prefix(prefix+\"_\")\n",
    "        self.poses_df = update_df(filtered_poses, f\"{prefix}_description\", self.poses_df, \"poses_description\", new_df_col_remove_layer=1)\n",
    "        self.poses_df.loc[:, \"poses\"] = self.poses_df[f\"{prefix}_location\"]\n",
    "        self.poses_df.loc[:, \"poses_description\"] = filtered_poses[f\"{prefix}_description\"]\n",
    "        self.poses_df.to_json(self.scorefile)\n",
    "        \n",
    "        return relax_output[\"pdb_dir\"]\n",
    "    \n",
    "    def create_relax_decoys(self, relax_options=\"\", pose_options=None, n=10, prefix=None) -> str:\n",
    "        '''\n",
    "        Runs (default) 10 relax trajectories of each pose and stores all poses from the relax run.\n",
    "        This version of relax does not use nstruct, as nstruct without mpirun is limited to one cpu which is inefficient on the cluster!\n",
    "        \n",
    "        Args:\n",
    "            <relax_options>         Commandline options that you want to use to run Rosetta Relax.\n",
    "                                    Default options are stored in the global variable _relax_options.\n",
    "            <n>                     How many relax-trajectories to run. If this option is set, it overwrites 'nstruct' given in <relax_options>!\n",
    "            <prefix>                Set a custom prefix which will be added to all relax_scores when they are written into poses_df.\n",
    "                                    By default, it will add relax_poses_0001.\n",
    "                                    ! Prefix also determines the name of the directory in which relax results will be stored !\n",
    "        '''\n",
    "        # Setup relax_run directory\n",
    "        if not prefix: prefix = f\"relax_run_{str(self.relax_runs).zfill(4)}\"\n",
    "        self.increment_attribute(\"relax_runs\")\n",
    "        relax_dir = f\"{self.dir}/{prefix}\"\n",
    "        \n",
    "        # Parse pose_options into dictionaries if pose_options is set. Otherwise create empty dictionaries for each pose (no pose_options)\n",
    "        if pose_options: pose_options = [parse_rosetta_options_string(options_string) for options_string in pose_options]\n",
    "        else: pose_options = [({}, []) for pose in self.poses_df[\"poses_description\"]]\n",
    "        \n",
    "        # Prepare relax options\n",
    "        relax_options, relax_flags = parse_rosetta_options_string(relax_options)\n",
    "        relax_flags = list(set(relax_flags) | _relax_flags) # combine with set operation and reconvert them into a list.\n",
    "        relax_options = prep_options(relax_options, _relax_options)\n",
    "        \n",
    "        # Run Rosetta Relax with relax_options and check if run was successful\n",
    "        relax_output = run_relax(self.poses, relax_options, relax_flags, pose_options=[x[0] for x in pose_options], pose_flags=[x[1] for x in pose_options],\n",
    "                                 n=n, work_dir=relax_dir, scorefile=\"relax_scores.sc\", max_cpus=self.max_relax_cpus)\n",
    "        \n",
    "        # Update poses_df and poses\n",
    "        scores = relax_output[\"scores\"]\n",
    "        self.poses = list(scores[\"location\"])\n",
    "        scores = scores.add_prefix(prefix+\"_\")\n",
    "        self.poses_df = update_df(scores, f\"{prefix}_description\", self.poses_df, \"poses_description\", new_df_col_remove_layer=1)\n",
    "        self.poses_df.loc[:, \"poses\"] = self.poses_df[f\"{prefix}_location\"]\n",
    "        self.poses_df.loc[:, \"poses_description\"] = self.poses_df[f\"{prefix}_description\"]\n",
    "        self.poses_df.to_json(self.scorefile)\n",
    "        \n",
    "        return relax_output[\"pdb_dir\"]\n",
    "    \n",
    "    def predict_sequences(self, predict_function: str, options=\"\", prefix=None, af2_rename_reference_col:str=None) -> str:\n",
    "        '''\n",
    "        Predicts sequences given by <poses> with <predict_function>.\n",
    "        <options> contains all options needed for running <predict_function>.\n",
    "        <options> should be a string in the style of command-line options that would be passed to the prediction model.\n",
    "        \n",
    "        <predict_function>: can be one of [run_AlphaFold2, run_OmegaFold]\n",
    "        <af2_rename_reference_col>: column in self.poses_df that contains reference pose paths.\n",
    "        \n",
    "        Returns directory that contains all predicted .pdbs\n",
    "        '''\n",
    "        # sanity checks\n",
    "        if all([x.endswith(\".fa\") for x in self.poses]):\n",
    "            pass\n",
    "        elif all([x.endswith(\".pdb\") for x in self.poses]):\n",
    "            self.poses_pdb_to_fasta()\n",
    "\n",
    "        # create prediction dir\n",
    "        predict_run = prefix or f\"predict_run_{str(self.prediction_runs).zfill(4)}\"\n",
    "        abs_predict_dir = f\"{self.dir}/{predict_run}\"\n",
    "        \n",
    "        # prepare options\n",
    "        options = parse_options_string(options, sep=\"--\")\n",
    "        options[\"output_dir\"] = abs_predict_dir\n",
    "        \n",
    "        # run prediction with <predict_function>\n",
    "        prediction_outputs = predict_function(self.poses_df['poses'].to_list(), options, max_gpus=self.max_predict_gpus)\n",
    "        scores = prediction_outputs[\"scores\"] ## lazy programming\n",
    "        \n",
    "        # update attributes:\n",
    "        self.increment_attribute(\"prediction_runs\")\n",
    "        scores = scores.add_prefix(predict_run+\"_\")\n",
    "        self.poses_df = update_df(scores, f\"{predict_run}_description\", self.poses_df, \"poses_description\", new_df_col_remove_layer=0)\n",
    "        self.poses_df.loc[:, \"poses\"] = self.poses_df[f\"{predict_run}_location\"]\n",
    "        self.poses = self.poses_df['poses']\n",
    "        self.poses_df.to_json(self.scorefile)\n",
    "\n",
    "        # rename af2 preds if option is set:\n",
    "        if af2_rename_reference_col: [utils.biopython_tools.rename_pdb_chains_pdbfile(pdb, ref_pdb) for pdb, ref_pdb in zip(self.poses_df['poses'].to_list(), self.poses_df[af2_rename_reference_col].to_list())]\n",
    "        \n",
    "        return prediction_outputs[\"pdb_dir\"]\n",
    "    \n",
    "    def poses_pdb_to_fasta(self, chain_sep=\":\") -> list[str]:\n",
    "        '''\n",
    "        Extracts sequences from self.poses if poses are .pdb files.\n",
    "        Stores .fa files where poses are stored.\n",
    "        Args:\n",
    "            <chain_sep>              Separator with which the chains should be joined if multiple chains are in one object.\n",
    "        \n",
    "        returns poses\n",
    "        '''\n",
    "        # sanity check\n",
    "        if any([not x.endswith(\".pdb\") for x in self.poses]):\n",
    "            raise TypeError(f\"ERROR: Not all poses are .pdb files! Check poses objects.\")\n",
    "    \n",
    "        # Start the parser\n",
    "        pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "        ppb = Bio.PDB.PPBuilder()\n",
    "    \n",
    "        # Get the structures\n",
    "        poses_l = [pdb_parser.get_structure(pose, pose) for pose in self.poses]\n",
    "        \n",
    "        # collect the sequences\n",
    "        sequences = [chain_sep.join([str(x.get_sequence()) for x in ppb.build_peptides(pose)]) for pose in poses_l]\n",
    "        \n",
    "        # write fasta-files\n",
    "        renaming_dict = dict()\n",
    "        for pose, seq in zip(self.poses, sequences):\n",
    "            fasta_name = pose.replace(\".pdb\", \".fa\")\n",
    "            description = pose.split(\"/\")[-1].split(\".\")[0]\n",
    "            with open(fasta_name, 'w') as f:\n",
    "                f.write(f\">{description}\\n{seq}\")\n",
    "            renaming_dict[pose] = fasta_name\n",
    "        \n",
    "        # set fasta_files as new poses\n",
    "        self.poses_df.loc[:, \"poses\"] = [renaming_dict[pose] for pose in list(self.poses_df[\"poses\"])]\n",
    "        self.poses = list(self.poses_df[\"poses\"])\n",
    "        \n",
    "        return self.poses\n",
    "    \n",
    "    def rosetta(self, executable: str, options=\"\", pose_options=None, n=10, prefix=None, force_options=None) -> str:\n",
    "        '''\n",
    "        Runs Rosetta Executable on poses.\n",
    "        Commandline arguments can be provided with <options>. <n> overwrites -nstruct commandline option.\n",
    "        Pose options can be provided as a dictionary {pose: \"pose_option\"}\n",
    "        '''\n",
    "        # Setup relax_run directory\n",
    "        if not prefix: prefix = f\"rosetta_run_{str(self.rosetta_runs).zfill(4)}\"\n",
    "        self.increment_attribute(\"rosetta_runs\")\n",
    "        rosetta_dir = f\"{self.dir}/{prefix}\"\n",
    "        \n",
    "        # Parse pose_options into dictionaries if pose_options is set. Otherwise create empty dictionaries for each pose (no pose_options)\n",
    "        if pose_options: pose_options = [parse_rosetta_options_string(options_string) for options_string in pose_options]\n",
    "        else: pose_options = [({}, []) for pose in self.poses_df[\"poses_description\"]]\n",
    "        \n",
    "        # Prepare Rosetta options\n",
    "        rosetta_options, rosetta_flags = parse_rosetta_options_string(options)\n",
    "        rosetta_flags = list(set(rosetta_flags) | _rosetta_flags) # combine with set operation and reconvert them into a list.\n",
    "        rosetta_options = prep_options(rosetta_options, _rosetta_options)\n",
    "        \n",
    "        # Run Rosetta Relax with relax_options and check if run was successful\n",
    "        rosetta_output = run_rosetta(self.poses_df[\"poses\"].to_list(), executable, rosetta_options, rosetta_flags, pose_options=[x[0] for x in pose_options], pose_flags=[x[1] for x in pose_options],\n",
    "                                    n=n, work_dir=rosetta_dir, scorefile=\"rosetta_scores.sc\", max_cpus=self.max_rosetta_cpus, force_options=force_options)\n",
    "        \n",
    "        # Update poses_df and poses\n",
    "        scores = rosetta_output[\"scores\"]\n",
    "        #self.poses = list(scores[\"location\"])\n",
    "        scores = scores.add_prefix(prefix+\"_\")\n",
    "        self.poses_df = update_df(scores, f\"{prefix}_description\", self.poses_df, \"poses_description\", new_df_col_remove_layer=1)\n",
    "        self.poses_df.loc[:, \"poses\"] = self.poses_df[f\"{prefix}_location\"]\n",
    "        self.poses = self.poses_df[\"poses\"].to_list()\n",
    "        self.poses_df.loc[:, \"poses_description\"] = self.poses_df[f\"{prefix}_description\"]\n",
    "        self.poses_df.to_json(self.scorefile)\n",
    "        \n",
    "        return rosetta_output[\"pdb_dir\"]\n",
    "    \n",
    "    def inpaint(self, options=\"\", pose_options=None, prefix=None, perres_lddt=False, perres_inpaint_lddt=False, trf_relax=True, calc_chainbreak=True) -> dict:\n",
    "        '''\n",
    "        <pose_options> overwrite global <options>\n",
    "        Args:\n",
    "            <options>\n",
    "            <pose_options>\n",
    "            <prefix>\n",
    "            <perres_lddt>\n",
    "            <perres_inpaint_lddt>\n",
    "        \n",
    "        Returns:\n",
    "            Directory where current poses are stored at.\n",
    "        '''\n",
    "        # setup directory\n",
    "        self.increment_attribute(\"inpainting_runs\")\n",
    "        prefix = prefix or f\"inpainting_run_{str(self.inpainting_runs).zfill(4)}\"\n",
    "        inpainting_dir = f\"{self.dir}/{prefix}\"\n",
    "        \n",
    "        # run inpainting\n",
    "        inpainting_output = run_inpainting(self.poses, work_dir=inpainting_dir, options=options, pose_options=pose_options, \n",
    "                                           scorefile=\"inpainting_scores.json\", max_gpus=self.max_inpaint_gpus, perres_lddt=perres_lddt, perres_inpaint_lddt=perres_inpaint_lddt)\n",
    "        \n",
    "        # update poses_df and poses\n",
    "        scores = inpainting_output[\"scores\"]\n",
    "        self.poses = list(scores[\"location\"])\n",
    "        scores = scores.add_prefix(prefix+\"_\")\n",
    "        self.poses_df = update_df(scores, f\"{prefix}_description\", self.poses_df, \"poses_description\", new_df_col_remove_layer=1)\n",
    "        self.poses_df.loc[:, \"poses\"] = self.poses_df[f\"{prefix}_location\"]\n",
    "        self.poses_df.loc[:, \"poses_description\"] = self.poses_df[f\"{prefix}_description\"]\n",
    "        self.poses_df.to_json(self.scorefile)\n",
    "        \n",
    "        # run trf_relax if specified:\n",
    "        if trf_relax:\n",
    "            trf_dir = trf_relax_dir(inpainting_output[\"pdb_dir\"])\n",
    "            \n",
    "            # calculate rmsds and integrate into poses_df\n",
    "            rmsds = calc_inpaint_trf_motif_rmsd(inpaint_dir=inpainting_output[\"pdb_dir\"], trf_dir=trf_dir, out_scorefile=f\"{inpainting_dir}/trf_motif_ca_rmsd.json\")\n",
    "            rmsds = rmsds.add_prefix(f\"{prefix}_\")\n",
    "            self.poses_df = update_df(rmsds, f\"{prefix}_description\", self.poses_df, \"poses_description\")\n",
    "            \n",
    "            # update poses_location\n",
    "            self.new_poses_path(trf_dir)\n",
    "            inpainting_output[\"pdb_dir\"] = trf_dir\n",
    "            self.poses_df[f\"{prefix}_trf_relax_location\"] = self.poses_df[\"poses\"]\n",
    "            \n",
    "            if calc_chainbreak: self.check_inpaint_chainbreak(prefix)\n",
    "        \n",
    "        return inpainting_output[\"pdb_dir\"]\n",
    "    \n",
    "    def hallucinate(self, options=\"\", pose_options=None, prefix=None, trf_relax=True, calc_chainbreak=False) -> dict:\n",
    "        '''\n",
    "        <pose_options> overwrite global <options>\n",
    "        Args:\n",
    "            <options>\n",
    "            <pose_options>\n",
    "            <prefix>\n",
    "            <perres_lddt>\n",
    "            <perres_inpaint_lddt>\n",
    "        \n",
    "        Returns:\n",
    "            Directory where current poses are stored at.\n",
    "        '''\n",
    "        # setup directory\n",
    "        self.increment_attribute(\"hallucination_runs\")\n",
    "        prefix = prefix or f\"hallucination_run_{str(self.hallucination_runs).zfill(4)}\"\n",
    "        hallucination_dir = f\"{self.dir}/{prefix}\"\n",
    "        \n",
    "        # run inpainting\n",
    "        hallucination_output = run_hallucination(self.poses_df[\"poses\"].to_list(), work_dir=hallucination_dir, options=options, pose_options=pose_options, \n",
    "                                           scorefile=\"hallucination_scores.json\", max_gpus=self.max_inpaint_gpus)\n",
    "        \n",
    "        # update poses_df and poses\n",
    "        scores = hallucination_output[\"scores\"]\n",
    "        self.poses = list(scores[\"location\"])\n",
    "        scores = scores.add_prefix(prefix+\"_\")\n",
    "        self.poses_df = update_df(scores, f\"{prefix}_description\", self.poses_df, \"poses_description\", new_df_col_remove_layer=1)\n",
    "        self.poses_df.loc[:, \"poses\"] = self.poses_df[f\"{prefix}_location\"]\n",
    "        self.poses_df.loc[:, \"poses_description\"] = self.poses_df[f\"{prefix}_description\"]\n",
    "        self.poses_df.to_json(self.scorefile)\n",
    "        \n",
    "        # run trf_relax if specified:\n",
    "        if trf_relax:\n",
    "            trf_dir = trf_relax_dir(hallucination_output[\"pdb_dir\"])\n",
    "            \n",
    "            # calculate rmsds and integrate into poses_df\n",
    "            rmsds = calc_inpaint_trf_motif_rmsd(inpaint_dir=hallucination_output[\"pdb_dir\"], trf_dir=trf_dir, out_scorefile=f\"{hallucination_dir}/trf_motif_ca_rmsd.json\")\n",
    "            rmsds = rmsds.add_prefix(f\"{prefix}_\")\n",
    "            self.poses_df = update_df(rmsds, f\"{prefix}_description\", self.poses_df, \"poses_description\")\n",
    "            \n",
    "            # update poses_location\n",
    "            self.new_poses_path(trf_dir)\n",
    "            hallucination_output[\"pdb_dir\"] = trf_dir\n",
    "            self.poses_df[f\"{prefix}_trf_relax_location\"] = self.poses_df[\"poses\"]\n",
    "            \n",
    "            if calc_chainbreak: self.check_inpaint_chainbreak(prefix)\n",
    "        \n",
    "        return hallucination_output[\"pdb_dir\"]\n",
    "\n",
    "    def rfdiffusion(self, options=\"\", pose_options=None, prefix=None) -> str:\n",
    "        '''runs RFDiffusion for you on acluster.'''\n",
    "        # setup directory\n",
    "        self.increment_attribute(\"diffusion_runs\")\n",
    "        prefix = prefix or f\"diffusion_run_{str(self.diffusion_runs).zfill(4)}\"\n",
    "        rfdiff_dir = f\"{self.dir}/{prefix}\"\n",
    "\n",
    "        # run diffusion\n",
    "        rfdiff_output = run_rfdiffusion(self.poses_df[\"poses\"].to_list(), work_dir=rfdiff_dir, options=options, pose_options=pose_options, scorefile=\"rfdiffusion_scores.json\", max_gpus=self.max_diffusion_gpus)\n",
    "\n",
    "        # update poses_df and poses\n",
    "        scores = rfdiff_output[\"scores\"]\n",
    "        scores = scores.add_prefix(prefix + \"_\")\n",
    "        self.poses_df = update_df(scores, f\"{prefix}_description\", self.poses_df, \"poses_description\", new_df_col_remove_layer=1)\n",
    "        self.poses_df.loc[:, \"poses\"] = self.poses_df[f\"{prefix}_location\"]\n",
    "        self.poses_df.loc[:, \"poses_description\"] = self.poses_df[f\"{prefix}_description\"]\n",
    "        self.poses_df.to_json(self.scorefile)\n",
    "        self.poses = list(self.poses_df[\"poses\"])\n",
    "\n",
    "        return rfdiff_output[\"pdb_dir\"]\n",
    "    \n",
    "    def update_motif_res_mapping(self, motif_col: str, inpaint_prefix: str) -> None:\n",
    "        '''AAA'''\n",
    "        # check if prefix occurs in DF\n",
    "        if not col_with_prefix_exists_in_df(self.poses_df, motif_col): raise KeyError(f\"Prefix {motif_col} not found in poses_df. Available columns: {', '.join(self.poses_df.columns)}\")\n",
    "        if not col_with_prefix_exists_in_df(self.poses_df, inpaint_prefix): raise KeyError(f\"Prefix {inpaint_prefix} not found in poses_df. Available columns: {', '.join(self.poses_df.columns)}\")\n",
    "        \n",
    "        self.poses_df[motif_col] = [reassign_motif(motif, ref_pdb_idx, hal_pdb_idx) for motif, ref_pdb_idx, hal_pdb_idx in zip(list(self.poses_df[motif_col]), list(self.poses_df[f\"{inpaint_prefix}_con_ref_pdb_idx\"]), list(self.poses_df[f\"{inpaint_prefix}_con_hal_pdb_idx\"]))]\n",
    "        return None\n",
    "    \n",
    "    def update_res_identities(self, identity_col: str, inpaint_prefix: str) -> None:\n",
    "        '''AAA'''\n",
    "        # check if prefix occurs in DF\n",
    "        if not col_with_prefix_exists_in_df(self.poses_df, identity_col): raise KeyError(f\"Prefix {identity_col} not found in poses_df. Available columns: {', '.join(self.poses_df.columns)}\")\n",
    "        if not col_with_prefix_exists_in_df(self.poses_df, inpaint_prefix): raise KeyError(f\"Prefix {inpaint_prefix} not found in poses_df. Available columns: {', '.join(self.poses_df.columns)}\")\n",
    "    \n",
    "        self.poses_df[identity_col] = [reassign_identity_keys(motif, ref_pdb_idx, hal_pdb_idx) for motif, ref_pdb_idx, hal_pdb_idx in zip(list(self.poses_df[identity_col]), list(self.poses_df[f\"{inpaint_prefix}_con_ref_pdb_idx\"]), list(self.poses_df[f\"{inpaint_prefix}_con_hal_pdb_idx\"]))]    \n",
    "        return None\n",
    "    \n",
    "    def check_inpaint_chainbreak(self, inpaint_prefix: str) -> None:\n",
    "        '''\n",
    "        '''\n",
    "        # check if prefix occurs in DF\n",
    "        if not col_with_prefix_exists_in_df(self.poses_df, inpaint_prefix): raise KeyError(f\"Prefix {inpaint_prefix} not found in poses_df. Available columns: {', '.join(self.poses_df.columns)}\")\n",
    "        \n",
    "        # add chainbreak value to poses_df       \n",
    "        self.poses_df[f\"{inpaint_prefix}_chainbreak\"] = [chainbreak_tools.search_chainbreak_in_pdb(relaxed_pose, chainbreak_tools.get_linker_contig(pose.replace(\".pdb\", \".trb\"))) for pose, relaxed_pose in zip(list(self.poses_df[f\"{inpaint_prefix}_location\"]), list(self.poses_df[f\"{inpaint_prefix}_trf_relax_location\"]))]\n",
    "\n",
    "        self.poses_df.to_json(self.scorefile)\n",
    "        return None\n",
    "    \n",
    "    def thread_sequences(self, template_dir:str, remove_layers:int=1, prefix:str=None, scripts_version=\"rosetta_scripts.default.linuxgccrelease\") -> str:\n",
    "        '''\n",
    "        Threads generated sequences onto poses found in <template_dir>\n",
    "        Args:\n",
    "            <template_dir>\n",
    "            <remove_layers>\n",
    "            <prefix>\n",
    "            \n",
    "        Returns:\n",
    "            Directory where threaded poses are stored.\n",
    "            \n",
    "        '''\n",
    "        def threading_read_pose_seq(path):\n",
    "            with open(path, 'r') as f:\n",
    "                return \"\".join([line.strip() for line in f.readlines()[1:]])\n",
    "        \n",
    "        # setup directory and prefix\n",
    "        self.increment_attribute(\"threading_runs\")\n",
    "        prefix = prefix or f\"threading_run_{str(self.threading_runs).zfill(4)}\"\n",
    "        thread_dir = f\"{self.dir}/{prefix}\"\n",
    "        if not os.path.isdir(thread_dir): os.makedirs(thread_dir)\n",
    "        os.makedirs((ros_dir := f\"{thread_dir}/raw/\"), exist_ok=True)\n",
    "        \n",
    "        # copy templates into temp_thread_dir\n",
    "        os.makedirs((temp_thread_dir := f\"{thread_dir}/temp_thread_dir\"), exist_ok=True)\n",
    "        poses = [x.split(\"/\")[-1] for x in self.poses]\n",
    "        if remove_layers:\n",
    "            copylist = [(f\"{template_dir}/{'_'.join(pose.split('_')[:-1*remove_layers])}.pdb\", f\"{temp_thread_dir}/{pose.replace('.fa', '.pdb')}\") for pose in poses]\n",
    "        else:\n",
    "            copylist = [(f\"{template_dir}/{pose.replace('.fa', '.pdb')}\", f\"{temp_thread_dir}/{pose.replace('.fa', '.pdb')}\") for pose in poses]\n",
    "        \n",
    "        for k, v in copylist:\n",
    "            shutil.copy(k, v)\n",
    "    \n",
    "        # parse pose_options (sequences to thread)\n",
    "        poses_pdb = [f\"{temp_thread_dir}/{pose.replace('.fa', '.pdb')}\" for pose in poses]\n",
    "        pose_options = [{\"parser:script_vars\": f\"seq={threading_read_pose_seq(pose)}\"} for pose in self.poses]\n",
    "        \n",
    "        # run_rosetta on templates with pose_sequences\n",
    "        thread_output = run_rosetta(poses=poses_pdb, rosetta_executable=scripts_version, rosetta_options={\"parser:protocol\": f\"{_scripts_path}/thread_repack.xml\"}, \n",
    "                                    rosetta_flags=[\"beta\", \"ex1\", \"ex2\"], pose_options=pose_options, pose_flags=[[] for x in pose_options], n=1, work_dir=ros_dir, \n",
    "                                    scorefile=\"rosetta_scores.sc\", max_cpus=self.max_rosetta_cpus)\n",
    "        \n",
    "        # update self.poses and self.poses_df attributes\n",
    "        scores = thread_output[\"scores\"]\n",
    "        self.poses = list(scores[\"location\"])\n",
    "        scores = scores.add_prefix(prefix+\"_\")\n",
    "        self.poses_df = update_df(scores, f\"{prefix}_description\", self.poses_df, \"poses_description\", new_df_col_remove_layer=1)\n",
    "        self.poses_df.loc[:, \"poses\"] = self.poses_df[f\"{prefix}_location\"]\n",
    "        self.poses_df.loc[:, \"poses_description\"] = self.poses_df[f\"{prefix}_description\"]\n",
    "        self.poses_df.to_json(self.scorefile)\n",
    "        \n",
    "        # remove index layer that was added by Rosetta\n",
    "        self.reindex_poses(out_dir=thread_dir, remove_layers=1, force_reindex=False)\n",
    "        shutil.rmtree(temp_thread_dir)\n",
    "        \n",
    "        return thread_output[\"pdb_dir\"]\n",
    "\n",
    "    # ----------------------------------Poses Metrics -----------------------------------------------\n",
    "    \n",
    "    def calc_metric(self, metric_function, metric_prefix: str, metric_args=[], metric_kwargs=None, pose_col=\"description\") -> None:\n",
    "        '''\n",
    "        Calculates a metric using <metric_function> that is given <metric_kwargs>\n",
    "        calc_metric will update self.poses_df by merging calc_metric scores with self.poses_df on <metric_prefix>_<pose_col>\n",
    "        \n",
    "        Args:\n",
    "            <metric_function>:   Has to be a function that takes it's arguments in this form: ([path_to_structures], {options})\n",
    "                                 Additionally it has to return a pd.DataFrame containing metric scores, where <pose_col>\n",
    "                                 identifies the structures (\"description\")\n",
    "            <metric_prefix>:     Prefix that will be added to the scores calculated by metric_function when merged with self.poses_df\n",
    "        '''\n",
    "        # calculate metric\n",
    "        metric_args = [self.poses] + metric_args\n",
    "        metric_kwargs = metric_kwargs or {}\n",
    "        scores = metric_function(*metric_args, **metric_kwargs)\n",
    "        \n",
    "        # add metric_prefix to scores and update self.poses_df\n",
    "        scores = scores.add_prefix(metric_prefix + \"_\")\n",
    "        self.poses_df = update_df(scores, f\"{metric_prefix}_{pose_col}\", self.poses_df, \"poses_description\", new_df_col_remove_layer=0)\n",
    "        self.poses_df.to_json(self.scorefile)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def calc_bb_rmsd_poses_df(self, ref_pdb_col: str, metric_prefix):\n",
    "        '''\n",
    "        Calculates bb_ca_rmsd of latest poses in self.poses_df to \"ref_pdb_col\" (which should be e.g. predict_run_0001_location)\n",
    "        '''\n",
    "        # Check for ref_pdb_col in self.poses_df\n",
    "        location_cols = [x for x in self.poses_df.columns if x.endswith(\"location\")]\n",
    "        if ref_pdb_col not in self.poses_df.columns:\n",
    "            # check if any column in self.poses_df contains <score_col> and if so, return the column that has the hightest number in it (for example mpnn_run_0002)\n",
    "            if any([x for x in location_cols if ref_pdb_col in x]):\n",
    "                score_col = sorted([x for x in list(location_cols) if ref_pdb_col in x])[-1]\n",
    "                print(f\"Calculating RMSD to poses in column {score_col}\")\n",
    "            else:\n",
    "                raise KeyError(f\"ERROR: Scoreterm {ref_pdb_col} not found in DataFrame. Available Scoreterms: {', '.join(list(location_cols))}\")\n",
    "        else:\n",
    "            print(f\"Calculating RMSD to poses in column {ref_pdb_col}\")\n",
    "            score_col = ref_pdb_col\n",
    "        \n",
    "        # calculate RMSDs\n",
    "        rmsds = [bb_rmsd.superimpose_calc_rmsd(ref_pdb, pose, atoms=\"CA\") for ref_pdb, pose in zip(self.poses_df[score_col], self.poses_df[\"poses\"])]\n",
    "        self.poses_df.loc[:, f\"{metric_prefix}_bb_ca_rmsd\"] = rmsds\n",
    "        self.poses_df.to_json(self.scorefile)\n",
    "        \n",
    "        return rmsds\n",
    "    \n",
    "    def calc_bb_rmsd_dir(self, ref_pdb_dir: str, metric_prefix: str, ref_chains=None, pose_chains=None, remove_layers=None, layer_separator=\"_\") -> list:\n",
    "        '''\n",
    "        Calculates bb_rmsd of all poses to their matches in ref_pdb_dir after removal of <remove_layers> index layers (_0001).\n",
    "        '''\n",
    "        print(f\"Calculating RMSDs of poses to reference poses in {ref_pdb_dir}.\\nRMSD will be stored as {metric_prefix}_bb_ca_rmsd in the poses_df.\")\n",
    "        # set metric name:\n",
    "        metric_name = f\"{metric_prefix}_bb_ca_rmsd\"\n",
    "        if metric_name in self.poses_df.columns:\n",
    "            print(f\"WARNING: Metric {metric_name} already found in poses_df! Skipping calculation! (Check if you set the same prefix twice!!)\")\n",
    "            return list(self.poses_df[metric_name])\n",
    "        \n",
    "        # go from poses to ref_poses by removing layers and adding the directory ref_pdb_dir/\n",
    "        if remove_layers:\n",
    "            layers_removed = self.poses_df[\"poses_description\"].str.split(layer_separator).str[:-1*remove_layers].str.join(layer_separator)\n",
    "            ref_pdbs = [description + \".pdb\" for description in layers_removed]\n",
    "        else:\n",
    "            ref_pdbs = [x+\".pdb\" for x in list(self.poses_df[\"poses_description\"])]\n",
    "        ref_pdbs = [\"/\".join([ref_pdb_dir, pdb]) for pdb in ref_pdbs]\n",
    "        \n",
    "        # if rmsds are already calculated, read them from rmsd scorefile. Otherwise, calc RMSDs.\n",
    "        if os.path.isfile((scorefile := f\"{self.scores_dir}/{metric_name}_scores.json\")):\n",
    "            rmsd_df = pd.read_json(scorefile)\n",
    "            self.poses_df = self.poses_df.merge(rmsd_df, on=\"poses_description\")\n",
    "            rmsds = list(self.poses_df[metric_name])\n",
    "        else:\n",
    "            rmsds = [bb_rmsd.superimpose_calc_rmsd(ref_pdb, pose, ref_chains=ref_chains, pose_chains=pose_chains, atoms=\"CA\") for ref_pdb, pose in zip(ref_pdbs, list(self.poses_df[\"poses\"]))]\n",
    "            pd.DataFrame({\"poses_description\": list(self.poses_df[\"poses_description\"]), metric_name: rmsds}).to_json(scorefile)\n",
    "            self.poses_df.loc[:, metric_name] = rmsds\n",
    "\n",
    "        self.poses_df.to_json(self.scorefile)\n",
    "        \n",
    "        return rmsds\n",
    "    \n",
    "    def calc_motif_heavy_rmsd_dir(self, ref_pdb_dir: str, ref_motif: dict, target_motif: dict, metric_prefix: str, remove_layers:int=1, layer_separator=\"_\") -> list:\n",
    "        '''\n",
    "            Calculates rmsd of current poses (of specified <atoms>) of a specified motif to pdbs found in <ref_pdb_dir>.\n",
    "            Name matching is accomplished by removing index layers (<remove_layer>).\n",
    "            Args:\n",
    "                <ref_pdb_dir>            Path to the directory containing the reference pdbs\n",
    "                <ref_motif>              Dictionary specifying the motif residues: {\"chain\": [res, ...], ...}\n",
    "                <target_motif>           Dictionary specifying the target residues: {\"chain\": [res, ...], ...} Should be same number of residues as <ref_motif> \n",
    "                <metric_prefix>          Prefix that should be added to the metric in the poses_df\n",
    "                <remove_layers>          How many index layers (_0001) to remove to reach the ref_pdb names in <ref_pdb_dir> from poses\n",
    "                <layer_separator>        Separator of index layers\n",
    "\n",
    "            Return:\n",
    "                List of Motif RMSDS calculated for the poses\n",
    "        '''\n",
    "        # define metric name:\n",
    "        metric_name = f\"{metric_prefix}_motif_heavy_rmsd\"\n",
    "        if metric_name in self.poses_df.columns:\n",
    "            print(f\"WARNING: Metric {metric_name} already found in poses_df! Skipping calculation! (Check if you set the same prefix twice!!)\")\n",
    "            return list(self.poses_df[metric_name])\n",
    "        \n",
    "        # check if ref_motif and target_motif are either list or dictionary and return rmsd_mode:\n",
    "        if type(ref_motif) == dict and type(target_motif) == dict:\n",
    "            pose_opts = False\n",
    "        elif type(ref_motif) == list and type(target_motif) == list:\n",
    "            pose_opts = True\n",
    "        \n",
    "         # go from poses to ref_poses by removing layers and adding the directory ref_pdb_dir/\n",
    "        ref_poses = self.poses_df[\"poses_description\"].str.split(layer_separator).str[:-1*remove_layers].str.join(layer_separator) if remove_layers else self.poses_df[\"poses_description\"]\n",
    "        \n",
    "        # collect list of reference pdbfiles\n",
    "        ref_pdbs = [description + \".pdb\" for description in ref_poses]\n",
    "        ref_pdb_list = [\"/\".join([ref_pdb_dir, pdb]) for pdb in ref_pdbs]\n",
    "        \n",
    "        # calculate rmsds\n",
    "        if os.path.isfile((scorefile := f\"{self.scores_dir}/{metric_name}_scores.json\")):\n",
    "            print(f\"Motif Heavy RMSDs found at {scorefile} Reading RMSDs directly from file.\")\n",
    "            rmsd_df = pd.read_json(scorefile)\n",
    "            self.poses_df = self.poses_df.merge(rmsd_df, on=\"poses_description\")\n",
    "            if len(self.poses_df) == 0: raise ValueError(\"ERROR: Length of DataFrame = 0. DataFrame merging failed!\")\n",
    "            rmsds = list(self.poses_df[metric_name])\n",
    "        else:\n",
    "            if pose_opts:\n",
    "                rmsds = [motif_rmsd.superimpose_calc_motif_rmsd_heavy(ref_pdb, target_pdb, ref_selection=e_ref_motif, target_selection=e_target_motif) for ref_pdb, target_pdb, e_ref_motif, e_target_motif in zip(ref_pdb_list, list(self.poses_df[\"poses\"]), ref_motif, target_motif)]\n",
    "            else:\n",
    "                rmsds = [motif_rmsd.superimpose_calc_motif_rmsd_heavy(ref_pdb, target_pdb, ref_selection=ref_motif, target_selection=target_motif) for ref_pdb, target_pdb in zip(ref_pdb_list, self.poses_df[\"poses\"])]\n",
    "            pd.DataFrame({\"poses_description\": list(self.poses_df[\"poses_description\"]), metric_name: rmsds}).to_json(scorefile)\n",
    "            self.poses_df.loc[:, metric_name] = rmsds\n",
    "\n",
    "        # add rmsds to self.poses_df\n",
    "        self.poses_df.to_json(self.scorefile)\n",
    "\n",
    "        return rmsds\n",
    "    \n",
    "    def calc_motif_bb_rmsd_dir(self, ref_pdb_dir: str, ref_motif: dict, target_motif: dict, metric_prefix: str, atoms:list[str]=[\"CA\"], remove_layers:int=1, layer_separator:str=\"_\") -> list:\n",
    "        '''\n",
    "        Calculates rmsd of current poses (of specified <atoms>) of a specified motif to pdbs found in <ref_pdb_dir>.\n",
    "        Name matching is accomplished by removing index layers (<remove_layer>).\n",
    "        Args:\n",
    "            <ref_pdb_dir>            Path to the directory containing the reference pdbs\n",
    "            <ref_motif>              Dictionary specifying the motif residues: {\"chain\": [res, ...], ...}\n",
    "            <target_motif>           Dictionary specifying the target residues: {\"chain\": [res, ...], ...} Should be same number of residues as <ref_motif> \n",
    "            <metric_prefix>          Prefix that should be added to the metric in the poses_df\n",
    "            <atoms>                  List of atoms for which to calculate RMSD with\n",
    "            <remove_layers>          How many index layers (_0001) to remove to reach the ref_pdb names in <ref_pdb_dir> from poses\n",
    "            <layer_separator>        Separator of index layers\n",
    "        \n",
    "        Return:\n",
    "            List of Motif RMSDS calculated for the poses\n",
    "        '''\n",
    "        # set metric name\n",
    "        metric_name = f\"{metric_prefix}_motif_rmsd\"\n",
    "        if metric_name in self.poses_df.columns:\n",
    "            print(f\"WARNING: Metric {metric_name} already found in poses_df! Skipping calculation! (Check if you set the same prefix twice!!)\")\n",
    "            return list(self.poses_df[metric_name])\n",
    "        \n",
    "        # check if ref_motif and target_motif are either list or dictionary and return rmsd_mode:\n",
    "        if type(ref_motif) == dict and type(target_motif) == dict:\n",
    "            pose_opts = False\n",
    "        elif type(ref_motif) == list and type(target_motif) == list:\n",
    "            pose_opts = True\n",
    "        else:\n",
    "            raise RuntimeError(f\"Parameters ref_motif and target_motif have to be of the same type (either list, or dict).\\nType(ref_motif): {type(ref_motif)}\\nType(target_motif): {type(target_motif)}\")\n",
    "        \n",
    "        # go from poses to ref_poses by removing layers and adding the directory ref_pdb_dir/\n",
    "        ref_poses = self.poses_df[\"poses_description\"].str.split(layer_separator).str[:-1*remove_layers].str.join(layer_separator) if remove_layers else self.poses_df[\"poses_description\"]\n",
    "\n",
    "        # collect list of reference pdbfiles\n",
    "        ref_pdbs = [description + \".pdb\" for description in ref_poses]\n",
    "        ref_pdb_list = [\"/\".join([ref_pdb_dir, pdb]) for pdb in ref_pdbs]\n",
    "        \n",
    "        # calculate rmsds\n",
    "        if os.path.isfile((scorefile := f\"{self.scores_dir}/{metric_name}_scores.json\")):\n",
    "            rmsd_df = pd.read_json(scorefile)\n",
    "            self.poses_df = self.poses_df.merge(rmsd_df, on=\"poses_description\")\n",
    "            rmsds = self.poses_df[metric_name]\n",
    "        else:\n",
    "            if pose_opts:\n",
    "                rmsds = [motif_rmsd.superimpose_calc_motif_rmsd(ref_pdb, target_pdb, ref_selection=e_ref_motif, target_selection=e_target_motif, atoms=atoms) for ref_pdb, target_pdb, e_ref_motif, e_target_motif in zip(ref_pdb_list, self.poses_df[\"poses\"], ref_motif, target_motif)]\n",
    "            else:\n",
    "                rmsds = [motif_rmsd.superimpose_calc_motif_rmsd(ref_pdb, target_pdb, ref_selection=ref_motif, target_selection=target_motif, atoms=atoms) for ref_pdb, target_pdb in zip(ref_pdb_list, self.poses_df[\"poses\"])]\n",
    "            pd.DataFrame({\"poses_description\": list(self.poses_df[\"poses_description\"]), metric_name: rmsds}).to_json(scorefile)\n",
    "            self.poses_df.loc[:, metric_name] = rmsds\n",
    "            \n",
    "        # add rmsds to self.poses_df\n",
    "        self.poses_df.to_json(self.scorefile)\n",
    "\n",
    "        return rmsds\n",
    "    \n",
    "    # ------------------------- Superimposition tools -------------------------------------------------\n",
    "    \n",
    "    def add_ligand_from_ref(self, ref_col: str, ref_motif, target_motif=None, atoms:list=None, lig_chain=\"X\", prefix=None) -> None:\n",
    "        '''\n",
    "        Superimpose a ligand chain from a reference PDB structure onto a motif in a pose PDB structure, and add the ligand chain to the pose.\n",
    "    \n",
    "        The motifs are specified as dictionaries where the keys are the IDs of the chains in the PDB structures, and the values are lists of residue numbers. The function will use atoms from the specified residues in the specified chains to perform the superimposition.\n",
    "    \n",
    "        The function modifies the poses in-place.\n",
    "        \n",
    "        Behavior of Arguments ref_motif and target_motif: \n",
    "            if a string is passed, the function will look up self.poses_df[ref_motif] to collect the list of motifs from the poses_df.\n",
    "            if a list is passed, the function will use the list as is, and it assumes a list of dictionaries (motifs).\n",
    "            if a dictionary is passed, the function will use this dictionary for every pose in the Poses object.\n",
    "            if target_motif is not passed, the motif specified in ref_motif will be used.\n",
    "    \n",
    "        Args:\n",
    "            ref_col (str): The name of the column in the `self.poses_df` dataframe containing the reference PDB file paths.\n",
    "            ref_motif (str, list, or dict): A nested dictionary specifying the chains and residues of the atoms in the reference structure to use for the superimposition.\n",
    "            target_motif (str, list or dict, optional): A nested dictionary specifying the chains and residues of the atoms in the pose structure to use for the superimposition. If not provided, the `ref_motif` will be used for the pose as well.\n",
    "            atoms (list, optional): A list of atom names to use for the superimposition. If not provided, all atoms in the specified residues will be used.\n",
    "            lig_chain (str, optional): The ID of the chain in the reference PDB file to be added to the pose. Defaults to \"X\".\n",
    "            prefix (str, optional): The name of the directory in which the new poses should be stored into. Defaults to 'add_chain_0001'\n",
    "\n",
    "        Returns:\n",
    "            Path to directory where the new poses are stored.\n",
    "        '''\n",
    "        # setup prefix and directory\n",
    "        prefix = prefix or f\"add_ligand_{str(self.increment_attribute('add_ligand')).zfill(4)}\"\n",
    "        \n",
    "        # if adding of ligand is already done, skip:\n",
    "        if os.path.isdir((lig_dir := f\"{self.dir}/{prefix}\")):\n",
    "            if glob(f\"{lig_dir}/*.pdb\"): \n",
    "                self.new_poses_path(new_path=lig_dir)\n",
    "                return None\n",
    "            else:\n",
    "                print(f\"WARNING: No files found at {lig_dir}, even though the directory exist. This might hint at a previous buggy run!\")\n",
    "            \n",
    "        # create directory\n",
    "        os.makedirs(lig_dir, exist_ok=True)\n",
    "        \n",
    "        # parse motifs\n",
    "        pose_motif = target_motif or ref_motif \n",
    "        \n",
    "        # parse reference pdbs\n",
    "        ref_l = self.poses_df[ref_col]\n",
    "        \n",
    "        # parse motif options into list\n",
    "        ref_motif_l = parse_motif(self.poses_df, ref_motif)\n",
    "        pose_motif_l = parse_motif(self.poses_df, pose_motif)\n",
    "        if len(ref_motif_l) != len(pose_motif_l): raise ValueError(f\"ERROR: ref_motif_l ({len(ref_motif_l)}) and pose_motif_l ({len(pose_motif_l)}) are not of the same length.\\nref_motif_l: {ref_motif_l}\\npose_motif_l: {pose_motif_l}.\")\n",
    "        \n",
    "        # go through each pose in self.poses_df[\"poses\"]\n",
    "        for pose, ref, r_motif, p_motif in zip(list(self.poses_df[\"poses\"]), ref_l, ref_motif_l, pose_motif_l):\n",
    "            # create path to new pose\n",
    "            new_pose = f\"{lig_dir}/{pose.split('/')[-1]}\"\n",
    "            \n",
    "            # run superimposition\n",
    "            superimposition_tools.superimpose_add_chain_by_motif(ref, pose, lig_chain, r_motif, p_motif, new_pose_path=new_pose, superimpose_atoms=atoms)\n",
    "        print(f\"Added ligand from reference specified in {ref_col} into poses.\")\n",
    "        \n",
    "        # add new location of poses to self.poses_df and self.poses\n",
    "        self.new_poses_path(new_path=lig_dir)\n",
    "        \n",
    "        return lig_dir\n",
    "            \n",
    "    def add_chain_from_ref(self, ref_col, copy_chain, superimpose_chain=\"A\", prefix=None) -> str:\n",
    "        '''\n",
    "        Superimpose a chain from a reference PDB structure onto a pose PDB structure, and add the chain to the pose.\n",
    "        The function uses the alpha-carbon (CA) atoms of the two chains to perform the superimposition\n",
    "        The function modifies the poses in-place.\n",
    "\n",
    "        Args:\n",
    "            ref_col (str): The name of the column in the `self.poses_df` dataframe containing the reference PDB file paths.\n",
    "            copy_chain (str): The ID of the chain in the reference PDB file to be added to the pose.\n",
    "            superimpose_chain (str, optional): The ID of the chain in the pose PDB file to use for the superimposition. Defaults to \"A\".\n",
    "            prefix (str, optional): The name of the directory in which the new poses should be stored into. Defaults to 'add_chain_0001'.\n",
    "\n",
    "        Returns:\n",
    "            Path to directory where the new poses are stored.\n",
    "\n",
    "        Examples:\n",
    "            add_chain_from_ref(\"ref_poses\", \"B\", superimpose_chain=\"C\", prefix=\"add_chain_B\")\n",
    "        '''\n",
    "        # setup prefix and directory\n",
    "        prefix = prefix or f\"add_chain_{str(self.increment_attribute('add_chain')).zfill(4)}\"\n",
    "                \n",
    "        # if adding of ligand is already done, skip:\n",
    "        if os.path.isdir((lig_dir := f\"{self.dir}/{prefix}\")):\n",
    "            if glob(f\"{lig_dir}/*.pdb\"): \n",
    "                self.new_poses_path(new_path=lig_dir)\n",
    "                return None\n",
    "            else:\n",
    "                print(f\"WARNING: No files found at {lig_dir}, even though the directory exist. This might hint at a previous buggy run!\")\n",
    "        \n",
    "        # create directory\n",
    "        os.makedirs(lig_dir, exist_ok=True)\n",
    "        \n",
    "        # iterate over poses and add chain\n",
    "        for pose, ref in zip(list(self.poses_df[\"poses\"]), self.poses_df[ref_col]):\n",
    "            new_pose = f\"{lig_dir}/{pose.split('/')[-1]}\"\n",
    "            superimposition_tools.superimpose_add_chain(ref, pose, copy_chain, superimpose_chain=superimpose_chain, new_pose_path=new_pose)\n",
    "        print(f\"Added chain {copy_chain} from reference specified in {ref_col} into poses.\")\n",
    "        \n",
    "        # add new location of poses to self.poses_df and self.poses\n",
    "        self.new_poses_path(new_path=lig_dir)\n",
    "        \n",
    "        return lig_dir\n",
    "    \n",
    "    def remove_chain_from_poses(self, remove_chain, prefix=None) -> str:\n",
    "        '''\n",
    "        Superimpose a chain from a reference PDB structure onto a pose PDB structure, and add the chain to the pose.\n",
    "        The function uses the alpha-carbon (CA) atoms of the two chains to perform the superimposition\n",
    "        The function modifies the poses in-place.\n",
    "\n",
    "        Args:\n",
    "            ref_col (str): The name of the column in the `self.poses_df` dataframe containing the reference PDB file paths.\n",
    "            copy_chain (str): The ID of the chain in the reference PDB file to be added to the pose.\n",
    "            superimpose_chain (str, optional): The ID of the chain in the pose PDB file to use for the superimposition. Defaults to \"A\".\n",
    "            prefix (str, optional): The name of the directory in which the new poses should be stored into. Defaults to 'add_chain_0001'.\n",
    "\n",
    "        Returns:\n",
    "            Path to directory where poses are stored.\n",
    "\n",
    "        Examples:\n",
    "            add_chain_from_ref(\"ref_poses\", \"B\", superimpose_chain=\"C\", prefix=\"add_chain_B\")\n",
    "        '''\n",
    "        # setup prefix and directory\n",
    "        prefix = prefix or f\"remove_chain_{str(self.increment_attribute('remove_chain')).zfill(4)}\"\n",
    "                \n",
    "        # if removing of chain is already done, skip:\n",
    "        if os.path.isdir((pdb_dir := f\"{self.dir}/{prefix}\")):\n",
    "            if glob(f\"{pdb_dir}/*.pdb\"): \n",
    "                self.new_poses_path(new_path=pdb_dir)\n",
    "                return None\n",
    "            else:\n",
    "                print(f\"WARNING: No files found at {pdb_dir}, even though the directory exist. This might hint at a previous buggy run!\")\n",
    "        \n",
    "        # create directory\n",
    "        os.makedirs(pdb_dir, exist_ok=True)\n",
    "        \n",
    "        # iterate over poses and remove chain\n",
    "        for pose in list(self.poses_df[\"poses\"]):\n",
    "            new_pose = f\"{pdb_dir}/{pose.split('/')[-1]}\"\n",
    "            superimposition_tools.remove_chain(pose, remove_chain, new_pose_path=new_pose)\n",
    "        print(f\"Removed chain {remove_chain} from poses.\")\n",
    "        \n",
    "        # add new location of poses to self.poses_df and self.poses\n",
    "        self.new_poses_path(new_path=pdb_dir)\n",
    "        \n",
    "        return pdb_dir\n",
    "        \n",
    "    \n",
    "    # ------------------------- Poses Misc Functions --------------------------------------------------\n",
    "    \n",
    "    def calc_composite_score(self, name: str, scoreterms, weights):\n",
    "        '''\n",
    "        Calculates a composite score named <name> from <scoreterms> weighted by <weights>.\n",
    "        Adds new scoreterm as attribute to self.poses_df.\n",
    "        <name> has to be a unique name that does not exists yet in poses_df!\n",
    "        If <name> already exists in DataFrame, then calc_composite_score skips the calculation!\n",
    "        \n",
    "        Args:\n",
    "            <name>: Name of the composite_score that you want to calculate and add to your DataFrame.\n",
    "            <scoreterms>: Scoreterms from which to calculate the composite_score.\n",
    "            <weights>: Weights to give to <scoreterms>. Weights has to be same length as <scoreterms>!\n",
    "        \n",
    "        Returns composite score\n",
    "        '''\n",
    "        # TODO: implement scoreterm lookup!\n",
    "        \n",
    "        # check if <name> exists in poses_df.columns\n",
    "        if name in self.poses_df.columns: return print(f\"WARNING: Scoreterm {name} already exists in poses DataFrame. Skipping Calculation.\")\n",
    "        \n",
    "        # calculate composite score\n",
    "        configs = \"\\n\".join([f\"{score}: {weight}\" for score, weight in zip(scoreterms, weights)])\n",
    "        print(f\"\\nCalculating composite score {name} with settings:\\n{configs}\\n\")\n",
    "        new_df = calc_composite_score.calc_composite_score(self.poses_df, name, scoreterms, weights)\n",
    "        \n",
    "        # update self.poses_df\n",
    "        self.poses_df = new_df\n",
    "        self.poses_df.to_json(self.scorefile)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def update_poses(self) -> None:\n",
    "        '''\n",
    "        Filters the self.poses object down to the poses present in self.poses_df[\"poses_description\"].\n",
    "\n",
    "        Goes through each pose in self.poses and checks if any description in self.poses_df[\"poses_description\"] fits into pose. \n",
    "        Keeps the pose, if it finds a fitting description in poses_df.\n",
    "        '''\n",
    "        len_before = len(self.poses)\n",
    "        self.poses = [pose for pose in self.poses if any([description in pose for description in list(self.poses_df[\"poses_description\"])])]\n",
    "        print(f\"Reducing number of poses from {str(len_before)} to {str(len(self.poses))}.\\n\")\n",
    "        return None\n",
    "    \n",
    "    def dump_poses(self, out_path: str) -> None:\n",
    "        '''\n",
    "        Stores poses at <path>, returns <path>.\n",
    "        '''\n",
    "        # check if directory exists:\n",
    "        if not os.path.isdir(out_path): os.makedirs(out_path, exist_ok=True)\n",
    "            \n",
    "        # copy poses into out_path\n",
    "        print(f\"Copying {len(self.poses)} poses into {out_path}\")\n",
    "        for pose in self.poses:\n",
    "            loc = f'{out_path}/{pose.split(\"/\")[-1]}'\n",
    "            run(f\"cp {pose} {loc}\", shell=True, stdout=True, stderr=True, check=True)\n",
    "        \n",
    "        return out_path\n",
    "    \n",
    "    def set_poses(self, poses: list[str], scores=None, prefix=None, description_col=\"description\", remove_layers=0, sep=\"_\") -> list[str]:\n",
    "        '''\n",
    "        Sets a list of <poses> as the new poses of the object. Updates the poses in the DataFrame and fills up previous scores.\n",
    "        Also adds scores if needed.\n",
    "        \n",
    "        Args:\n",
    "            <poses>               List with paths to where the poses are stored.\n",
    "            <scores>              Optional: pd.DataFrame containing the scores \n",
    "            <prefix>              Optional: Prefix that will be added to every score of the added pd.DataFrame (<scores>). Default: 'add_0001_'\n",
    "            <description_col>     Column that contains the 'index' by which to merge scores with poses_df\n",
    "            <remove_layers>       Number of index layers to remove to reach the names of 'poses_description' in self.poses_df\n",
    "        \n",
    "        Returns:\n",
    "            List of Poses.\n",
    "        '''\n",
    "        # if scores is not given, create pd.DataFrame with 'new_description' column\n",
    "        scores = scores or pd.DataFrame({description_col: [pose.split(sep)[-1].split(\".\")[0] for pose in poses]})\n",
    "        scores.loc[:, \"location\"] = poses\n",
    "        prefix = prefix or f\"add_{str(self.set_poses_number).zfill(4)}\"\n",
    "        \n",
    "        # update poses_df based on scores\n",
    "        scores = scores.add_prefix(prefix + \"_\")\n",
    "        self.poses_df = update_df(scores, f\"{prefix}_{description_col}\", self.poses_df, \"poses_description\", new_df_col_remove_layer=remove_layers)\n",
    "        \n",
    "        # update essential columns in poses_df\n",
    "        self.poses_df.loc[:, \"poses\"] = self.poses_df[f\"{prefix}_location\"]\n",
    "        self.poses_df.loc[:, \"poses_description\"] = self.poses_df[f\"{prefix}_description\"]\n",
    "        \n",
    "        # if argument self.auto_dump_df is set to True, dump the new poses_df in self.dir\n",
    "        if self.auto_dump_df: self.poses_df.to_json(self.scorefile)\n",
    "        \n",
    "        # set self.poses and return poses\n",
    "        self.poses = list(self.poses_df[f\"{prefix}_location\"])\n",
    "\n",
    "        return self.poses\n",
    "    \n",
    "    def new_poses_path(self, new_path: str) -> None:\n",
    "        '''\n",
    "        !!!!!!! WIP !!!!!!!!\n",
    "        \n",
    "        Sets a new location for the current poses: resets self.poses and self.poses_df[\"poses\"]]. Everything else remains unchanged.\n",
    "        This function is intended to be used after small changes were made to the poses that did not require renaming.\n",
    "        \n",
    "        '''\n",
    "        old_path_l = list(self.poses_df[\"poses\"].str.split(\"/\").str[:-1].str.join(\"/\").unique())\n",
    "        \n",
    "        # sanity checks\n",
    "        if len(old_path_l) > 1: raise ValueError(f\"ERROR: Your poses are stored at multiple, different locations. Method new_poses_path() is only to be used with one unique location for all poses.\")\n",
    "        if not os.path.isdir(os.path.abspath(new_path)): raise FileNotFoundError(f\"ERROR: Directory {new_path} does not exist! Are you sure you specified the correct path?\")\n",
    "        \n",
    "        # update poses\n",
    "        self.poses_df.loc[:, \"poses\"] = [x.replace(old_path_l[0], new_path) for x in list(self.poses_df[\"poses\"])]\n",
    "        self.poses = list(self.poses_df[\"poses\"])\n",
    "        \n",
    "        return print(f\"relocated poses to {new_path}\")\n",
    "    \n",
    "    def reindex_poses(self, out_dir:str=None, remove_layers:int=None, force_reindex:bool=True) -> str:\n",
    "        '''\n",
    "        Removes <remove_layers> from poses and reindexes them.\n",
    "        '''\n",
    "        remove_layers = remove_layers or self.index_layers\n",
    "        \n",
    "        # set up out_dir:\n",
    "        out_dir = out_dir or f\"reindex_{str(self.reindex_number).zfill(4)}\"\n",
    "        out_dir = self.dir + \"/\" + out_dir\n",
    "        if not os.path.isdir(out_dir): os.makedirs(out_dir, exist_ok=True)\n",
    "        \n",
    "        # reindex and copy poses into new location:\n",
    "        renaming_dict = remove_index_layers.remove_index_layers(self.poses, output_dir=out_dir, n_layers=remove_layers, force_reindex=force_reindex)\n",
    "        renaming_dict = {k.split(\"/\")[-1].split(\".\")[0]: v for k, v in renaming_dict.items()}\n",
    "        \n",
    "        # add 'reindexed_description' column to self.poses_df according to the renaming_dict:\n",
    "        old_descriptions = deepcopy(list(self.poses_df[\"poses_description\"]))\n",
    "        self.poses_df.loc[:, \"poses_description\"] = [renaming_dict[location].split(\"/\")[-1].split(\".\")[0] for location in old_descriptions]\n",
    "        self.poses_df.loc[:, \"poses\"] = [renaming_dict[location] for location in old_descriptions]\n",
    "        \n",
    "        # set reindexed poses as new poses:\n",
    "        self.poses = list(self.poses_df[\"poses\"])\n",
    "        \n",
    "        # if argument self.auto_dump_df is set to True, dump the new poses_df in self.dir\n",
    "        if self.auto_dump_df:\n",
    "            self.poses_df.to_json(self.scorefile)\n",
    "        \n",
    "        return out_dir\n",
    "    \n",
    "    def calc_protparams(self, prefix=None) -> None:\n",
    "        '''\n",
    "        Calculates protparam scores for poses and stores them in poses_df.\n",
    "        '''\n",
    "        # set the prefix\n",
    "        prefix = prefix or f\"params_{str(self.increment_attribute('protparams')).zfill(4)}\"\n",
    "        \n",
    "        # Start the parser\n",
    "        pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "        ppb = Bio.PDB.PPBuilder()\n",
    "        \n",
    "        # if poses are pdb-files:\n",
    "        if all([x.endswith(\".pdb\") for x in self.poses_df[\"poses\"].to_list()]):  \n",
    "            # Get the structures\n",
    "            poses_l = [pdb_parser.get_structure(pose, pose) for pose in self.poses_df[\"poses\"].to_list()]\n",
    "\n",
    "            # collect the sequences\n",
    "            sequences = [\"\".join([str(x.get_sequence()) for x in ppb.build_peptides(pose)]) for pose in poses_l]\n",
    "        \n",
    "        elif all([x.endswith(\".fa\") for x in self.poses]):\n",
    "            sequences = [read_fasta_sequence(pose) for pose in self.poses]\n",
    "            \n",
    "        else:\n",
    "            raise TypeError(f\"ERROR: one or more of your poses are of invalid type (Only .fa and .pdb are allowed.)\")\n",
    "\n",
    "        # collect metrics for sequences\n",
    "        analyses = [ProteinAnalysis(sequence) for sequence in sequences]\n",
    "        metrics_dict = {\"pI\": [x.isoelectric_point() for x in analyses],\n",
    "                        \"extinction_coefficient\": [x.molar_extinction_coefficient() for x in analyses],\n",
    "                        \"instability_index\": [x.instability_index() for x in analyses],\n",
    "                        \"aromaticity\": [x.aromaticity() for x in analyses],\n",
    "                        \"molecular_weight\": [x.molecular_weight() for x in analyses],\n",
    "                       }\n",
    "        \n",
    "        # add metrics to self.poses_df\n",
    "        for metric in metrics_dict:\n",
    "            self.poses_df.loc[:, f\"{prefix}_{metric}\"] = metrics_dict[metric]\n",
    "            \n",
    "        # if argument self.auto_dump_df is set to True, dump the new poses_df in self.dir\n",
    "        if self.auto_dump_df:\n",
    "            self.poses_df.to_json(self.scorefile)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def store_sequences(self, prefix:str=None) -> None:\n",
    "        '''\n",
    "        Method to store sequences of the current poses in self.poses_df\n",
    "        '''\n",
    "        # set the column name for the poses_df:\n",
    "        col = prefix or f\"sequence_{str(self.increment_attribute('seqs')).zfill(4)}\"\n",
    "        \n",
    "        # Start the parser\n",
    "        pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "        ppb = Bio.PDB.PPBuilder()\n",
    "        \n",
    "        # gather sequences from poses into a list\n",
    "        if all([x.endswith(\".pdb\") for x in list(self.poses_df[\"poses\"])]):  \n",
    "            # if poses are pdb-files, get the structures and collect the sequences:\n",
    "            poses_l = [pdb_parser.get_structure(pose, pose) for pose in list(self.poses_df[\"poses\"])]\n",
    "            sequences = [\"\".join([str(x.get_sequence()) for x in ppb.build_peptides(pose)]) for pose in poses_l]\n",
    "        \n",
    "        elif all([x.endswith(\".fa\") for x in list(self.poses_df[\"poses\"])]):\n",
    "            # if poses are .fa-files, just read fasta-files:\n",
    "            sequences = [read_fasta_sequence(pose) for pose in list(self.poses_df[\"poses\"])]\n",
    "            \n",
    "        else:\n",
    "            raise TypeError(f\"ERROR: one or more of your poses are of invalid type (Only .fa and .pdb are allowed.)\")\n",
    "            \n",
    "        # set new scoreterm in poses_df:\n",
    "        self.poses_df.loc[:, col] = sequences\n",
    "        self.poses_df.to_json(self.scorefile)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def biopython_mutate(self, col: str, mutation_dict:dict=None):\n",
    "        '''\n",
    "        requires dict with mutations listed like this:\n",
    "        {\"A10\": \"ALA\", \"B12\": \"CYS\", \"C3\": \"PHE\"}\n",
    "        Dict can be read out from poses_df (set the col parameter)\n",
    "        '''\n",
    "        # argument processing\n",
    "        if col: mutations_dictlist = self.poses_df[col].to_list()\n",
    "        elif mutation_dict: mutations_dictlist = [mutation_dict for pose in self.poses_df[\"poses\"].to_list()]\n",
    "        \n",
    "        # check if poses are pdb-files:\n",
    "        if any([not x.endswith(\".pdb\") for x in self.poses_df[\"poses\"].to_list()]): raise TypeError(f\"Poses must be .pdb files to run biopython_mutate()\\nPoses:{', '.join(self.poses_df['poses'].to_list())}\")\n",
    "        \n",
    "        for pdb, mutation_dict in zip(self.poses_df[\"poses\"].to_list(), mutations_dictlist):\n",
    "            pose_path = utils.biopython_mutate.mutate_pdb(pdb, mutation_dict=mutation_dict, out_pdb_path=pdb, biopython_model_number=0)\n",
    "        return None\n",
    "\n",
    "# ------------------ Operational functions -----------------------------------------\n",
    "\n",
    "def prep_options(dict_a: dict, dict_b: dict) -> dict:\n",
    "    '''\n",
    "    dict_b is overwritten by dict_a\n",
    "    '''\n",
    "    #print(dict_a, dict_b)\n",
    "    dict_b.update(dict_a)\n",
    "    return dict_b\n",
    "\n",
    "## ------------------ Inpainting ---------------------------------------------------\n",
    "def run_inpainting(poses: list, work_dir: str, options=\"\", pose_options=None, scorefile=\"inpainting_scores.json\", max_gpus=5, perres_lddt=False, perres_inpaint_lddt=False) -> dict:\n",
    "    '''\n",
    "    Important: options specified in <pose_options> overwrite global options specified in <options>!\n",
    "    '''\n",
    "    # setup workdir:\n",
    "    work_dir = os.path.abspath(work_dir)\n",
    "    pdb_dir = f\"{work_dir}/output_pdbs/\"\n",
    "    if not os.path.isdir(pdb_dir): os.makedirs(pdb_dir, exist_ok=True)\n",
    "    \n",
    "    # setup pdb_dir and look for output file. Skip inpainting, if output is present:\n",
    "    return_dict = {\"pdb_dir\": (pdb_dir := f\"{work_dir}/output_pdbs/\")}\n",
    "    if os.path.isfile((inpaint_scorefile := f\"{work_dir}/{scorefile}\")):\n",
    "        return {\"pdb_dir\": pdb_dir, \"scores\": pd.read_json(inpaint_scorefile)}\n",
    "    \n",
    "    # parse options, flags, pose_options and pose_flags\n",
    "    options, flags = parse_options_flags(options, sep='--')\n",
    "    if pose_options:\n",
    "        #sanity check:\n",
    "        if len(poses) != len(pose_options): raise ValueError(f\"ERROR: Arguments <poses> and <pose_option> of the function run_inpainting() must be of the same length!\\nlen(poses) = {len(poses)}\\nlen(pose_options) = {len(pose_options)}\")\n",
    "        pose_options, pose_flags = zip(*[parse_options_flags(opt_str, sep=\"--\") for opt_str in pose_options])\n",
    "    else: \n",
    "        pose_options, pose_flags = [{} for x in poses], [[] for x in poses]\n",
    "\n",
    "    # write inpainting commands for each pose in poses:\n",
    "    cmds = [write_inpainting_cmd(pose, options=dict(options, **pose_opts), flags=list(set(flags) | set(pose_flgs)), out_dir=pdb_dir) for pose, pose_opts, pose_flgs in zip(poses, pose_options, pose_flags)]\n",
    "    \n",
    "    # execute inpainting commands in a jobarray on maximum <max_gpus>\n",
    "    sbatch_options = [f\"--gpus-per-node 1 -c1 -e {work_dir}/inpainting_err.log -o {work_dir}/inpainting_out.log\"]\n",
    "    job = sbatch_array_jobstarter(cmds=cmds, sbatch_options=sbatch_options, jobname=\"inpaint_poses\", \n",
    "                                  max_array_size=max_gpus, wait=True, remove_cmdfile=False, cmdfile_dir=work_dir)\n",
    "    \n",
    "    # collect inpainting scores and return outputs.\n",
    "    return_dict[\"scores\"] = collect_inpainting_scores(inpaint_dir=pdb_dir, scorefile=inpaint_scorefile, rename_pdbs=True, perres_lddt=perres_lddt, perres_inpaint_lddt=perres_inpaint_lddt)\n",
    "    return_dict[\"scores\"].to_json(inpaint_scorefile)\n",
    "\n",
    "    return return_dict\n",
    "\n",
    "def collect_inpainting_scores(inpaint_dir: str, scorefile=\"inpainting_scores.json\", rename_pdbs=True, perres_lddt=False, perres_inpaint_lddt=False) -> pd.DataFrame:\n",
    "    '''\n",
    "    Collects scores from .trb and .npz files of inpainting and renames (if <rename_pdbs> is set) output .pdbs from output_1.pdb to output_0001.pdb\n",
    "    Args:\n",
    "        <inpaint_dir>                   Output directory of your inpainting run.\n",
    "        <scorefile>                     Name of the scorefile you want to write\n",
    "        <rename_pdbs>                   (True)  bool: If set, renames .pdb files in dataframe and inpaint_dir to standard indeces (_0001 instead of _1, etc.)\n",
    "        <perres_lddt>                   (False) bool: If set, perresidue lddts will be collected as lists into the output DataFrame\n",
    "        <perres_inapint_lddt>           (False) bool: If set, perresidue inpainting lddts will be collected as lists into the output DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Pandas DataFrame containing all scores and pdb-file locations (+names)\n",
    "    '''\n",
    "    # collect scores from .files into one pandas DataFrame\n",
    "    pl = glob(f\"{inpaint_dir}/*.pdb\")\n",
    "    if not pl: raise FileNotFoundError(f\"ERROR: No .pdb files were found in the inpainting output direcotry {inpaint_dir}. Inpainting might have crashed (check inpainting error-log), or the path might be wrong!\")\n",
    "    \n",
    "    # collect inpaint scores into DataFrame by parsing each .trbfile for which a .pdb file exists\n",
    "    df = pd.concat([parse_inpainting_trbfile(p.replace(\".pdb\", \".trb\"), perres_lddt=perres_lddt, perres_inpaint_lddt=perres_inpaint_lddt) for p in pl])\n",
    "    \n",
    "    # if option <rename_pdbs> is set, update indeces to standard indexing (using zfill)\n",
    "    if rename_pdbs:\n",
    "        # rename description to standard indexing (_0001 instead of _1, etc...)\n",
    "        df.loc[:, \"new_description\"] = [\"_\".join(desc.split(\"_\")[:-1]) + \"_\" + str(int(desc.split(\"_\")[-1]) + 1).zfill(4) for desc in df[\"description\"]]\n",
    "        df.loc[:, \"new_loc\"] = [loc.replace(old_desc, new_desc) for loc, old_desc, new_desc in zip(list(df[\"location\"]), list(df[\"description\"]), list(df[\"new_description\"]))]\n",
    "        \n",
    "        # rename all inpainting outputfiles according to new indeces:\n",
    "        _empty = [[os.rename(f, f.replace(old_desc, new_desc)) for f in glob(f\"{inpaint_dir}/{old_desc}.*\")] for old_desc, new_desc in zip(list(df[\"description\"]), list(df[\"new_description\"]))]\n",
    "        #_empty = [os.rename(old, new) for old, new in zip(df[\"location\"], df[\"new_loc\"])]\n",
    "\n",
    "        # Collect information of path to .pdb files into DataFrame under 'location' column\n",
    "        df = df.drop(columns=[\"location\"]).rename(columns={\"new_loc\": \"location\"})\n",
    "        df = df.drop(columns=[\"description\"]).rename(columns={\"new_description\": \"description\"})\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def parse_inpainting_trbfile(trbfile: str, perres_lddt=False, perres_inpaint_lddt=False) -> pd.DataFrame:\n",
    "    '''\n",
    "    Reads scores from inpainting trb-file\n",
    "    Args:\n",
    "        <trbfile>                        Path to inpainting .trb file.\n",
    "        <perres_lddt>                    bool: If set, perresidue lddts will be collected as lists into the output DataFrame\n",
    "        <perres_inpaint_lddt>            bool: If set, perresidue inpainting lddts will be collected as lists into the output DataFrame\n",
    "    '''\n",
    "    # read trbfile:\n",
    "    if trbfile.endswith(\".trb\"): data_dict = np.load(trbfile, allow_pickle=True)\n",
    "    else: raise ValueError(f\"ERROR: only .trb-files can be passed into parse_inpainting_trbfile. <trbfile>: {trbfile}\")\n",
    "    \n",
    "    # instantiate scoresdict and start collecting:\n",
    "    sd = dict()\n",
    "    sd[\"lddt\"] = np.mean(data_dict[\"lddt\"])\n",
    "    sd[\"inpaint_lddt\"] = np.mean(data_dict[\"inpaint_lddt\"])\n",
    "    sd[\"template\"] = data_dict[\"settings\"][\"pdb\"]\n",
    "    sd[\"con_ref_pdb_idx\"] = [data_dict[\"con_ref_pdb_idx\"]]\n",
    "    sd[\"con_hal_pdb_idx\"] = [data_dict[\"con_hal_pdb_idx\"]]\n",
    "    sd[\"res_mapping\"] = [[(x[1], y[1]) for x, y in zip(data_dict[\"con_ref_pdb_idx\"], data_dict[\"con_hal_pdb_idx\"])]]\n",
    "    sd[\"location\"] = trbfile.replace(\".trb\", \".pdb\")\n",
    "    sd[\"description\"] = trbfile.split(\"/\")[-1].replace(\".trb\", \"\")\n",
    "    \n",
    "    if perres_lddt: sd[\"perresidue_lddt\"] = [data_dict[\"lddt\"]]\n",
    "    if perres_inpaint_lddt: sd[\"perresidue_inpaint_lddt\"] = [data_dict[\"inpaint_lddt\"]]\n",
    "    \n",
    "    return pd.DataFrame(sd)\n",
    "\n",
    "def write_inpainting_cmd(pose: str, options: dict, flags: list, out_dir: str) -> str:\n",
    "    '''\n",
    "    Writes an inpainting command that can be run on acluster on a slurm script.\n",
    "    Args:\n",
    "        <pose>\n",
    "        <options>\n",
    "        <flags>\n",
    "    Returns:\n",
    "        cmd that can be run on the cluster\n",
    "    '''\n",
    "    if options == None: options = {}\n",
    "    if flags == None: flags = []\n",
    "\n",
    "    # parse pose name as output into options:\n",
    "    options[\"out\"] = out_dir + pose.split(\"/\")[-1].replace(\".pdb\", \"\")\n",
    "    if not \"dump_all\" in flags: flags.append(\"dump_all\")\n",
    "    \n",
    "    options_string = \" \".join([f\"--{k}={v}\" for k, v in options.items()])\n",
    "    flags_string = \" \".join([\"--\"+flag for flag in flags])\n",
    "    \n",
    "    return f\"{_rfdesign_python} {_inpaint_path}/inpaint.py --pdb {pose} {options_string} {flags_string}\"\n",
    "\n",
    "def trf_relax_dir(path_to_dir: str, max_array_size=512) -> str:\n",
    "    '''\n",
    "    runs trf_relax script on inpaints.\n",
    "    '''\n",
    "    # sanity check\n",
    "    if not os.path.isdir(path_to_dir): raise FileNotFoundError\n",
    "        \n",
    "    # check if outputs are already present:\n",
    "    if os.path.isdir((trf_dir := f\"{path_to_dir}/trf_relax\")):\n",
    "        if len(glob(f\"{path_to_dir}/*.pdb\")) == len(glob(f\"{trf_dir}/*.pdb\")):\n",
    "            ### IMPORTANT: Here, I use the number of .pdb files in the inpainting output directory and the trf_relax directory as an indirect metric.\n",
    "            print(f\"Inpaints at {path_to_dir} are already relaxed at {trf_dir}. Skipping trf_relax.sh\")\n",
    "            return trf_dir\n",
    "    \n",
    "    # create list of commands to run trf_relax\n",
    "    cmdfile = f\"{path_to_dir}/trf_relax_commands\"\n",
    "    run(f\"{_trf_relax_script_path} {path_to_dir} {cmdfile}\", stdout=True, stderr=True, check=True, shell=True)\n",
    "\n",
    "    with open(cmdfile, 'r') as f:\n",
    "        cmds = [x.strip() for x in f.readlines() if x]\n",
    "    \n",
    "    # sbatch commandslist\n",
    "    sbatch_options = [f\"-e {path_to_dir}/trf_relax.err -o {path_to_dir}/trf_relax.out\"]\n",
    "    sbatch_array_jobstarter(cmds, sbatch_options, jobname=\"trf_relax\", max_array_size=max_array_size, remove_cmdfile=False, cmdfile_dir=path_to_dir)\n",
    "\n",
    "    return trf_dir\n",
    "\n",
    "def run_hallucination(poses: list, work_dir: str, options=\"\", pose_options=None, scorefile=\"inpainting_scores.json\", max_gpus=5, perres_lddt=False, perres_inpaint_lddt=False) -> dict:\n",
    "    '''\n",
    "    Important: options specified in <pose_options> overwrite global options specified in <options>!\n",
    "    '''\n",
    "    # setup workdir:\n",
    "    work_dir = os.path.abspath(work_dir)\n",
    "    pdb_dir = f\"{work_dir}/output_pdbs/\"\n",
    "    if not os.path.isdir(pdb_dir): os.makedirs(pdb_dir, exist_ok=True)\n",
    "    \n",
    "    # setup pdb_dir and look for output file. Skip inpainting, if output is present:\n",
    "    return_dict = {\"pdb_dir\": (pdb_dir := f\"{work_dir}/output_pdbs/\")}\n",
    "    if os.path.isfile((hallucination_scorefile := f\"{work_dir}/{scorefile}\")):\n",
    "        return {\"pdb_dir\": pdb_dir, \"scores\": pd.read_json(hallucination_scorefile)}\n",
    "    \n",
    "    # parse options, flags, pose_options and pose_flags\n",
    "    options, flags = parse_options_flags(options, sep='--')\n",
    "    if pose_options:\n",
    "        #sanity check:\n",
    "        if len(poses) != len(pose_options): raise ValueError(f\"ERROR: Arguments <poses> and <pose_option> of the function run_inpainting() must be of the same length!\\nlen(poses) = {len(poses)}\\nlen(pose_options) = {len(pose_options)}\")\n",
    "        pose_options, pose_flags = zip(*[parse_options_flags(opt_str, sep=\"--\") for opt_str in pose_options])\n",
    "    else: \n",
    "        pose_options, pose_flags = [{} for x in poses], [[] for x in poses]\n",
    "\n",
    "    # write inpainting commands for each pose in poses:\n",
    "    cmds = [write_hallucination_cmd(pose, options=dict(options, **pose_opts), flags=list(set(flags) | set(pose_flgs)), out_dir=pdb_dir) for pose, pose_opts, pose_flgs in zip(poses, pose_options, pose_flags)]\n",
    "    \n",
    "    # execute inpainting commands in a jobarray on maximum <max_gpus>\n",
    "    sbatch_options = [f\"--gpus-per-node 1 -c1 -e {work_dir}/hallucination_err.log -o {work_dir}/hallucination_out.log\"]\n",
    "    job = sbatch_array_jobstarter(cmds=cmds, sbatch_options=sbatch_options, jobname=\"hallucinate_poses\", \n",
    "                                  max_array_size=max_gpus, wait=True, remove_cmdfile=False, cmdfile_dir=work_dir)\n",
    "    \n",
    "    # collect inpainting scores and return outputs.\n",
    "    return_dict[\"scores\"] = collect_hallucination_scores(hallucination_dir=pdb_dir, scorefile=hallucination_scorefile, rename_pdbs=True)\n",
    "    return_dict[\"scores\"].to_json(hallucination_scorefile)\n",
    "\n",
    "    return return_dict\n",
    "\n",
    "def write_hallucination_cmd(pose: str, options: dict, flags: list, out_dir: str) -> str:\n",
    "    '''\n",
    "    Writes an hallucination command that can be run on acluster on a slurm script.\n",
    "    Args:\n",
    "        <pose>\n",
    "        <options>\n",
    "        <flags>\n",
    "    Returns:\n",
    "        cmd that can be run on the cluster\n",
    "    '''\n",
    "    if options == None: options = {}\n",
    "    if flags == None: flags = []\n",
    "\n",
    "    # parse pose name as output into options:\n",
    "    options[\"out\"] = out_dir + pose.split(\"/\")[-1].replace(\".pdb\", \"\")\n",
    "    \n",
    "    options_string = \" \".join([f\"--{k}={v}\" for k, v in options.items()])\n",
    "    flags_string = \" \".join([\"--\"+flag for flag in flags])\n",
    "    \n",
    "    return f\"{_rfdesign_python} {_hallucination_path}/hallucinate.py --pdb {pose} {options_string} {flags_string}\"\n",
    "\n",
    "def collect_hallucination_scores(hallucination_dir: str, scorefile=\"hallucination_scores.json\", rename_pdbs=True) -> pd.DataFrame:\n",
    "    '''\n",
    "    Collects scores from .trb and .npz files of inpainting and renames (if <rename_pdbs> is set) output .pdbs from output_1.pdb to output_0001.pdb\n",
    "    Args:\n",
    "        <inpaint_dir>                   Output directory of your inpainting run.\n",
    "        <scorefile>                     Name of the scorefile you want to write\n",
    "        <rename_pdbs>                   (True)  bool: If set, renames .pdb files in dataframe and inpaint_dir to standard indeces (_0001 instead of _1, etc.)\n",
    "        <perres_lddt>                   (False) bool: If set, perresidue lddts will be collected as lists into the output DataFrame\n",
    "        <perres_inapint_lddt>           (False) bool: If set, perresidue inpainting lddts will be collected as lists into the output DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Pandas DataFrame containing all scores and pdb-file locations (+names)\n",
    "    '''\n",
    "    # collect scores from .files into one pandas DataFrame\n",
    "    pl = glob(f\"{hallucination_dir}/*.pdb\")\n",
    "    if not pl: raise FileNotFoundError(f\"ERROR: No .pdb files were found in the hallucination output direcotry {hallucination_dir}. Hallucination might have crashed (check inpainting error-log), or the path might be wrong!\")\n",
    "    \n",
    "    # collect inpaint scores into DataFrame by parsing each .trbfile for which a .pdb file exists\n",
    "    df = pd.concat([parse_hallucination_trbfile(p.replace(\".pdb\", \".trb\")) for p in pl])\n",
    "    \n",
    "    # if option <rename_pdbs> is set, update indeces to standard indexing (using zfill)\n",
    "    if rename_pdbs:\n",
    "        # rename description to standard indexing (_0001 instead of _1, etc...)\n",
    "        df.loc[:, \"new_description\"] = [\"_\".join(desc.split(\"_\")[:-1]) + \"_\" + str(int(desc.split(\"_\")[-1]) + 1).zfill(4) for desc in df[\"description\"]]\n",
    "        df.loc[:, \"new_loc\"] = [loc.replace(old_desc, new_desc) for loc, old_desc, new_desc in zip(list(df[\"location\"]), list(df[\"description\"]), list(df[\"new_description\"]))]\n",
    "        \n",
    "        # rename all inpainting outputfiles according to new indeces:\n",
    "        _empty = [[os.rename(f, f.replace(old_desc, new_desc)) for f in glob(f\"{hallucination_dir}/{old_desc}.*\")] for old_desc, new_desc in zip(list(df[\"description\"]), list(df[\"new_description\"]))]\n",
    "\n",
    "        # Collect information of path to .pdb files into DataFrame under 'location' column\n",
    "        df = df.drop(columns=[\"location\"]).rename(columns={\"new_loc\": \"location\"})\n",
    "        df = df.drop(columns=[\"description\"]).rename(columns={\"new_description\": \"description\"})\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def parse_hallucination_trbfile(trbfile: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Reads scores from inpainting trb-file\n",
    "    Args:\n",
    "        <trbfile>                        Path to inpainting .trb file.\n",
    "        <perres_lddt>                    bool: If set, perresidue lddts will be collected as lists into the output DataFrame\n",
    "        <perres_inpaint_lddt>            bool: If set, perresidue inpainting lddts will be collected as lists into the output DataFrame\n",
    "    '''\n",
    "    # read trbfile:\n",
    "    if trbfile.endswith(\".trb\"): data_dict = np.load(trbfile, allow_pickle=True)\n",
    "    else: raise ValueError(f\"ERROR: only .trb-files can be passed into parse_inpainting_trbfile. <trbfile>: {trbfile}\")\n",
    "    \n",
    "    # instantiate scoresdict and start collecting:\n",
    "    sd = dict()\n",
    "    for loss in [x for x in data_dict.keys() if x.startswith(\"loss_\")]:\n",
    "        sd[loss] = data_dict[loss][0] # take first element, because, for an unknown reason, losses are stored in 1D lists ([loss_value]).\n",
    "    sd[\"sampled_mask\"] = data_dict[\"sampled_mask\"]\n",
    "    sd[\"template\"] = data_dict[\"settings\"][\"pdb\"]\n",
    "    sd[\"con_ref_pdb_idx\"] = [data_dict[\"con_ref_pdb_idx\"]]\n",
    "    sd[\"con_hal_pdb_idx\"] = [data_dict[\"con_hal_pdb_idx\"]]\n",
    "    sd[\"res_mapping\"] = [[(x[1], y[1]) for x, y in zip(data_dict[\"con_ref_pdb_idx\"], data_dict[\"con_hal_pdb_idx\"])]]\n",
    "    sd[\"location\"] = trbfile.replace(\".trb\", \".pdb\")\n",
    "    sd[\"description\"] = trbfile.split(\"/\")[-1].replace(\".trb\", \"\")\n",
    "    \n",
    "    return pd.DataFrame(sd)\n",
    "\n",
    "def parse_tuples_into_dict(input_list: list[tuple]) -> dict:\n",
    "    '''\n",
    "    Convert a list of tuples into a dictionary where the first element of each tuple is the key and the second element is added to a list associated with that key.\n",
    "\n",
    "    Parameters:\n",
    "    input_list (list[tuple]): The list of tuples to be converted\n",
    "    \n",
    "    Returns:\n",
    "    dict: The resulting dictionary\n",
    "    '''\n",
    "    d = defaultdict(list)\n",
    "    dl = [d[a].append(b) for a, b in input_list]\n",
    "    return dict(d)\n",
    "\n",
    "def compile_motif_from_inpaint_trb(input_trb: str) -> dict:\n",
    "    '''\n",
    "    Compile reference and target motif of an inpainting trb-file into a motif.\n",
    "\n",
    "    Parameters:\n",
    "    input_trb (str): The path to the input trb-file\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary containing reference and target motifs\n",
    "    '''\n",
    "    # sanity\n",
    "    if not os.path.isfile(input_trb): raise FileNotFoundError\n",
    "\n",
    "    # load data_dict from file\n",
    "    trb_dict = np.load(input_trb, allow_pickle=True)\n",
    "    \n",
    "    # compile motif dict from data_dict and return  \n",
    "    return parse_tuples_into_dict(trb_dict[\"con_ref_pdb_idx\"]), parse_tuples_into_dict(trb_dict[\"con_hal_pdb_idx\"])\n",
    "\n",
    "def calc_inpaint_trf_motif_rmsd(inpaint_dir: str, trf_dir:str=None, out_scorefile:str=None):\n",
    "    '''\n",
    "    Calculates motif RMSD after running trf_relax.sh on an inpainting output.\n",
    "    '''\n",
    "    # specify trf_dir and scorefile:\n",
    "    trf_dir = trf_dir or f\"{inpaint_dir}/trf_relax/\"\n",
    "    \n",
    "    # collect trb-files and from them derive poses and description\n",
    "    if not (fl := glob(f\"{inpaint_dir}/*.trb\")): raise FileNotFoundError(f\"ERROR: No *.trb files found at {trf_dir}\")\n",
    "    poses = [x.split(\"/\")[-1].replace(\".trb\", \".pdb\") for x in fl]\n",
    "    descriptions = [x.replace(\".pdb\", \"\") for x in poses]\n",
    "    \n",
    "    # compile motifs for rmsd calculation from trb-files\n",
    "    motifs = [compile_motif_from_inpaint_trb(f) for f in fl]\n",
    "    \n",
    "    # run motif_rmsd calculation (inpaint_dir/pose trf_dir/pose motif atoms=\"CA\") for every pose\n",
    "    rmsds = [motif_rmsd.superimpose_calc_motif_rmsd(f\"{inpaint_dir}/{pose}\", f\"{trf_dir}/{pose}\", ref_selection=motif[1], target_selection=motif[1]) for pose, motif in zip(poses, motifs)]\n",
    "    \n",
    "    # compile RMSD DataFrame\n",
    "    df = pd.DataFrame({\"description\": descriptions, \"trf_motif_bb_ca_rmsd\": rmsds})\n",
    "    \n",
    "    # store DataFrame if out_scorefile option is set\n",
    "    if out_scorefile:\n",
    "        df.to_json(out_scorefile)\n",
    "\n",
    "    return df\n",
    "\n",
    "## ------------------ RFDiffusion -------------------------------------------------\n",
    "def run_rfdiffusion(poses: list, work_dir:str, options=\"\", pose_options=None, scorefile=\"rfdiffusion_scores.json\", max_gpus=5) -> dict:\n",
    "    '''run RFDiffusion on acluster'''\n",
    "    # setup workdir\n",
    "    work_dir = os.path.abspath(work_dir)\n",
    "    pdb_dir = f\"{work_dir}/output_pdbs/\"\n",
    "    if not os.path.isdir(pdb_dir): os.makedirs(pdb_dir, exist_ok=True)\n",
    "\n",
    "    # Look for output-file in pdb-dir. If output is present and correct, then skip diffusion step.\n",
    "    rfdiff_out_dict = {\"pdb_dir\": pdb_dir}\n",
    "    if os.path.isfile((scorefilepath := f\"{work_dir}/{scorefile}\")):\n",
    "        return {\"pdb_dir\": pdb_dir, \"scores\": pd.read_json(scorefilepath)}\n",
    "    \n",
    "    # parse options and pose_options:\n",
    "    if pose_options:\n",
    "        if len(poses) != len(pose_options): raise ValueError(f\"Arguments <poses> and <pose_options> for RFDiffusion must be of the same length. There might be an error with your pose_options argument!\\nlen(poses) = {poses}\\nlen(pose_options) = {len(pose_options)}\")\n",
    "    else:\n",
    "        pose_options = [\"\" for x in poses]\n",
    "\n",
    "    # write rfdiffusion cmds\n",
    "    cmds = [write_rfdiffusion_cmd(pose, options, pose_opts, output_dir=rfdiff_out_dict[\"pdb_dir\"]) for pose, pose_opts in zip(poses, pose_options)]\n",
    "\n",
    "    #options, pose_options = parse_rfdiffusion_opts(options, pose_options, n=len(poses))\n",
    "    sbatch_options = [f\"--gpus-per-node 1 -c1 -e {work_dir}/hallucination_err.log -o {work_dir}/hallucination_out.log\"]\n",
    "    job = sbatch_array_jobstarter(cmds=cmds, sbatch_options=sbatch_options, jobname=\"diffuse_poses\",\n",
    "                                  max_array_size=max_gpus, wait=True, remove_cmdfile=False, cmdfile_dir=work_dir)\n",
    "    \n",
    "    # collect rfdiff scores and return outputs:\n",
    "    rfdiff_out_dict[\"scores\"] = collect_rfdiffusion_scores(dir=rfdiff_out_dict[\"pdb_dir\"], scorefile=scorefile, rename_pdbs=True)\n",
    "    rfdiff_out_dict[\"scores\"].to_json(scorefile)\n",
    "\n",
    "    return rfdiff_out_dict\n",
    "\n",
    "def collect_rfdiffusion_scores(dir: str, scorefile: str, rename_pdbs=True) -> pd.DataFrame:\n",
    "    '''collects and returns (most likely used) RFDiffusion scores'''\n",
    "    # collect scores from .trb-files into one pandas DataFrame:\n",
    "    pl = glob(f\"{dir}/*.pdb\")\n",
    "    if not pl: raise FileNotFoundError(f\"No .pdb files were found in the diffusion output direcotry {dir}. RFDiffusion might have crashed (check inpainting error-log), or the path might be wrong!\")\n",
    "\n",
    "    # collect hallucination scores into a DataFrame:\n",
    "    df = pd.concat([parse_diffusion_trbfile(p.replace(\".pdb\", \".trb\")) for p in pl])\n",
    "\n",
    "    # rename pdbs if option is set:\n",
    "    if rename_pdbs:\n",
    "        df.loc[:, \"new_description\"] = [\"_\".join(desc.split(\"_\")[:-1]) + \"_\" + str(int(desc.split(\"_\")[-1]) + 1).zfill(4) for desc in df[\"description\"]]\n",
    "        df.loc[:, \"new_loc\"] = [loc.replace(old_desc, new_desc) for loc, old_desc, new_desc in zip(list(df[\"location\"]), list(df[\"description\"]), list(df[\"new_description\"]))]\n",
    "\n",
    "        # rename all diffusion outputfiles according to new indeces:\n",
    "        _empty = [[os.rename(f, f.replace(old_desc, new_desc)) for f in glob(f\"{dir}/{old_desc}.*\")] for old_desc, new_desc in zip(list(df[\"description\"]), list(df[\"new_description\"]))]\n",
    "\n",
    "        # Collect information of path to .pdb files into DataFrame under 'location' column\n",
    "        df = df.drop(columns=[\"location\"]).rename(columns={\"new_loc\": \"location\"})\n",
    "        df = df.drop(columns=[\"description\"]).rename(columns={\"new_description\": \"description\"})\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def parse_diffusion_trbfile(path: str) -> pd.DataFrame:\n",
    "    '''AAA'''\n",
    "    # read trbfile:\n",
    "    if path.endswith(\".trb\"): data_dict = np.load(path, allow_pickle=True)\n",
    "    else: raise ValueError(f\"only .trb-files can be passed into parse_inpainting_trbfile. <trbfile>: {path}\")\n",
    "\n",
    "    # calc mean_plddt:\n",
    "    last_plddts = data_dict[\"plddt\"][-1]\n",
    "    sd[\"mean_plddt\"] = [sum(last_plddts) / len(last_plddts)]\n",
    "    sd[\"perres_plddt\"] = [last_plddts]\n",
    "\n",
    "    # instantiate scoresdict and start collecting:\n",
    "    sd = dict()\n",
    "    scoreterms = [\"con_hal_pdb_idx\", \"con_ref_pdb_idx\", \"sampled_mask\"]\n",
    "    for st in scoreterms:\n",
    "        sd[st] = [data_dict[st]]\n",
    "\n",
    "    # collect metadata\n",
    "    sd[\"location\"] = path.replace(\".trb\", \".pdb\")\n",
    "    sd[\"description\"] = path.split(\"/\")[-1].replace(\".trb\", \"\")\n",
    "    sd[\"input_pdb\"] = data_dict[\"config\"][\"input_pdb\"]\n",
    "\n",
    "    return pd.DataFrame(sd)\n",
    "\n",
    "def write_rfdiffusion_cmd(pose: str, options: str, pose_opts: str, output_dir: str) -> str:\n",
    "    '''AAA'''\n",
    "    # parse description:\n",
    "    desc = pose.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    # parse options:\n",
    "    start_opts = parse_rfdiffusion_opts(options, pose_opts)\n",
    "    \n",
    "    start_opts[\"inference.output_prefix\"] = f\"{output_dir}/{desc}\"\n",
    "    start_opts[\"inference.input_pdb\"] = pose\n",
    "    opts_str = \" \".join([f\"{k}={v}\" for k, v in start_opts.items()])\n",
    "    \n",
    "    # return cmd\n",
    "    return f\"{_rfdiffusion_inference_script} {opts_str}\"\n",
    "\n",
    "def parse_rfdiffusion_opts(options: str, pose_options: str) -> dict:\n",
    "    '''AAA'''\n",
    "    splitstr = [x for x in options.strip().split(\" \") + pose_options.strip().split(\" \") if x]# adding pose_opts after options makes sure that pose_opts overwrites options!\n",
    "    return {x.split(\"=\")[0]: \"=\".join(x.split(\"=\")[1:]) for x in splitstr}\n",
    "\n",
    "## ------------------ ProteinMPNN --------------------------------------------------\n",
    "\n",
    "def check_fixed_positions(fixed_positions_list: list) -> list:\n",
    "    '''\n",
    "    Checks if the fixed_positions in fixed_positions_list are of the correct format.\n",
    "    [\n",
    "    {\"A\": [], \"B\": []}\n",
    "    {\"A\": [], \"B\": []}\n",
    "    ]\n",
    "    '''\n",
    "    print(f\"\\nChecking fixed_positions passed to ProteinMPNN for correct structure.\")\n",
    "    for fixedpos_dict in fixed_positions_list:\n",
    "        if type(fixedpos_dict) != dict:\n",
    "            raise TypeError(f\"ERROR: Your fixed_positions_list has the wrong structure. To find the correct structure, look at the examples in the ProteinMPNN directory!\")\n",
    "        for key, values in fixedpos_dict.items():\n",
    "            try:\n",
    "                fixedpos_dict[key] = [int(x) for x in values]\n",
    "            except ValueError:\n",
    "                raise TypeError(f\"ERROR: Wrong value for fixed position. Has to be of type int! To find the correct structure (with correct types) for fixedpositions dict, look at the examples in the ProteinMPNN directory.\")\n",
    "\n",
    "    return fixed_positions_list\n",
    "\n",
    "def write_keysvalues_to_file(keys: list, values: list, outfile_name: str) -> str:\n",
    "    '''\n",
    "    Writes a key-value pair submitted as two lists [keys], [values] to a json file.\n",
    "    Function returns name of the json file.\n",
    "    Args:\n",
    "        <keys>:          list of keys\n",
    "        <values>:        list of values\n",
    "        <outfile_name:   name of the file you want to write\n",
    "    '''\n",
    "    # compile dictionary\n",
    "    write_dict = {k: v for k, v in zip(keys, values)}\n",
    "    \n",
    "    # write dictionary to json-file\n",
    "    with open(outfile_name, 'w') as f:\n",
    "        f.write(json.dumps(write_dict))\n",
    "\n",
    "    return outfile_name\n",
    "\n",
    "def write_proteinmpnn_cmd(pose_json, mpnn_options: dict, pose_options=None):\n",
    "    '''\n",
    "    Writes commandline to run ProteinMPNN on the cluster.\n",
    "    <mpnn_option> are options for running ProteinMPNN that apply to ALL poses.\n",
    "    Options for individual poses have to be set with <pose_options>\n",
    "    '''\n",
    "    # if pose_options are set, then update (and overwrite if necessary) mpnn_options with pose_options\n",
    "    if pose_options:\n",
    "        mpnn_options.update(pose_options)\n",
    "    \n",
    "    # parse options as --key=value from {mpnn_options}\n",
    "    options = ' '.join([f\"--{key}={str(value)}\" for key, value in mpnn_options.items()])\n",
    "    cmd = f\"{_python_path} {_proteinmpnn_path}/protein_mpnn_run.py --jsonl_path={pose_json} {options}\"\n",
    "    return cmd\n",
    "\n",
    "def sbatch_proteinmpnn(mpnn_cmds, mpnn_options, max_gpus=5):\n",
    "    '''\n",
    "    Runs ProteinMPNN on a slurm cluster. Assumes that GPUs are available on the cluster.\n",
    "    '''\n",
    "    sbatch_options = [\"--gpus-per-node 1\", \"-c2\", f'-e {mpnn_options[\"out_folder\"]}/ProteinMPNN_err.log -o {mpnn_options[\"out_folder\"]}/ProteinMPNN_out.log']\n",
    "    sbatch_array_jobstarter(mpnn_cmds, sbatch_options, jobname=\"mpnn\", max_array_size=max_gpus, \n",
    "                            remove_cmdfile=False, cmdfile_dir=mpnn_options[\"out_folder\"]+\"/\")\n",
    "    return\n",
    "\n",
    "def proteinmpnn(poses: list, mpnn_options: dict, pose_options=None, max_gpus=5, scorefile=\"mpnn_scores.json\"):\n",
    "    '''\n",
    "    Runs ProteinMPNN on the cluster with <mpnn_options> for all poses. Individual options for poses can be specified with pose_options.\n",
    "    \n",
    "    !!! scorefile is written into mpnn_options['out_folder'] !!!\n",
    "    '''\n",
    "    # If no pose_options are set, make sure that None is passed for each pose to the function write_proteinmpnn_cmd():\n",
    "    if not pose_options: pose_options = [None for pose in poses]\n",
    "    \n",
    "    # Check if mpnn_scorefile exists at mpnn_out_folder, if yes, just return the DataFrame from the scorefile.\n",
    "    if os.path.isfile((mpnn_scorefile := f\"{mpnn_options['out_folder']}/{scorefile}\")):\n",
    "        # if scorefile already exists, just read scorefile and return scores from there.\n",
    "        with open(mpnn_scorefile, 'r') as f:\n",
    "            scores_df = json.load(f)\n",
    "        print(f\"\\nScorefile for ProteinMPNN run {mpnn_options['out_folder']} already exists at {mpnn_scorefile}. Skipping step.\")\n",
    "        return scores_df\n",
    "    \n",
    "    # create mpnn_out_dir:\n",
    "    if not os.path.isdir(mpnn_options['out_folder']): os.makedirs(mpnn_options[\"out_folder\"], exist_ok=True)\n",
    "\n",
    "    # parse sequences\n",
    "    jsonl_dir = f\"{mpnn_options['out_folder']}/jsonl_dir/\"\n",
    "    os.makedirs(jsonl_dir, exist_ok=True)\n",
    "    print(f\"\\nParsing poses into .json files at {jsonl_dir}\")\n",
    "    poses_jsonl_list = parse_poses(poses, jsonl_dir) # returns list of paths to jsonl files for poses\n",
    "        \n",
    "    # write mpnn commands and print the options\n",
    "    mpnn_cmds = [write_proteinmpnn_cmd(pose_json, mpnn_options, pose_option) for pose_json, pose_option in zip(poses_jsonl_list, pose_options)]\n",
    "    options_string = '\\n'.join([f\"--{k} {v}\" for k, v in mpnn_options.items()])\n",
    "    print(f\"\\nRunning ProteinMPNN with options: \\n{options_string}\")\n",
    "    \n",
    "    # run ProteinMPNN with sbatch\n",
    "    sbatch_proteinmpnn(mpnn_cmds, mpnn_options, max_gpus=max_gpus) #(automatically waits for job to finish)\n",
    "    scores_df = rename_mpnn_fastas(input_dir=f'{mpnn_options[\"out_folder\"]}/seqs/', scorefile=mpnn_scorefile, write=True)\n",
    "    \n",
    "    return scores_df\n",
    "\n",
    "# Prep ProteinMPNN pose_options string!\n",
    "\n",
    "## ----------------------- Rosetta ------------------------------------------------------------\n",
    "\n",
    "def run_rosetta(poses: list, rosetta_executable: str, rosetta_options: dict, rosetta_flags: list, pose_options: list[dict], pose_flags: list[list], n: int, work_dir: str, scorefile=\"rosetta_scores.sc\", max_cpus=256, force_options=None) -> dict:\n",
    "    '''\n",
    "    Runs Rosetta executable on Slurm.\n",
    "    Args:\n",
    "        <poses>                   List of paths to pdb-files. Rosetta runs are run on all poses in this list.\n",
    "        <rosetta_executable>      str, name of the Rosetta executable that you want to run.\n",
    "        <relax_options>           dictionary, containing Rosetta commandline options. Example: {\"-extra_res_fa\": \"rotlib/path\"}\n",
    "        <relax_flags>             list, containing Rosetta commandline flags. Example: [\"-beta\", \"-ex1\", \"-ex2\"]\n",
    "        <n>                       int, overwrites the Rosetta option -nstruct. This is implemented as a separate argument to maximize parallel running CPUs.\n",
    "        <work_dir>                str, working directory of the relax application.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containg [\"pdb_dir\"] and [\"scores\"]\n",
    "    '''\n",
    "    # setup output_dir and return dictionary\n",
    "    rosetta_path = find_rosetta_path(_rosetta_paths, rosetta_executable, stringent=True)\n",
    "    rosetta_work_dir = os.path.abspath(work_dir)\n",
    "    if not os.path.isdir(rosetta_work_dir): os.makedirs(rosetta_work_dir, exist_ok=True)\n",
    "    \n",
    "    # if relax outputs are already present, skip relax step\n",
    "    logfile = f\"{rosetta_work_dir}/log.txt\"\n",
    "    return_dict = {\"pdb_dir\": rosetta_work_dir}\n",
    "    rosetta_scorefile = f\"{rosetta_work_dir}/rosetta_scores.json\"\n",
    "    donefile = f\"{rosetta_work_dir}/done.txt\"\n",
    "    if os.path.isfile(rosetta_scorefile):\n",
    "        scores = pd.read_json(rosetta_scorefile)\n",
    "        if os.path.isfile(donefile):\n",
    "            return_dict[\"scores\"] = scores\n",
    "            print(f\"\\nOutputs of rosetta run {rosetta_work_dir} already found at {rosetta_work_dir}/rosetta_scores.json\\nSkipping rosetta step.\")\n",
    "            with open(logfile, 'w') as f:\n",
    "                f.write(\"Outputs of rosetta run already found. Skipping Rosetta Step.\")\n",
    "            return return_dict\n",
    "        else:\n",
    "            with open(logfile, 'w') as f:\n",
    "                f.write(f\"Found outputs of Rosetta Run, but number of relaxed poses does not match number of expected poses. \\nlen(scores) = {len(scores)}\\nlen(poses)={len(poses)}\\nn={n}\")\n",
    "            #print(f\"\\nFound outputs of rosetta run, but number of relaxed poses in scorefile does not match expected number of poses. Removing old outputs and restarting relax!\")\n",
    "            #raise ValueError(f\"ERROR: Rosetta outputs inconsistent with commands!\")\n",
    "            run(f\"rm {rosetta_work_dir}/*\", shell=True, stdout=True, stderr=True, check=True)\n",
    "            \n",
    "    # update relax_options and relax_flags with global variables!\n",
    "    rosetta_options[\"out:file:scorefile\"] = f\"{rosetta_work_dir}/{scorefile}\"\n",
    "    rosetta_options[\"out:path:all\"] = rosetta_work_dir\n",
    "    \n",
    "    # write relax commands for each pose in poses (for <n> relax runs)\n",
    "    rosetta_cmds = list()\n",
    "    for pose, pose_options_d, pose_flags_l in zip(poses, pose_options, pose_flags):\n",
    "        for i in range(1, n+1):\n",
    "            rosetta_cmds.append(write_rosetta_cmd(rosetta_path, pose=pose, i=i, options=rosetta_options, flags=rosetta_flags, pose_options=pose_options_d, pose_flags=pose_flags_l, force_options=force_options))\n",
    "    \n",
    "    #assert len(rosetta_cmds) <= 1000 # Slurm does not allow jobarrays bigger than 1000 jobs. Don't ask me why...\n",
    "    \n",
    "    # remove Rosetta Scorefile if one exists:\n",
    "    if os.path.isfile(rosetta_options[\"out:file:scorefile\"]): run(f\"rm {rosetta_options['out:file:scorefile']}\", shell=True, stderr=True, stdout=True, check=True)\n",
    "\n",
    "    # execute relax commands in a jobarray on maximum <max_cpus>\n",
    "    sbatch_options = [f\"-e {rosetta_work_dir}/rosetta_err.log -o {rosetta_work_dir}/rosetta_out.log\"]\n",
    "    sbatch_array_jobstarter(cmds=rosetta_cmds, sbatch_options=sbatch_options, jobname=\"rosetta_poses\", \n",
    "                            max_array_size=max_cpus, wait=True, remove_cmdfile=False, cmdfile_dir=rosetta_work_dir)\n",
    "    \n",
    "    # collect relax scores and rename relaxed pdbs.\n",
    "    time.sleep(60) # Rosetta does not have time to write the last score into the scorefile otherwise?\n",
    "    scores = collect_rosetta_scores(rosetta_options[\"out:path:all\"], rosetta_options[\"out:file:scorefile\"])\n",
    "    \n",
    "    # write donefile\n",
    "    with open(donefile, 'w') as f:\n",
    "        f.write(\"done\")\n",
    "\n",
    "    # return poses as path to pdb_dir and scores as DataFrame\n",
    "    return {\"scores\": scores, \"pdb_dir\": rosetta_work_dir}\n",
    "\n",
    "def run_relax(poses: list, relax_options: dict, relax_flags: list, pose_options: dict, pose_flags: list, n: int, work_dir: str, scorefile=\"relax_scores.sc\", max_cpus=256) -> dict:\n",
    "    '''\n",
    "    Runs Rosetta Relax application on Slurm.\n",
    "    <poses>: List of paths to pdb-files. Relax runs are run on all poses in this list.\n",
    "    <relax_options>: dictionary, containing Rosetta commandline options. Example: {\"-extra_res_fa\": \"rotlib/path\"}\n",
    "    <relax_flags>: list, containing Rosetta commandline flags. Example: [\"-beta\", \"-ex1\", \"-ex2\"]\n",
    "    <n>: int, overwrites the Rosetta option -nstruct. This is implemented as a separate argument to maximize parallel running CPUs.\n",
    "    <work_dir>: working directory of the relax application.\n",
    "    '''\n",
    "    # setup output_dir and return dictionary\n",
    "    relax_path = find_rosetta_path(_rosetta_paths, \"relax\")\n",
    "    relax_work_dir = os.path.abspath(work_dir)\n",
    "    if not os.path.isdir(relax_work_dir): os.makedirs(relax_work_dir, exist_ok=True)\n",
    "    donefile = f'{relax_work_dir}/done.txt'\n",
    "    \n",
    "    # if relax outputs are already present, skip relax step\n",
    "    return_dict = {\"pdb_dir\": relax_work_dir}\n",
    "    relax_scorefile = f\"{relax_work_dir}/relax_scores.json\"\n",
    "    if os.path.isfile(donefile):\n",
    "        if os.path.isfile(relax_scorefile):\n",
    "            scores = pd.read_json(relax_scorefile)\n",
    "            return_dict[\"scores\"] = scores\n",
    "            print(f\"\\nOutputs of relax run {relax_work_dir} already found at {relax_work_dir}/relax_scores.json\\nSkipping relax step.\")\n",
    "            return return_dict\n",
    "        else:\n",
    "            print(f\"\\nInconsistent relax outputs. Removing old files and rerunning relax.\")\n",
    "            #run(f\"rm {relax_work_dir}/*\", shell=True, stdout=True, stderr=True, check=True)\n",
    "    else:\n",
    "        pass\n",
    "            \n",
    "    # update relax_options and relax_flags with global variables!\n",
    "    relax_options[\"out:file:scorefile\"] = f\"{relax_work_dir}/{scorefile}\"\n",
    "    relax_options[\"out:path:all\"] = relax_work_dir\n",
    "    \n",
    "    # write relax commands for each pose in poses (for <n> relax runs)\n",
    "    relax_cmds = list()\n",
    "    for pose, pose_options_d, pose_flags_l in zip(poses, pose_options, pose_flags):\n",
    "        for i in range(1, n+1):\n",
    "            relax_cmds.append(write_rosetta_cmd(relax_path, pose=pose, i=i, options=relax_options, flags=relax_flags, pose_options=pose_options_d, pose_flags=pose_flags_l))\n",
    "    \n",
    "    #assert len(relax_cmds) <= 1000 # Slurm does not allow jobarrays bigger than 1000 jobs. Don't ask me why...\n",
    "    \n",
    "    # remove Rosetta Scorefile if one exists:\n",
    "    if os.path.isfile(relax_options[\"out:file:scorefile\"]): run(f\"rm {relax_options['out:file:scorefile']}\", shell=True, stderr=True, stdout=True, check=True)\n",
    "\n",
    "    # execute relax commands in a jobarray on maximum <max_cpus>\n",
    "    sbatch_options = [f\"-e {relax_work_dir}/relax_err.log -o {relax_work_dir}/relax_out.log\"]\n",
    "    sbatch_array_jobstarter(cmds=relax_cmds, sbatch_options=sbatch_options, jobname=\"relax_poses\", \n",
    "                            max_array_size=max_cpus, wait=True, remove_cmdfile=False, cmdfile_dir=relax_work_dir)\n",
    "    \n",
    "    # collect relax scores and rename relaxed pdbs.\n",
    "    scores = collect_rosetta_scores(relax_options[\"out:path:all\"], relax_options[\"out:file:scorefile\"])\n",
    "\n",
    "    # write donefile\n",
    "    with open(donefile, 'w') as f:\n",
    "        f.write(\"Done\")\n",
    "    \n",
    "    # return poses as path to pdb_dir and scores as DataFrame \n",
    "    return {\"scores\": scores, \"pdb_dir\": relax_work_dir}\n",
    "\n",
    "def write_rosetta_cmd(path_to_executable: str, pose: str, i: int, options={}, flags=[], pose_options=None, pose_flags=None, force_options=None) -> str:\n",
    "    '''\n",
    "    Write command to run relax on a <pose>. \n",
    "    <i> is the index of the pose when multiple relax trajectories are run (which is default).\n",
    "    This index (<i>) will be added to the beginning of the pdb as a prefix <r0001_>. \n",
    "    Args:\n",
    "        <path_to_executable>       Path to the Rosetta executable that you would like to use \n",
    "        <pose>                     path to the pose on which the executable should be run on\n",
    "        <i>                        Index that will be added to the pose as a prefix (gets corrected by rosetta scorecollector)\n",
    "        <options>                  Global Rosetta options, as a dictionary e.g. {\"out:path:all\": \"my_output/path\"}\n",
    "        <flags>                    Global Rosetta flags, as a list e.g. [\"beta\", \"ex1\", \"ex2\"]\n",
    "        <pose_options>             Pose-level options, as a dictionary e.g. {\"parser:script_vars\": \"resfile=/path/to/resfile\"}\n",
    "        <pose_flags>               Pose-level flags, as a list e.g. [\"ignore_unrecognized_res\", ...]\n",
    "        \n",
    "    Returns:\n",
    "        Command for Rosetta executable that can be run by slurm.\n",
    "        \n",
    "    '''\n",
    "    # if pose_options are set: overwrite general with pose options\n",
    "    if pose_options:\n",
    "        options.update(pose_options)\n",
    "            \n",
    "    # if pose_flags are set: merge general flags with pose_flags\n",
    "    if pose_flags:\n",
    "        flags = list(set(flags) | set(pose_flags))\n",
    "            \n",
    "    # parse rosetta options and flags into string\n",
    "    options[\"out:prefix\"] = f\"r{str(i).zfill(4)}_\"\n",
    "    options_string = \" \".join([f\"-{k} {v}\" for k, v in options.items()])\n",
    "    flags_string = \" \".join([\"-\"+x for x in flags])\n",
    "    if force_options: \n",
    "        if not type(force_options) == str: raise TypeError(f\"parameter <force_option> must be of type str. Type(force_options): {type(force_options)}\")\n",
    "        options_string += \" \" + force_options\n",
    "    \n",
    "    # write and return command\n",
    "    return f\"{path_to_executable} -s {pose} {options_string} {flags_string}\"\n",
    "\n",
    "def parse_rosetta_options_string(relax_options: str) -> tuple:\n",
    "    '''\n",
    "    Takes string (Rosetta commandline options) and parses them into options {key: value} and flags {value}\n",
    "    \n",
    "    Example: '-s input_pdb -constrain_relax_to_start_coords -nstruct 10 -beta '\n",
    "        options: {\"s\": \"input_pdb\", \"nstruct\": \"10\"}\n",
    "        flags: [\"constrain_relax_to_start_coords\", \"beta\"]\n",
    "    '''\n",
    "    firstsplit = [x.strip() for x in re.split(r'^-| -', relax_options) if x]\n",
    "    \n",
    "    options = dict()\n",
    "    flags = list()\n",
    "    for item in firstsplit:\n",
    "        if len((x := item.split())) > 1:\n",
    "            options[x[0]] = \" \".join(x[1:])\n",
    "        else:\n",
    "            flags.append(x[0])\n",
    "    \n",
    "    return options, flags\n",
    "\n",
    "def collect_rosetta_scores(rosetta_work_dir: str, scorefile: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Collects scores and reindeces .pdb files. Stores scores as .json file.\n",
    "    '''\n",
    "    # collect scores from Rosetta Scorefile\n",
    "    scores = pd.read_csv(scorefile, delim_whitespace=True, header=[1], na_filter=True)\n",
    "    \n",
    "    # create reindexed names of relaxed pdb-files: [r0003_pose_unrelaxed_0001.pdb -> pose_unrelaxed_0003.pdb]\n",
    "    scores.rename(columns={\"description\": \"raw_description\"}, inplace=True)\n",
    "    scores.loc[:, \"description\"] = scores[\"raw_description\"].str.split(\"_\").str[1:-1].str.join(\"_\") + \"_\" + scores[\"raw_description\"].str.split(\"_\").str[0].str.replace(\"r\", \"\")\n",
    "    \n",
    "    # wait for all Rosetta output files to appear in the output directory (for some reason, they are sometimes not there after the runs completed.)\n",
    "    while len(glob(f\"{rosetta_work_dir}/r*.pdb\")) < len(scores):\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # rename .pdb files in work_dir to the reindexed names.\n",
    "    names_dict = scores[[\"raw_description\", \"description\"]].to_dict()\n",
    "    print(f\"Renaming and reindexing {len(scores)} Rosetta output .pdb files\")\n",
    "    for oldname, newname in zip(names_dict[\"raw_description\"].values(), names_dict[\"description\"].values()):\n",
    "        shutil.move(f\"{rosetta_work_dir}/{oldname}.pdb\", (nf := f\"{rosetta_work_dir}/{newname}.pdb\"))\n",
    "        if not os.path.isfile(nf):\n",
    "            print(f\"WARNING: Could not rename file {oldname} to {nf}\\n Retrying renaming.\")\n",
    "            shutil.move(f\"{rosetta_work_dir}/{oldname}.pdb\", (nf := f\"{rosetta_work_dir}/{newname}.pdb\"))\n",
    "    \n",
    "    # Collect information of path to .pdb files into dataframe under \"location\" column\n",
    "    scores.loc[:, \"location\"] = rosetta_work_dir + \"/\" + scores[\"description\"] + \".pdb\"\n",
    "\n",
    "    # safetycheck rename all remaining files with r*.pdb into proper filename:\n",
    "    if (remaining_r_pdbfiles := glob(f\"{rosetta_work_dir}/r*.pdb\")):\n",
    "        for pdb_path in remaining_r_pdbfiles:\n",
    "            pdb_path = pdb_path.split(\"/\")[-1]\n",
    "            idx = pdb_path.split(\"_\")[0].replace(\"r\", \"\")\n",
    "            new_name = \"_\".join(pdb_path.split(\"_\")[1:-1]).replace(\".pdb\", \"\") + \"_\" + idx + \".pdb\"\n",
    "            shutil.move(f\"{rosetta_work_dir}/{pdb_path}\", f\"{rosetta_work_dir}/{new_name}\")\n",
    "    \n",
    "    # reset index and write scores to file\n",
    "    scores.reset_index(drop=\"True\", inplace=True)\n",
    "    scores.to_json(scorefile.replace(\".sc\", \".json\"))\n",
    "    \n",
    "    return scores\n",
    "\n",
    "## ----------------------- Structure Prediction -----------------------------------------------\n",
    "\n",
    "###### ------------------- AlphaFold2 Functions -----------------------------------------------\n",
    "\n",
    "def run_AlphaFold2(poses: list, af2_options: dict, max_gpus=10, calc_rmsd=False):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # drop 'output_dir' from of_options and prepare directories\n",
    "    af_work_dir = af2_options.pop(\"output_dir\")\n",
    "    af_output_dir = af_work_dir + \"/af2_preds\"\n",
    "    pdb_dir = af_work_dir + \"/af2_pdbs\"\n",
    "    return_dict = {\"pdb_dir\": pdb_dir}\n",
    "    af2_scorefile = f\"{af_work_dir}/af2_scores.json\"\n",
    "    \n",
    "    # check if AlphaFold2 output is already present, if so, skip\n",
    "    if os.path.isfile(af2_scorefile):\n",
    "        print(f\"\\nOutputs of prediction run {af_output_dir} already found at {af2_scorefile}\\nSkipping prediction step.\")\n",
    "        return_dict[\"scores\"] = pd.read_json(af2_scorefile)\n",
    "        return return_dict\n",
    "\n",
    "    # create output directories \n",
    "    print(f\"\\nPredicting {len(poses)} Structures at {af_output_dir}\")\n",
    "    af_input = af_work_dir + \"/af_input/\"\n",
    "    os.makedirs(af_output_dir, exist_ok=True)\n",
    "    os.makedirs(af_input, exist_ok=True)\n",
    "    \n",
    "    # split poses list into <max_gpus> sublists and write them to files as input to Alphafold.\n",
    "    splitnum = len(poses) if len(poses) < max_gpus else max_gpus\n",
    "    poses_split = [list(x) for x in np.array_split(poses, int(splitnum))]\n",
    "    pose_fastas = [mergefastas(poses, f\"{af_input}/af_fasta_{str(i+1).zfill(4)}.fa\", replace_colon=True) for i, poses in enumerate(poses_split)]\n",
    "    \n",
    "    # write AlphaFold cmds\n",
    "    af_cmds = [write_af2_cmd(fasta, af_output_dir, af2_options) for fasta in pose_fastas]\n",
    "    \n",
    "    # sbatch AlphaFold\n",
    "    sbatch_options = [f\"--gpus-per-node 1 -c2 -e {af_output_dir}/AF_err.log -o {af_output_dir}/AF_out.log\"]\n",
    "    sbatch_array_jobstarter(cmds=af_cmds, sbatch_options=sbatch_options, jobname=\"af2_predict\", \n",
    "                            max_array_size=max_gpus, wait=True, remove_cmdfile=False, cmdfile_dir=af_output_dir)\n",
    "    \n",
    "    # collect scores and pdbs\n",
    "    return_dict[\"scores\"] = collect_af2_scores(af_output_dir, pdb_dir=pdb_dir, scorefile=af2_scorefile, calc_rmsd=calc_rmsd)\n",
    "    \n",
    "    return return_dict\n",
    "\n",
    "def summarize_af2_json(input_json):\n",
    "    '''\n",
    "    Takes raw AF2_scores.json file and calculates mean pLDDT over the entire structure, also puts perresidue pLDDTs and paes in list.\n",
    "    \n",
    "    Returns pd.DataFrame\n",
    "    '''\n",
    "    df = pd.read_json(input_json)\n",
    "    means = df.mean(numeric_only=True).to_frame().T\n",
    "    means[\"pldtt_list\"] = [df[\"plddt\"]]\n",
    "    means[\"pae_list\"] = [df[\"pae\"]]\n",
    "    means[\"json_file\"] = input_json\n",
    "    return means\n",
    "\n",
    "def calc_statistics_over_AF2_models(index, input_list: list, calc_rmsd=None, calc_motif_rmsd=None) -> list:\n",
    "    '''\n",
    "    takes list of .json files from af2_predictions and collects scores (mean_plddt, max_plddt, etc.)\n",
    "\n",
    "    <calc_motif_rmsd>: Has to be list of residue IDs that comprise the motif. Example (calc_motif_rmsd=[1, 2, 10, 15])\n",
    "    '''\n",
    "    df = pd.concat([summarize_af2_json(x) for x in input_list], ignore_index=True)\n",
    "    means = df.mean(numeric_only=True).to_frame().T.add_prefix(\"mean_\")\n",
    "    stds = df.std(numeric_only=True).to_frame().T.add_prefix(\"std_\")\n",
    "    top = df[df[\"json_file\"].str.split(\"/\").str[-1].str.contains(\"rank_1_\", regex=True, na=False)].add_prefix(\"top_\").reset_index(drop=True)\n",
    "    return_df = pd.concat([top, means, stds], axis=1)\n",
    "    return_df[\"description\"] = index\n",
    "\n",
    "    # If calc_rmsd option is set, then calculate bb_ca_rmsd of each model to the rank_1 model.\n",
    "    if calc_rmsd:\n",
    "        return_df[\"mean_af2_ca_rmsd\"] = calc_af2_rmsd(df)\n",
    "\n",
    "    if calc_motif_rmsd:\n",
    "        assert type(calc_motif_rmsd) == list # calc_motif_rmsd has to be list with motif IDs, e.g.: [1, 2, 10, 15]\n",
    "        return_df[\"mean_af2_motif_heavy_rmsd\"] = calc_af2_motif_rmsd(df, calc_motif_rmsd)\n",
    "\n",
    "    return return_df.set_index(\"description\")\n",
    "\n",
    "def calc_af2_rmsd(df: pd.DataFrame) -> float:\n",
    "    '''\n",
    "    Input:\n",
    "        <df> pd.DataFrame, has to be DataFrame of AF2 prediction (collected by summarize_af2_json)\n",
    "    Returns mean_ca_rmsd of rank_1 to all other models.\n",
    "    '''\n",
    "    # separate rank_1 from the rest.\n",
    "    df.loc[:, \"pdb\"] = df[\"json_file\"].str.replace(\"_scores.json\", \".pdb\", regex=False)\n",
    "    ref = list(df[df[\"pdb\"].str.contains(\"rank_1_\", regex=True, na=False)][\"pdb\"])[0]\n",
    "    targets = list(df[~df[\"pdb\"].str.contains(\"rank_1_\", regex=True, na=False)][\"pdb\"])\n",
    "\n",
    "    # calculate rmsd of \"targets\" to rank_1\n",
    "    rmsds = [bb_rmsd.superimpose_calc_rmsd(ref, target) for target in targets]\n",
    "\n",
    "    return np.mean(rmsds)\n",
    "\n",
    "def calc_af2_motif_rmsd(df: pd.DataFrame, motif: list) -> dict:\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    # separate rank_1 from the rest.\n",
    "    df.loc[:, \"pdb\"] = df[\"json_file\"].str.replace(\"_scores.json\", \".pdb\", regex=False)\n",
    "    ref = list(df[df[\"pdb\"].str.contains(\"rank_1_\", regex=True, na=False)][\"pdb\"])[0]\n",
    "    targets = list(df[~df[\"pdb\"].str.contains(\"rank_1_\", regex=True, na=False)][\"pdb\"])\n",
    "\n",
    "    # calculate motif_rmsd\n",
    "    rmsds = [motif_rmsd.superimpose_calc_rmsd_heavy(ref, target, motif, motif) for target in targets]\n",
    "\n",
    "    return np.mean(rmsds)\n",
    "\n",
    "def collect_af2_pdb(description, pdb, pdb_dir):\n",
    "    '''\n",
    "    Takes pdb-file of af2_localcolabfold output and copies it (removing the rank_1 stuff..) into <pdb_dir>\n",
    "    \n",
    "    Returns new location of pdb file in <pdb_dir>.\n",
    "    '''\n",
    "    new_pdb_path = f\"{pdb_dir}/{description}.pdb\"\n",
    "    run(f\"cp {pdb} {new_pdb_path}\", shell=True, stdout=True, stderr=True, check=True)\n",
    "    return new_pdb_path\n",
    "\n",
    "def collect_af2_scores(af_output_dir, pdb_dir, scorefile=f\"af2_scores.json\", calc_rmsd=False, calc_motif_rmsd=None):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # create pdb_dir\n",
    "    if not os.path.isdir(pdb_dir): os.makedirs(pdb_dir, exist_ok=True)\n",
    "    \n",
    "    # collect all unique 'descriptions' leading to predictions\n",
    "    descriptions = [x.split(\"/\")[-1].replace(\".done.txt\", \"\") for x in glob(f\"{af_output_dir}/*/*.done.txt\")]\n",
    "    if not descriptions: raise FileNotFoundError(f\"ERROR: No AF2 prediction output found at {af_output_dir} Are you sure it was the correct path?\")\n",
    "    \n",
    "    # Collect all .json files of each 'description' into a dictionary. (This collects scores from all 5 models)\n",
    "    scores_dict = {description: [x for x in glob(f\"{af_output_dir}/*/{description}*rank*.json\") if re.search(f\"{description}_.?.?relaxed_rank_._model_._scores\\.json\", x)] for description in descriptions}\n",
    "    \n",
    "    # Calculate statistics over prediction scores for each of the five models.\n",
    "    af2_preds_df_list = [calc_statistics_over_AF2_models(description, json_files, calc_rmsd=calc_rmsd, calc_motif_rmsd=calc_motif_rmsd) for description, json_files in scores_dict.items()]\n",
    "    \n",
    "    # concatenate all individual score DataFrames into one single (and central) DataFrame:\n",
    "    scores_df = pd.concat(af2_preds_df_list)\n",
    "    \n",
    "    # Copy *_rank_1*.pdb files to pdb_dir and store location in DataFrame\n",
    "    scores_df = scores_df.reset_index()\n",
    "    scores_df.loc[:, \"location\"] = [collect_af2_pdb(desc, jsonf.replace(\"_scores.json\", \".pdb\"), pdb_dir) for desc, jsonf in zip(scores_df[\"description\"], scores_df[\"top_json_file\"])]\n",
    "\n",
    "    scores_df.to_json(scorefile)\n",
    "    \n",
    "    return scores_df\n",
    "\n",
    "def write_af2_cmd(fa_file, af_output_dir, options):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "     # create of_output_dir/fa_file\n",
    "    af_output_dir = f\"{af_output_dir}/{fa_file.split('/')[-1].replace('.fa', '')}\"\n",
    "    print(f\"Creating AlphaFold2 output directory at {af_output_dir}\")\n",
    "    os.makedirs(af_output_dir, exist_ok=True)\n",
    "    \n",
    "    options = \" \".join([f\"--{k} {v}\" for k, v in options.items()])\n",
    "    af_cmd = f\"{_af2_path}colabfold_batch {options} {fa_file} {af_output_dir}\"\n",
    "    return af_cmd\n",
    "\n",
    "# ---------------------------- ESMFold Functions --------------------------------------------\n",
    "\n",
    "def run_ESMFold(poses: list, esm_options: dict, max_gpus:int=10) -> dict:\n",
    "    '''\n",
    "    Runs <ESMFold> on poses with <esm_options> using maximum <max_gpus>\n",
    "    '''\n",
    "    # setup work_dir\n",
    "    work_dir = os.path.abspath(esm_options.pop(\"output_dir\"))\n",
    "    out_dict = {\"pdb_dir\": (pdb_dir := f\"{work_dir}/esm_pdbs/\")}\n",
    "    \n",
    "    # check if results of run are already present, if so, skip prediction:\n",
    "    if os.path.isfile((scorefile := f\"{work_dir}/ESMFold_scores.json\")):\n",
    "        print(f\"\\nOutputs of prediction run {work_dir} already found at {scorefile}\\nSkipping prediction step.\")\n",
    "        out_dict[\"scores\"] = pd.read_json(scorefile)\n",
    "        return out_dict\n",
    "    \n",
    "    # create output_dir and directory for input fasta-files:\n",
    "    os.makedirs(pdb_dir, exist_ok=True)\n",
    "    os.makedirs((fasta_dir := f\"{work_dir}/input_fastas\"), exist_ok=True)\n",
    "    os.makedirs((esm_preds_dir := f\"{work_dir}/esm_preds\"), exist_ok=True)\n",
    "    \n",
    "    # split poses list into <max_gpus> sublists and write them to files as input for ESMFold\n",
    "    pose_fastas = prep_fastas_for_prediction(poses=poses, fasta_dir=fasta_dir, max_filenum=max_gpus)\n",
    "\n",
    "    # write ESMFold commands:\n",
    "    esm_cmds = [write_esm_cmd(fasta, esm_preds_dir, esm_options) for fasta in pose_fastas]\n",
    "    \n",
    "    # sbatch ESMFold:\n",
    "    sbatch_options = [f\"--gpus-per-node 1 -c1 -e {work_dir}/esm_err.log -o {work_dir}/esm_out.log\"]\n",
    "    sbatch_array_jobstarter(cmds=esm_cmds, sbatch_options=sbatch_options, jobname=\"esmfold\",\n",
    "                            max_array_size=max_gpus, wait=True, remove_cmdfile=False, cmdfile_dir=work_dir)\n",
    "    \n",
    "    # collect scores and pdbs\n",
    "    out_dict[\"scores\"] = collect_esm_scores(esm_preds_dir, pdb_dir=pdb_dir, scorefile=scorefile)\n",
    "    \n",
    "    return out_dict\n",
    "\n",
    "def write_esm_cmd(fa_file: str, output_dir: str, options: dict) -> str:\n",
    "    '''\n",
    "    Writes commandlines for ESMFold.\n",
    "    '''\n",
    "    # overwrite options with globally set ESMFold options!\n",
    "    options = dict(options, **_esm_opts)\n",
    "    options = \" \".join([f\"--{k} {v}\" for k, v in options.items()])\n",
    "    return f\"{_esmfold_inference_script} --fasta {fa_file} --output_dir {output_dir} {options}\"\n",
    "\n",
    "def collect_esm_scores(esm_preds_dir: str, pdb_dir: str, scorefile: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Collects output scores from ESM.\n",
    "    '''\n",
    "    # collect all .json files\n",
    "    fl = glob(f\"{esm_preds_dir}/*/*.json\")\n",
    "    \n",
    "    # read the files, add origin column, and concatenate into single DataFrame:\n",
    "    dl = [pd.read_json(f) for f in fl]\n",
    "    for d, f in zip(dl, fl):\n",
    "        d.loc[:, f\"{[x for x in esm_preds_dir.split('/') if x][-1]}_path\"] = f\n",
    "    df = pd.concat(dl).reset_index(drop=True)\n",
    "    \n",
    "    # collect pdbs into pdb_dir\n",
    "    pdbs = glob(f\"{esm_preds_dir}/*/*.pdb\")\n",
    "    [shutil.copy(pdb, pdb_dir+\"/\") for pdb in glob(f\"{esm_preds_dir}/*/*.pdb\")]\n",
    "    print(f\"Collecting {str(len(pdbs))} predicted pdb-files into {pdb_dir}\")\n",
    "    \n",
    "    # store location of .pdb files in output dataframe as \"location\":\n",
    "    df.loc[:, \"location\"] = f\"{pdb_dir}/\" + df[\"description\"] + \".pdb\"\n",
    "    df.to_json(scorefile)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prep_fastas_for_prediction(poses: list[str], fasta_dir: str, max_filenum: int) -> list[str]:\n",
    "    '''\n",
    "    Args:\n",
    "        <poses>             List of paths to *.fa files\n",
    "        <fasta_dir>         Directory to which the new fastas should be written into\n",
    "        <max_filenum>          Maximum number of *.fa files that should be written\n",
    "    '''\n",
    "    # determine how to split the poses into <max_gpus> fasta files:\n",
    "    splitnum = len(poses) if len(poses) < max_filenum else max_filenum\n",
    "    poses_split = [list(x) for x in np.array_split(poses, int(splitnum))]\n",
    "    \n",
    "    # Write fasta files according to the fasta_split determined above and then return:\n",
    "    return [mergefastas(poses, f\"{fasta_dir}/fasta_{str(i+1).zfill(4)}.fa\", replace=(\"/\",\":\")) for i, poses in enumerate(poses_split)]\n",
    "\n",
    "#----------------------------- OmegaFold Functions ------------------------------------------\n",
    "\n",
    "def run_OmegaFold(poses: list, of_options: dict, max_gpus=10):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    of_work_dir = of_options.pop(\"output_dir\")\n",
    "    of_output_dir = of_work_dir + \"/of_preds\"\n",
    "    pdb_dir = of_work_dir + \"/of_pdbs\"\n",
    "    scorefile = f\"{of_work_dir}/of_scores.json\"\n",
    "    return_dict = {\"pdb_dir\": pdb_dir}\n",
    "    \n",
    "    # check if omegafold output is already present, if so, skip\n",
    "    if os.path.isfile(scorefile):\n",
    "        print(f\"\\nOutputs of prediction run {of_output_dir} already found at {scorefile}\\nSkipping prediction step.\")\n",
    "        return_dict[\"scores\"] = pd.read_json(scorefile)\n",
    "        return return_dict\n",
    "\n",
    "    # drop 'output_dir' from of_options and create output_dir:\n",
    "    print(f\"\\nPredicting {len(poses)} Structures at {of_output_dir}\")   \n",
    "    of_input = of_work_dir + \"/of_input/\"\n",
    "    os.makedirs(of_output_dir, exist_ok=True)\n",
    "    os.makedirs(of_input, exist_ok=True)\n",
    "    \n",
    "    # split poses list into <max_gpus> sublists and write them to files as input to omegafold.\n",
    "    splitnum = len(poses) if len(poses) < max_gpus else max_gpus\n",
    "    poses_split = [list(x) for x in np.array_split(poses, int(splitnum))]\n",
    "    pose_fastas = [mergefastas(poses, f\"{of_input}/of_fasta_{str(i+1).zfill(4)}.fa\", replace_colon=True) for i, poses in enumerate(poses_split)]\n",
    "    \n",
    "    # write omegafold cmds\n",
    "    of_cmds = [write_of_cmd(fasta, of_output_dir, of_options) for fasta in pose_fastas]\n",
    "    \n",
    "    # sbatch omegafold\n",
    "    sbatch_options = [f\"--gpus-per-node 1 -c2 -e {of_output_dir}/OF_err.log -o {of_output_dir}/OF_out.log\"]\n",
    "    sbatch_array_jobstarter(cmds=of_cmds, sbatch_options=sbatch_options, jobname=\"of_predict\", \n",
    "                            max_array_size=max_gpus, wait=True, remove_cmdfile=False, cmdfile_dir=of_output_dir)\n",
    "    \n",
    "    # collect scores and pdbs\n",
    "    return_dict[\"scores\"] = collect_omegafold_scores(of_output_dir, pdb_dir=pdb_dir, scorefile=scorefile)\n",
    "    \n",
    "    return return_dict\n",
    "\n",
    "def mergefastas(files: list, path: str, replace_colon=False, replace=None) -> str: \n",
    "    '''\n",
    "    Merges Fastas located in <files> into one single fasta-file called <path>\n",
    "    '''\n",
    "    fastas = list()\n",
    "    for f in files:\n",
    "        with open(f, 'r') as f:\n",
    "            fastas.append(f.read().strip())\n",
    "    \n",
    "    if replace_colon: fastas = [x.replace(\"/\", \":\") for x in fastas]\n",
    "    if replace: fastas = [x.replace(replace[0], replace[1]) for x in fastas]\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(\"\\n\".join(fastas))\n",
    "    \n",
    "    return path\n",
    "\n",
    "def write_of_cmd(fa_file, of_output_dir, options):\n",
    "    '''\n",
    "    Writes command for OmegaFold that can be executed by sbatch_array_jobstarter.\n",
    "    '''\n",
    "    # create of_output_dir/fa_file\n",
    "    of_output_dir = f\"{of_output_dir}/{fa_file.split('/')[-1].replace('.fa', '')}\"\n",
    "    print(f\"Creating OmegaFold output directory at {of_output_dir}\")\n",
    "    os.makedirs(of_output_dir, exist_ok=True)\n",
    "    \n",
    "    options = \" \".join([f\"--{k} {v}\" for k, v in options.items()])\n",
    "    of_cmd = f\"{_python_path} {_of_path} {options} {fa_file} {of_output_dir}\"\n",
    "    return of_cmd\n",
    "\n",
    "def collect_omegafold_scores(input_dir, pdb_dir=\"../of_pdbs\", scorefile=None):\n",
    "    '''\n",
    "    Goes into each prediction subfolder in your omegafold prediction directory, combines the scores and writes them into a single .json scorefile if the <scorefile> option is set.\n",
    "\n",
    "    Returns the dictionary with all combined scores.\n",
    "    '''\n",
    "    # Glob all .json files in input_dir.\n",
    "    fl = glob(f\"{input_dir}/*/*.json\")\n",
    "\n",
    "    # read the files with pandas, add \"origin\" column!\n",
    "    dl = [pd.read_json(f) for f in fl]\n",
    "    for d, f in zip(dl, fl):\n",
    "        d.loc[:, f\"{[x for x in input_dir.split('/') if x][-1]}_path\"] = f\n",
    "    df = pd.concat(dl).reset_index(drop=True)\n",
    "\n",
    "    # collect pdbs into of_pdbs and store their location into the scorefile:\n",
    "    \n",
    "    predicted_pdbs = glob(f\"{input_dir}/*/*.pdb\")\n",
    "    print(f\"Collecting {str(len(predicted_pdbs))} predicted pdb-files into {pdb_dir}\")\n",
    "    os.makedirs(pdb_dir, exist_ok=True)\n",
    "    for pdb in predicted_pdbs:\n",
    "        run(f\"cp {pdb} {pdb_dir}/\", shell=True, stdout=True, stderr=True, check=True)\n",
    "    \n",
    "    df.loc[:, \"location\"] = f\"{pdb_dir}/\" + df[\"description\"] + \".pdb\"\n",
    "    \n",
    "    # if scorefile is set, write scorefile\n",
    "    if scorefile:\n",
    "        print(f\"Writing {len(df)} scores into {scorefile}.\")\n",
    "        df.to_json(scorefile)\n",
    "    \n",
    "    return df\n",
    "\n",
    "## ------------------------- Metrics ----------------------------------------\n",
    "\n",
    "def calc_bb_rmsd(poses: list, bb_rmsd_args: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "    Calculates bb_ca_rmsd of all poses to ref_poses in <bb_rmsd_args>\n",
    "    \n",
    "    <bb_rmsd_args>:         Dictionary with options for bb_rmsd function. Has to contain:\n",
    "                                \"ref_pdb\": Path to either a directory containing pdb files or a single pdb file that should be taken as reference for RMSD calculation. Will be matched by description.\n",
    "                            Optional:\n",
    "                                \"rmsd_scorename\": Name of the score you want to add. Default: bb_ca_rmsd\n",
    "                                \"remove_layers\"\n",
    "    '''\n",
    "    if not bb_rmsd_args[\"rmsd_scorename\"]: bb_rmsd_args[\"rmsd_scorename\"] = \"bb_ca_rmsd\"\n",
    "    if not bb_rmsd_args[\"remove_layers\"]: bb_rmsd_args[\"remove_layers\"] = 0\n",
    "    \n",
    "    # setup rmsd_dict\n",
    "    rmsd_dict = {\"description\": [], bb_rmsd_args[\"rmsd_scorename\"]: []}\n",
    "    \n",
    "    # This only works, if poses and ref_pdbs are already an aligned list. which they are not!!!\n",
    "    if bb_rmsd_args[\"ref_pdb\"].endswith(\".pdb\"):\n",
    "        ref_pdbs = [bb_rmsd_args[\"ref_pdb\"]]\n",
    "    for pose, ref_pdb in zip(poses, ref_pdbs):\n",
    "        pose_description = pose.split(\"/\")[-1].replace(\".pdb\", \"\", regex=False)\n",
    "        ref_pose = [x for x in bb_rmsd_args[\"ref_pdbs\"] if x.replace(\".pdb\", \"\", regex=True).endswith(pose_description)][0]\n",
    "        rmsd_dict[\"description\"].append(pose)\n",
    "        rmsd_dict[bb_rmsd_args[\"rmsd_scorename\"]].append(bb_rmsd.superimpose_calc_rmsd(ref_pose, pose, atoms=[\"CA\"]))\n",
    "    \n",
    "    scores_df = pd.DataFrame(rmsd_dict)\n",
    "    return scores_df\n",
    "\n",
    "# -------------------------- Misc -------------------------------------------\n",
    "\n",
    "def update_df(new_df: pd.DataFrame, new_df_col: str, old_df: pd.DataFrame, old_df_col: str, new_df_col_remove_layer=0, sep=\"_\") -> pd.DataFrame:\n",
    "    '''\n",
    "    Fill all rows of <new_df> where [new_df_col == old_df_col] with values from <old_df>\n",
    "    \n",
    "    Args:\n",
    "        <new_df>                      New DataFrame\n",
    "        <new_df_col>                  Column in the new DataFrame that contains the 'index' for merging scores from the old DataFrame\n",
    "        <old_df>                      Old DataFrame\n",
    "        <old_df_col>                  Column in the old DataFrame that conatins the 'index' for copying its scores into new DataFrame\n",
    "        <new_df_col_remove_layer>     How many index layers need to be removed from every item in <new_df_col> to reach the name in <old_df_col>\n",
    "        <sep>                         Index separator, Default=\"_\"\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame where scores from old_df were copied into new_df.\n",
    "    '''\n",
    "    startlen = len(new_df)\n",
    "    # Remove layers if option is set\n",
    "    if new_df_col_remove_layer: new_df[\"select_col\"] = new_df[new_df_col].str.split(sep).str[:-1*new_df_col_remove_layer].str.join(sep)\n",
    "    else: new_df[\"select_col\"] = new_df[new_df_col]\n",
    "    \n",
    "    new_df = new_df.merge(old_df, left_on='select_col', right_on=old_df_col)\n",
    "    new_df.drop(columns=\"select_col\", inplace=True)\n",
    "    \n",
    "    if len(new_df) == 0: raise ValueError(f\"ERROR: Merging DataFrames failed. This means there was no overlap found between old_df[old_df_col] and new_df[new_df_col]\")\n",
    "    if len(new_df) < startlen: raise ValueError(f\"ERROR: Merging DataFrames failed. Some rows in new_df[new_df_col] were not found in old_df[old_df_col]\")\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def search_for_path(pathlist: list, glob_pattern: str) -> str:\n",
    "    '''\n",
    "    Checks if file is in any path given in <pathlist> and returns the first instance it finds.\n",
    "    '''\n",
    "    # glob through all paths in pathlist and look if there is a file that matches regex\n",
    "    pl = [glob(f\"{path}/{glob_pattern}\") for path in pathlist]\n",
    "    return pl\n",
    "\n",
    "def find_rosetta_path(pathlist: list, executable: str, stringent=False) -> str:\n",
    "    '''\n",
    "    Function designated to find paths to Rosetta executables regardless of static or dynamic compilation (static- or default.linuxgccrelease)\n",
    "    \n",
    "    Example: relax_path = find_rosetta_path('/home/florian_wieser/Rosetta/', 'relax')\n",
    "    Returns: relax_path = '/home/florian_wieser/Rosetta/main/source/bin/relax.default.linuxgccrelease'\n",
    "    '''\n",
    "    # find all relax executables in all paths in <pathlist> and put them in onedimensional list:\n",
    "    if stringent:\n",
    "        searchpaths = [f\"{path}/main/source/bin/{executable}\" for path in pathlist]\n",
    "    else:\n",
    "        searchpaths = [f\"{path}/main/source/bin/{executable}.*\" for path in pathlist]\n",
    "    pl = sum([glob(path) for path in searchpaths], [])\n",
    "    if not pl: raise FileNotFoundError(f'ERROR: no Rosetta executables were found in pathlist {\", \".join(searchpaths)}')\n",
    "    \n",
    "    # pick either default, or static version of executable, depending on which is available.\n",
    "    for path in pl:\n",
    "        if 'default' in path:\n",
    "            print(f\"Rosetta executable {path} will be used for relax.\")\n",
    "            return path\n",
    "        elif 'static' in path:\n",
    "            print(f\"Rosetta executable {path} will be used for relax.\")\n",
    "            return path\n",
    "    raise FileNotFoundError(f\"ERROR: No usable Rosetta executable was found in any of the specified paths. check Rosetta path variable _rosetta_paths\")\n",
    "\n",
    "def collect_scorefiles_from_dir(path_to_dir: str, file_extension='sc', index=None, silent=None) -> pd.DataFrame:\n",
    "    '''\n",
    "    Reads all files that end with extension <file_extension> and collects them into a singular dataframe.\n",
    "    '''\n",
    "    # compile regex pattern:\n",
    "    regex_pattern = '^.*\\.' + file_extension + '$'\n",
    "    \n",
    "    # Collect Dataframes from files that match <regex> into List.\n",
    "    df_list = list()\n",
    "    for i in os.listdir(path_to_dir):\n",
    "        if re.match(regex_pattern, i):\n",
    "            filename = path_to_dir + i\n",
    "            if not silent:\n",
    "                print(f\"Scores collected from file: {filename}\")\n",
    "            if file_extension == 'sc':\n",
    "                re_df = pd.read_csv(filename, delim_whitespace=True, header=[1], na_filter=True)\n",
    "            elif file_extension == 'json':\n",
    "                re_df = pd.read_json(filename)\n",
    "            else:\n",
    "                raise TypeError(f\"ERROR: Unsupported file extension for 'collect_scores_from_dir': {file_extension}\\nPick different file-extension!\")\n",
    "            re_df[\"scorefile_path\"] = filename\n",
    "            df_list.append(re_df)\n",
    "    \n",
    "    # Merge List and return\n",
    "    merged_df = pd.concat(df_list).reset_index(drop=True)\n",
    "    if index: merged_df.set_index(index)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def filter_dataframe(df: pd.DataFrame, col: str, n, remove_layers=None, layer_col=\"description\", sep=\"_\", ascending=True) -> pd.DataFrame:\n",
    "    '''\n",
    "    remove_layers option allows to filter dataframe based on groupings after removing index layers.\n",
    "    If the option remove_layers is set (has to be type: int), then n determines how many c\n",
    "    '''\n",
    "    # make sure <col> and <layer_col> (if remove_layers is set) are existing columns in <df>\n",
    "    \n",
    "    # if remove_layers is set, compile list of unique pose descriptions after removing one index layer:\n",
    "    if remove_layers:\n",
    "        # TODO: make sure that df[layer_col] returns a Series and not a DataFrame (layer_col must be a unique column)\n",
    "        if type(remove_layers) != int: raise TypeError(f\"ERROR: only value of type 'int' allowed for remove_layers. You set it to {type(remove_layers)}\")\n",
    "        unique_list = list(df[layer_col].str.split(sep).str[:-1*int(remove_layers)].str.join(sep).unique())\n",
    "        \n",
    "        # go through each unique name in unique list, select the matching rows in df\n",
    "        unique_dfs = [select_rows_by_regex_in_column_x(df, layer_col, unique_name) for unique_name in unique_list]\n",
    "        \n",
    "        # filter them down to the number specified by n and concatenate back into one df\n",
    "        filtered_df = pd.concat([unique_df.sort_values(by=col, ascending=ascending).head(determine_filter_n(unique_df, n)) for unique_df in unique_dfs]).reset_index(drop=True)\n",
    "    \n",
    "    else:\n",
    "        filtered_df = df.sort_values(by=col, ascending=ascending).head(determine_filter_n(df, n))\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "def select_rows_by_regex_in_column_x(df, col, regex_pattern):\n",
    "    '''    \n",
    "    Selects all rows of <dataframe> in which column <column_name> contains the <regex_pattern>.\n",
    "    \n",
    "    Returns: pd.DataFrame\n",
    "    '''\n",
    "    if col not in df.columns:\n",
    "        raise KeyError(f'Scoreterm {col} not found in this Score_Table. Available scoreterms:\\n{[x for x in df.columns]}')\n",
    "    \n",
    "    return_df = df[df[col].str.contains(regex_pattern, regex=True, na=False)]\n",
    "    if return_df.empty:\n",
    "        raise KeyError(f\"Your DataFrame is Empty, regex '{regex_pattern}' was not found in specified column: '{col}'. Change Column, or Regex\")\n",
    "    return return_df\n",
    "\n",
    "def determine_filter_n(df: pd.DataFrame, n: float) -> int:\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    filter_n = float(n)\n",
    "    if filter_n < 1:\n",
    "        filter_n = round(len(df) * filter_n)\n",
    "    elif filter_n <= 0:\n",
    "        raise ValueError(f\"ERROR: Argument <n> of filter functions cannot be smaller than 0. It has to be positive number. If n < 1, the top n fraction is taken from the DataFrame. if n > 1, the top n rows are taken from the DataFrame\")\n",
    "    \n",
    "    return int(filter_n)\n",
    "\n",
    "def parse_options_string(options_string: str, sep='-', flags=False) -> dict:\n",
    "        '''\n",
    "        Parses an options string ('--option_A var_A --option_B var_B') into a dictionary:\n",
    "        {\"option_A\": \"var_A\", \"option_B\": \"var_B\"}\n",
    "        \n",
    "        WARNING: if options contain dash '-' for example in colabofold_batch (--msa-mode --stop-at-score etc.), then you should provide\n",
    "                 the <sep> argument. In this cas it would be: sep=\"--\". Otherwise the function will split at all '-' characters and the\n",
    "                 AF2 options will not be parsed correctly.\n",
    "        \n",
    "        Note: If no separator is given, the function also works if optio/ns start with single dash: -option_A var_A -option_B var_B\n",
    "        \n",
    "        Passing the <sep> argument is recommended.\n",
    "        \n",
    "        Returns dictionary with {option: value}\n",
    "        '''\n",
    "        # if options_string was passed with None, return empty dictionary:\n",
    "        if options_string == None:\n",
    "            return {}\n",
    "\n",
    "        # if options_string is a string, split around (sep) and parse options into a list. Convert all first occurances of \"=\" of each option into \" \".\n",
    "        l = [x.replace(\"=\", \" \", 1).strip().split(\" \", 1) for x in options_string.split(sep) if x]\n",
    "\n",
    "        return {k: v for k, v in l}\n",
    "    \n",
    "def parse_options_flags(options: str, sep=\"-\"):\n",
    "    '''\n",
    "    Parses an options string with flags ('--option_A var_A --option_B var_B --dump_all --calc_rmsd') into a dictionary and a list:\n",
    "    {\"option_A\": \"var_A\", \"option_B\": \"var_B\"}, [\"dump_all\", \"calc_rmsd\"]\n",
    "\n",
    "    WARNING: if options contain dash '-' for example in colabofold_batch (--msa-mode --stop-at-score etc.), then you should provide\n",
    "             the <sep> argument. In this cas it would be: sep=\"--\". Otherwise the function will split at all '-' characters and the\n",
    "             AF2 options will not be parsed correctly.\n",
    "\n",
    "    Note: If no separator is given, the function also works if options start with single dash: -option_A var_A -option_B var_B\n",
    "\n",
    "    Passing the <sep> argument is recommended.\n",
    "\n",
    "    Returns dictionary and a list with {option: value}, [flags]\n",
    "    '''\n",
    "    # sanity check:\n",
    "    if \"=\" in sep: raise ValueError(f\"ERROR: Symbol '=' cannot be used in argument sep! sep = '{sep}'\")\n",
    "    if type(options) != str: raise ValueError(f\"ERROR: incorrect type of argument <options>. Only str allowed. Type = {type(options)}\")\n",
    "    \n",
    "    # return empty dictionary and list for options and flags if no options were passed.\n",
    "    if options == None: \n",
    "        return {}, []\n",
    "        \n",
    "    # run first split across separator and replace any first occurences of '=' in the options string with ' ' \n",
    "    firstsplit = [x.replace(\"=\", \" \", 1).strip() for x in options.split(sep) if x]\n",
    "    \n",
    "    # split options list (firstsplit) into options and flags based on if they are separatable by \" \" (whitespace):\n",
    "    opts = dict()\n",
    "    flags = list()\n",
    "    for item in firstsplit:\n",
    "        if len((x := item.split(\" \"))) > 1:\n",
    "            opts[x[0]] = \" \".join(x[1:])\n",
    "        else:\n",
    "            flags.append(x[0])\n",
    "    \n",
    "    return opts, flags\n",
    "\n",
    "def make_tied_chains_list(pose: str, chains: list) -> list:\n",
    "    '''\n",
    "    Creates a tied_chains_list for ProteinMPNN for a given input <pose> for chains listed in <chains>.\n",
    "    Args:\n",
    "        <pose>              Should be path to a .pdb file.\n",
    "        <chains>            Should be a list of chain Letters (e.g. [\"A\", \"B\"]) indicating which chains should be tied.\n",
    "                            All chains to be tied should have the same length!\n",
    "        \n",
    "    Returns:\n",
    "        List with dictionaries of the format: [{\"A\": [1], \"B\": [1]}, ..., {\"A\": [n], \"B\": [n]}]\n",
    "    '''\n",
    "    # Start the parser\n",
    "    pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "\n",
    "    # Get the structure\n",
    "    structure = pdb_parser.get_structure(\"pose\", pose)\n",
    "    model = structure[0]\n",
    "    \n",
    "    # check if specified <chains> are in model\n",
    "    model_chains = [chain.id for chain in model.get_chains()]\n",
    "    for chain in chains:\n",
    "        if chain not in model_chains:\n",
    "            raise ValueError(f\"ERROR: Chain {chain} not found in {pose}\\nPossible chains: {', '.join(model_chains)}\")\n",
    "            \n",
    "    # TODO implement check if all chains are of the same length!\n",
    "    \n",
    "    # iterate through residues of first chain and write a list out of it:\n",
    "    tied_chains_list = list()\n",
    "    for resi in model[chains[0]].get_residues():\n",
    "        tied_chains_list.append({chain: [resi.get_id()[1]] for chain in chains})\n",
    "    \n",
    "    return tied_chains_list\n",
    "\n",
    "def pose_extract_chain(path_to_pose: str, chain: str, new_path=None) -> str:\n",
    "    '''\n",
    "        Extracts Chain <chain> from pose at <path_to_pose>.\n",
    "        Returns path to new pose, that only contains chain <chain>.\n",
    "        \n",
    "        Args:\n",
    "            <path_to_pose>            Path to your pdb-file\n",
    "            <chain>                   Letter identifier of your chain (E.g. chain=\"A\")\n",
    "            <new_path>                New path where extracted chain should be saved into. By Default it will be \"<path_to_pose>_chain_<chain>.pdb\"\n",
    "        \n",
    "        Returns:\n",
    "            Path where the new pose is stored.\n",
    "        \n",
    "    '''\n",
    "    # Start the parser\n",
    "    pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "    \n",
    "    # Get the structure\n",
    "    pose = pdb_parser.get_structure(\"pose\", path_to_pose)\n",
    "    \n",
    "    # Select chain\n",
    "    if chain not in [chain.id for chain in pose[0].get_chains()]:\n",
    "        raise ValueError(f\"ERROR: Chain {chain} not found in {path_to_pose}. Are you sure your inputs are correct?\")\n",
    "    extract_chain = pose[0][chain]\n",
    "    \n",
    "    # create path to save the pose to\n",
    "    new_path_to_pose = new_path or f\"{path_to_pose.replace('.pdb', '')}_chain_{chain}.pdb\"\n",
    "    \n",
    "    # save the pose\n",
    "    io = PDBIO()\n",
    "    io.set_structure(extract_chain)\n",
    "    io.save(new_path_to_pose)\n",
    "    \n",
    "    return new_path_to_pose\n",
    "\n",
    "def read_fasta_sequence(file: str) -> str:\n",
    "    '''\n",
    "    Reads fasta-formatted file and returns sequence.\n",
    "    '''\n",
    "    with open(file, 'r') as f:\n",
    "        x = \"\".join(f.readlines()[1:])\n",
    "    return x\n",
    "\n",
    "def read_pdb_sequence(file: str, chain_sep=\"/\") -> str:\n",
    "    '''\n",
    "    Reads .fasta sequence of a protein from a .pdb file\n",
    "    '''\n",
    "    # sanity check\n",
    "    if not file.endswith(\".pdb\"): print(f\"WARNING: Input file does not end with .pdb Are you sure the file-type is correct?\\nFile: {file}\")\n",
    "    \n",
    "    # Start the parser\n",
    "    pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "    ppb = Bio.PDB.PPBuilder()\n",
    "\n",
    "    # Get the structure\n",
    "    pose = pdb_parser.get_structure(file, file)\n",
    "\n",
    "    # collect the sequence\n",
    "    sequence = chain_sep.join([str(x.get_sequence()) for x in ppb.build_peptides(pose)])\n",
    "\n",
    "    return sequence\n",
    "\n",
    "def get_list_items_by_index(input_list: list, indeces: list[int]) -> list:\n",
    "    '''\n",
    "    Uses Pandas to access items of a list by their index and return them as a list.\n",
    "    '''\n",
    "    return list(pd.Series(input_list)[indeces])\n",
    "\n",
    "def calc_aliphatic_content_poses(poses: list, residues=None) -> pd.DataFrame:\n",
    "    '''\n",
    "    Calculates aliphatic content of poses (for a specified set of <residues> if specified).\n",
    "    Args:\n",
    "        <poses>             List of paths to .pdb files\n",
    "        <residues>          list: Has to be a list with indeces (starting with 1, so Rosetta Numbering or PDB numbering (without chains)) of residues for which to calculate aliphatic content \n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing the aliphatic content score for each pose.\n",
    "    '''\n",
    "    descriptions = [x.split(\"/\")[-1].split(\".\")[0] for x in poses]\n",
    "    \n",
    "    if all([x.endswith(\".fa\") for x in poses]):\n",
    "        # read sequences\n",
    "        seqs = [read_fasta_sequence(file) for file in poses]\n",
    "    elif all([x.endswith(\".pdb\") for x in poses]):\n",
    "        # convert *.pdb files into sequences\n",
    "        seqs = [read_pdb_sequence(file) for file in poses]\n",
    "    else:\n",
    "        raise ValueError(f\"ERROR: Your list of poses contains inconsistent file types. All files must be either *.pdb or *.fa files. \\nPoses: {poses}\")\n",
    "    \n",
    "    # if residues is set, extract specified residues from sequences:\n",
    "    if residues:\n",
    "        seqs = [\"\".join(list(pd.Series(list(seq))[[int(x)-1 for x in residues]])) for seq in seqs]\n",
    "    \n",
    "    #for seq in seqs:\n",
    "    #    print(seq)\n",
    "        \n",
    "    aliphatic_score_l = [calc_aliphatic_content(seq) for seq in seqs]\n",
    "    \n",
    "    return pd.DataFrame({\"description\": descriptions, \"aliphatic_score\": aliphatic_score_l})\n",
    "\n",
    "def calc_aliphatic_content(seq: str) -> float:\n",
    "    '''\n",
    "    Calculates aliphatic score for a sequence.\n",
    "    Aliphatic score is defined as the percentage of amino acids in the total sequence being any one of [alanine, glycine, isoleucine, leucine, proline, phenylalanine, and valine]\n",
    "    '''\n",
    "    num = sum(map(seq.upper().count, ['A', 'G', \"I\", \"L\", \"P\", \"V\", \"F\"]))\n",
    "    aliphatic_score = num / len(seq)\n",
    "    return aliphatic_score\n",
    "\n",
    "def parse_motif(poses_df: pd.DataFrame, motif) -> list:\n",
    "    '''\n",
    "    '''\n",
    "    if type(motif) == str:\n",
    "        return list(poses_df[motif])\n",
    "    elif type(motif) == dict:\n",
    "        return [motif for x in list(poses_df[\"poses\"])]\n",
    "    elif type(motif) == list:\n",
    "        return motif\n",
    "    else:\n",
    "        raise TypeError(f\"ERROR: unsupported datatype for Argument 'motif': {type(motif)}. \\n motif: {motif}\")\n",
    "\n",
    "def col_with_prefix_exists_in_df(df: pd.DataFrame, prefix: str) -> bool:\n",
    "    '''\n",
    "    Checks if there is any column in <df> that starts with <prefix>.\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame to check\n",
    "        prefix (str): Prefix for which you want to check if it exists in the DF\n",
    "    \n",
    "    Returns: (bool)\n",
    "    '''\n",
    "    return any([x.startswith(prefix) for x in df.columns])\n",
    "\n",
    "def reassign_motif(motif_res: dict, ref_pdb_idx: list, hal_pdb_idx: list) -> dict:\n",
    "    '''AAA'''\n",
    "    # expand motif into list of tuples for dict lookup:\n",
    "    motif_expanded_nested_list = [[(key, val) for val in values] for key, values in motif_res.items()]\n",
    "    motif_expanded_list = list(itertools.chain.from_iterable(motif_expanded_nested_list))\n",
    "    \n",
    "    # convert motif to new mapping:\n",
    "    exchange_dict = {tuple(ref_idx): hal_idx for ref_idx, hal_idx in zip(ref_pdb_idx, hal_pdb_idx)}\n",
    "    reassigned_motif_list = [exchange_dict[idx] for idx in motif_expanded_list]\n",
    "    \n",
    "    # convert motif_list into dict:\n",
    "    reassigned_motif = defaultdict(list)\n",
    "    for idx in reassigned_motif_list:\n",
    "        reassigned_motif[idx[0]].append(idx[1])\n",
    "\n",
    "    return dict(reassigned_motif)\n",
    "\n",
    "def reassign_identity_keys(identity_dict: dict, ref_pdb_idx: list, hal_pdb_idx: list) -> dict:\n",
    "    '''AAA'''\n",
    "    # transform idx into pdb_numbering:\n",
    "    ref_keys = [(x+str(y)) for x, y in ref_pdb_idx]\n",
    "    inp_keys = [(x+str(y)) for x, y in hal_pdb_idx]\n",
    "    \n",
    "    # collect mapping_dict\n",
    "    mapping_dict = {ref: inp for ref, inp in zip(ref_keys, inp_keys)}\n",
    "    return {mapping_dict[pdb_idx]: res_name for pdb_idx, res_name in identity_dict.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff7717",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b501eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpaint_scores = collect_inpainting_scores(\"testing/il37_test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa3eb02a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inpaint_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11193/145721804.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpaint_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"new_loc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'inpaint_scores' is not defined"
     ]
    }
   ],
   "source": [
    "print(inpaint_scores[\"new_loc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4da7d545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#test = mpnn_tools.write_mpnn_jsonl(values = [[\"A\"], [\"B\", \"C\"]], poses = \"test_pose\", json_filename = \"test_jsonl.json\", iterate=False)\n",
    "\n",
    "#with open(test, 'r') as f:\n",
    "    #print(f.read())\n",
    "    \n",
    "print(mpnn_tools.check_design_chains([[\"A\"], [\"B\", \"C\"], [\"A\"]], [\"test_pose\", \"\", \"\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8039b145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e16d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_poses = Poses(\"testing_fasta_conversion\", glob(f\"/home/markus/Desktop/script_development/iterative_refinement/heme_binder_redesign/OmegaFold_run1/of_pdbs/*.pdb\"))\n",
    "#testing_poses.poses_pdb_to_fasta()\n",
    "testing_poses.calc_protparams(prefix=\"test_params\")\n",
    "#display(testing_poses.poses_df.head(10)[[x for x in testing_poses.poses_df.columns if x.startswith(\"test_params_\")]])\n",
    "#testing_poses.new_poses_path(\"/home/markus/Desktop/script_development/iterative_refinement/heme_binder_redesign/OmegaFold_run1/\")\n",
    "print([x for x in testing_poses.poses_df[\"poses\"].head(5)])\n",
    "\n",
    "aliphatics = testing_poses.calc_metric(calc_aliphatic_content_poses, metric_prefix=\"test\", metric_kwargs={\"residues\": [1, 2, 3, 4, 5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a514877a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testing_poses.poses_df[[\"test_description\", \"test_aliphatic_score\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60682848",
   "metadata": {},
   "outputs": [],
   "source": [
    "af_output_dir = \"af2_preds/\"\n",
    "af2_predstest = collect_af2_scores(af_output_dir, pdb_dir=f\"{af_output_dir}/af2_pdbs\", scorefile=f\"{af_output_dir}/af2_scores.json\", calc_motif_rmsd=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a64c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(af2_predstest[[\"description\", \"top_plddt\", \"mean_plddt\", \"mean_af2_ca_rmsd\", \"location\"]].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ad987",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cycle_1.poses_df[[\"poses_description\", \"predict_run_0001_confidence\", \"template_bb_ca_rmsd\", \"of_motif_ca_rmsd\", \"mpnn_run_0001_score\", \"composite_score\"]].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2224aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"a.pdb\"\n",
    "if a.endswith(\".pdb\"): print(\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de72a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(Cycle.calc_composite_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c157c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([x for x in \"/home/markus//a///\".split(\"/\") if x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"mpnn_run_0001_score\", \"mpnn_run_0005_score\", \"mpnn_run_0003_score\", \"mpnn_run_0004_score\", \"mpnn_run_0002_score\", \"filter_run1\", \"confidence_0001\"]\n",
    "print(sorted([x for x in cols if x.endswith(\"score\")])[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e79bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cycle_1.poses_df = cycle_1.poses_df.set_index(\"poses_description\")\n",
    "print([(index, y) for index, y in cycle_1.poses_df[\"mpnn_run_0001_score\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece9a824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b1fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = make_tied_chains_list(\"/home/markus/projects/heme_binder/heme_inputs/heme_binder_3.pdb\", [\"A\", \"C\"])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7511dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "\n",
    "pose = \"/home/markus/projects/heme_binder/heme_inputs/heme_binder_3.pdb\"\n",
    "\n",
    "# Get the structure\n",
    "structure = pdb_parser.get_structure(\"pose\", pose)\n",
    "model = structure[0]\n",
    "\n",
    "possible_chains = [chain.id for chain in model.get_chains()]\n",
    "\n",
    "print(possible_chains)\n",
    "chains = [\"A\", \"B\"]\n",
    "\n",
    "if any([x not in possible_chains for x in chains]): print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e5a6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    int(\"A\")\n",
    "except ValueError:\n",
    "    raise ValueError(f\"Wrong Type for fixedposition.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7b41a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "testpose = pose_extract_chain(\"/home/markus/projects/heme_binder/heme_inputs/heme_binder_3.pdb\", \"A\")\n",
    "\n",
    "pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "tt = pdb_parser.get_structure(\"tt\", testpose)\n",
    "print([chain.id for chain in tt[0].get_chains()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4a00e",
   "metadata": {},
   "source": [
    "# Test Superimposition Part of Heme_binder_script\n",
    "### 1. Try loading Ligand (Heme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48158eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "testpose_path = \"/home/markus/Desktop/script_development/iterative_refinement/heme_binder_redesign/OmegaFold_run1/of_pdbs/heme_binder_3_0001_0006.pdb\"\n",
    "ref_path = \"/home/markus/projects/heme_binder/heme_inputs/heme_binder_3.pdb\"\n",
    "\n",
    "# load boath structures\n",
    "pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "tt = pdb_parser.get_structure(\"tt\", testpose_path)\n",
    "ref = pdb_parser.get_structure(\"ref\", ref_path)\n",
    "\n",
    "# test adding ligand chain to tt and saving it as a new structure:\n",
    "tt_model = tt[0]\n",
    "tt_model.add(ref[0][\"X\"])\n",
    "\n",
    "io = PDBIO()\n",
    "io.set_structure(tt)\n",
    "io.save(\"/home/markus/Desktop/script_development/iterative_refinement/testing/structure_superimposition/heme_binder_3_0001_0006_HEME.pdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7887e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chain_to_model(model: Bio.PDB.Structure, chain) -> Bio.PDB.Structure:\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99997862",
   "metadata": {},
   "source": [
    "## 2. Add second chain (duplicate of chain_A) into the heme_binder_structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "testpose_path = \"/home/markus/Desktop/script_development/iterative_refinement/heme_binder_redesign/OmegaFold_run1/of_pdbs/heme_binder_3_0001_0006.pdb\"\n",
    "ref_path = \"/home/markus/projects/heme_binder/heme_inputs/heme_binder_3.pdb\"\n",
    "\n",
    "# load both structures\n",
    "pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "tt = pdb_parser.get_structure(\"tt\", testpose_path)\n",
    "ref = pdb_parser.get_structure(\"ref\", ref_path)\n",
    "\n",
    "# extract chain and change chain.id:\n",
    "chain_B = deepcopy(tt[0][\"A\"])\n",
    "chain_B.id = \"B\"\n",
    "print(chain_B.id)\n",
    "print([chain.id for chain in tt[0].get_chains()])\n",
    "\n",
    "tt_model = tt[0]\n",
    "tt_model.add(chain_B)\n",
    "\n",
    "print([chain.id for chain in tt[0].get_chains()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab26611",
   "metadata": {},
   "source": [
    "## 3. Add second chain and ligand into the model and then superimpose based on reference structure!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c576d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CA_of_chain(model: Bio.PDB.Structure, chain: str) -> list:\n",
    "    '''\n",
    "    Collects C-alpha atoms of a chain into a list.\n",
    "    Args:\n",
    "        <structure>           Has to be a model of a Bio.PDB.Structure object (Bio.PDB.Structure[0])\n",
    "        <chain>               Chain name of which you want to gather C-alpha atoms.\n",
    "        \n",
    "    Returns:\n",
    "        List of C-alpha atoms\n",
    "    '''\n",
    "    # Return C-alpha atom for every residue in structure[chain] if the residue is an amino acid (if res.id[0] == \" \")\n",
    "    return [res[\"CA\"] for res in model[chain] if res.id[0] == \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50c7756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load both structures\n",
    "pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "tt = pdb_parser.get_structure(\"tt\", testpose_path)\n",
    "ref = pdb_parser.get_structure(\"ref\", ref_path)\n",
    "\n",
    "chain_B = deepcopy(tt[0][\"A\"])\n",
    "chain_B.id = \"B\"\n",
    "\n",
    "# Add ligand and chain_B to structure\n",
    "tt[0].add(chain_B)\n",
    "tt[0].add(ref[0][\"X\"])\n",
    "\n",
    "# start superimposition\n",
    "fixed_atom_IDs = [79, 16, 72, 12, 13, 9, 83, 75, 19]\n",
    "\n",
    "# collect lists of fixed and moving atoms for superimposition.\n",
    "fixed_A = get_CA_of_chain(ref[0], \"A\")\n",
    "moving_A = get_CA_of_chain(tt[0], \"A\")\n",
    "fixed_B = get_CA_of_chain(ref[0], \"B\")\n",
    "moving_B = get_CA_of_chain(tt[0], \"B\")\n",
    "fixed_X = list(ref[0][\"X\"].get_atoms())\n",
    "moving_X = list(tt[0][\"X\"].get_atoms())\n",
    "\n",
    "super_imposer_A = Bio.PDB.Superimposer()\n",
    "super_imposer_B = Bio.PDB.Superimposer()\n",
    "super_imposer_X = Bio.PDB.Superimposer()\n",
    "\n",
    "super_imposer_A.set_atoms(fixed_A, moving_A)\n",
    "super_imposer_A.apply(tt[0][\"A\"])\n",
    "super_imposer_B.set_atoms(fixed_B, moving_B)\n",
    "super_imposer_B.apply(tt[0][\"B\"])\n",
    "super_imposer_X.set_atoms(fixed_X, moving_X)\n",
    "super_imposer_X.apply(tt[0][\"X\"])\n",
    "\n",
    "io = PDBIO()\n",
    "io.set_structure(tt)\n",
    "io.save(\"/home/markus/Desktop/script_development/iterative_refinement/testing/structure_superimposition/heme_binder_3_0001_0006_superimposed.pdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041b4ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def superimpose_heme_binder(ref_path: str, pose_path: str):\n",
    "    '''\n",
    "    For Veronica's heme binder design.\n",
    "    Recreates the heme-binding dimer from the monomer. After that, the \n",
    "    '''\n",
    "    pdb_parser = Bio.PDB.PDBParser(QUIET = True)\n",
    "    tt = pdb_parser.get_structure(\"tt\", pose_path)\n",
    "    ref = pdb_parser.get_structure(\"ref\", ref_path)\n",
    "\n",
    "    chain_B = deepcopy(tt[0][\"A\"])\n",
    "    chain_B.id = \"B\"\n",
    "\n",
    "    # Add ligand and chain_B to structure\n",
    "    tt[0].add(chain_B)\n",
    "    tt[0].add(ref[0][\"X\"])\n",
    "\n",
    "    # start superimposition\n",
    "    #fixed_atom_IDs = [79, 16, 72, 12, 13, 9, 83, 75, 19]  ##!! TODO: for later, if I want to, I can add weighted superimposition on the motif sidechains.\n",
    "\n",
    "    # collect lists of fixed and moving atoms for superimposition.\n",
    "    fixed_A = get_CA_of_chain(ref[0], \"A\")\n",
    "    moving_A = get_CA_of_chain(tt[0], \"A\")\n",
    "    fixed_B = get_CA_of_chain(ref[0], \"B\")\n",
    "    moving_B = get_CA_of_chain(tt[0], \"B\")\n",
    "    fixed_X = list(ref[0][\"X\"].get_atoms())\n",
    "    moving_X = list(tt[0][\"X\"].get_atoms())\n",
    "\n",
    "    super_imposer_A = Bio.PDB.Superimposer()\n",
    "    super_imposer_B = Bio.PDB.Superimposer()\n",
    "    super_imposer_X = Bio.PDB.Superimposer()\n",
    "\n",
    "    super_imposer_A.set_atoms(fixed_A, moving_A)\n",
    "    super_imposer_A.apply(tt[0][\"A\"])\n",
    "    super_imposer_B.set_atoms(fixed_B, moving_B)\n",
    "    super_imposer_B.apply(tt[0][\"B\"])\n",
    "    super_imposer_X.set_atoms(fixed_X, moving_X)\n",
    "    super_imposer_X.apply(tt[0][\"X\"])\n",
    "\n",
    "    io = PDBIO()\n",
    "    io.set_structure(tt)\n",
    "    io.save(pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed391dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.match():  []\n",
      "re.search():  ['Hello World', 'lo Worl']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "l = [\"Hello World\", \"lo Worl\", \"Hello_World\", \"HellodWorld\"]\n",
    "print(\"re.match(): \", [x for x in l if re.match(\"o W\", x)])\n",
    "print(\"re.search(): \", [x for x in l if re.search(\"o W\", x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56d0c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [str(x-12) for x in [21,23,27,30,34,37,44,46,49,53,56,57,58,61,62,64,65,89,92,93,96,99,100,103,107,111,117,120,124,147,149,150,154,192,196,199,200,203,204,207,208,224,232,233,236,237,240,241,243,244,245,248,251,252,255,259]]\n",
    "print(\",\".join(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46db96a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if re.match(\"o W\", \"Hello World\"):\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1228fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
